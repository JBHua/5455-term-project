
Start -- Nov 10 16:33:01 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:33:01	Initing processor, tokenizer, and speaker_model
16:33:04	Loading Remote Dataset: facebook/voxpopuli. Sub-collection: en_accented
16:36:31	Finish Loading Remote Dataset. Length of dataset: 8387
16:36:31	Start Setting Sampling Rate to 16 kHz...
16:36:31	Setting Sampling Rate Successfully
16:36:31	Exception: <class 'TypeError'>. filter_and_prepare_dataset() missing 1 required positional argument: 'dataset'. Traceback: <traceback object at 0x7f1be4598dc0>

Start -- Nov 10 16:36:46 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:36:46	Initializing processor, tokenizer, and speaker_model
16:36:47	Loading Remote Dataset: facebook/voxpopuli. Sub-collection: en_accented
16:37:12	Finish Loading Remote Dataset. Length of dataset: 8387
16:37:12	Start Setting Sampling Rate to 16 kHz...
16:37:12	Setting Sampling Rate Successfully
16:37:12	Start Filtering Short Data
16:37:12	8358 data entries left after filtering
16:37:12	Start Preparing Dataset
16:38:41	KeyboardInterrupt detected. Exiting...

Start -- Nov 10 16:38:47 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:38:47	Initializing processor, tokenizer, and speaker_model
16:38:48	Loading Remote Dataset: facebook/voxpopuli. Sub-collection: en_accented
16:39:13	Finish Loading Remote Dataset. Length of dataset: 8387
16:39:13	Start Setting Sampling Rate to 16 kHz...
16:39:13	Setting Sampling Rate Successfully
16:39:13	Start Filtering Short Data
16:39:13	8358 data entries left after filtering
16:39:13	Start Preparing Dataset
16:40:04	Exception: <class 'multiprocess.context.TimeoutError'>. . Traceback: <traceback object at 0x7f1327d49f80>

Start -- Nov 10 16:40:08 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:40:08	Initializing processor, tokenizer, and speaker_model
16:40:09	Loading Remote Dataset: facebook/voxpopuli. Sub-collection: en_accented
16:40:34	Finish Loading Remote Dataset. Length of dataset: 8387
16:40:34	Start Setting Sampling Rate to 16 kHz...
16:40:34	Setting Sampling Rate Successfully
16:40:34	Start Filtering Short Data
16:40:34	8358 data entries left after filtering
16:40:34	Start Preparing Dataset
16:41:04	Exception: <class 'multiprocess.context.TimeoutError'>. . Traceback: <traceback object at 0x7fa037fc5100>

Start -- Nov 10 16:41:07 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:41:07	Initializing processor, tokenizer, and speaker_model
16:41:08	Loading Remote Dataset: facebook/voxpopuli. Sub-collection: en_accented
16:41:33	Finish Loading Remote Dataset. Length of dataset: 8387
16:41:33	Start Setting Sampling Rate to 16 kHz...
16:41:33	Setting Sampling Rate Successfully
16:41:33	Start Filtering Short Data
16:41:33	8358 data entries left after filtering
16:41:33	Start Preparing Dataset
16:47:00	Preparing dataset finished successfully
16:47:00	Start Filtering Long Data
16:47:00	6329 data entries left after filtering
16:47:00	save_processed_dataset is True. Saving processed dataset to dir: ./data/


Start -- Nov 10 16:50:43 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:50:43	Initializing processor, tokenizer, and speaker_model
16:50:44	Using Locally Processed Dataset. Skip Processing...
16:50:44	Failed to load local dataset. Please double check file exists and path is correct

Start -- Nov 10 16:51:12 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:51:12	Initializing processor, tokenizer, and speaker_model
16:51:12	Using Locally Processed Dataset. Skip Processing...
16:51:12	Failed to load local dataset. Please double check file exists and path is correct

Start -- Nov 10 16:52:45 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:52:45	Initializing processor, tokenizer, and speaker_model
16:52:45	Using Locally Processed Dataset. Skip Processing...
16:52:45	Failed to load local dataset. Please double check file exists and path is correct. You are trying to load a dataset that was saved using `save_to_disk`. Please use `load_from_disk` instead.

Start -- Nov 10 16:53:20 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
16:53:20	Initializing processor, tokenizer, and speaker_model
16:53:21	Using Locally Processed Dataset. Skip Processing...


Start -- Nov 10 17:42:43 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
17:42:43	Initializing processor, tokenizer, and speaker_model
17:43:04	Using Locally Processed Dataset. Skip Processing...
17:43:04	Generating Seq2SeqTrainer Arguments
17:43:04	Generating Seq2SeqTrainer
17:44:39	Exception: <class 'huggingface_hub.utils._errors.BadRequestError'>.  (Request ID: Root=1-654eb213-4bedca92381c9f61443bf392;dfd56855-90c4-42ec-bffa-f445e26543e3)

Bad request for commit endpoint:
"language[0]" with value "en_accented" is not valid. It must be an ISO 639-1, 639-2 or 639-3 code (two/three letters), or a special value like "code", "multilingual". If you want to use BCP-47 identifiers, you can specify them in language_bcp47.
"tags[0]" is not allowed to be empty. Traceback: <traceback object at 0x7eff3ca4d400>

Start -- Nov 10 17:46:04 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
17:46:04	Initializing processor, tokenizer, and speaker_model
17:46:06	Using Locally Processed Dataset. Skip Processing...
17:46:06	Generating Seq2SeqTrainer Arguments
17:46:06	Generating Seq2SeqTrainer
17:46:08	Start Training...
18:44:14	Start Saving the model...
18:44:17	Exception: <class 'KeyError'>. "Column test not in the dataset. Current columns in the dataset: ['input_ids', 'labels', 'speaker_embeddings']". Traceback: <traceback object at 0x7fc01cdca2c0>

Start -- Nov 10 20:53:15 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
20:53:15	Initializing processor, tokenizer, and speaker_model
20:53:17	Using Locally Processed Dataset. Skip Processing...
20:53:17	Loading Locally Stored Model from: ./model/trained_yifanhua-threadripper20:53:15
20:53:17	Exception: <class 'huggingface_hub.utils._validators.HFValidationError'>. Repo id must be in the form 'repo_name' or 'namespace/repo_name': './model/trained_yifanhua-threadripper20:53:15'. Use `repo_type` argument if needed.. Traceback: <traceback object at 0x7fca353cd380>

Start -- Nov 10 20:54:20 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
20:54:20	Initializing processor, tokenizer, and speaker_model
20:54:23	Using Locally Processed Dataset. Skip Processing...
20:54:23	Loading Locally Stored Model from: ./model/trained_yifanhua-threadripper20:54:20
20:54:23	Exception: <class 'huggingface_hub.utils._validators.HFValidationError'>. Repo id must be in the form 'repo_name' or 'namespace/repo_name': './model/trained_yifanhua-threadripper20:54:20'. Use `repo_type` argument if needed.. Traceback: <traceback object at 0x7fd3a2a8e1c0>

Start -- Nov 10 21:05:37 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:05:37	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:05:39	Using Locally Processed Dataset. Skip Processing...
21:05:39	Loading Locally Stored Model from: ./model/trained_yifanhua-threadripper21:03:24
21:05:51	KeyboardInterrupt detected. Exiting...

Start -- Nov 10 21:06:31 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:06:31	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:06:33	Using Locally Processed Dataset. Skip Processing...
21:06:33	Loading Locally Stored Model from: ./model/trained_yifanhua-threadripper21:06:31
21:06:36	Exception: <class 'IndexError'>. list index out of range. Traceback: <traceback object at 0x7fb5cb4a8380>

Start -- Nov 10 21:07:53 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:07:53	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:07:55	Using Locally Processed Dataset. Skip Processing...
21:07:55	Loading Locally Stored Model from: ./model/
21:08:01	Exception: <class 'huggingface_hub.utils._validators.HFValidationError'>. Repo id must be in the form 'repo_name' or 'namespace/repo_name': './model/trained_yifanhua-threadripper21:07:53'. Use `repo_type` argument if needed.. Traceback: <traceback object at 0x7fba6c0c8180>

Start -- Nov 10 21:08:30 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:08:30	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:08:32	Using Locally Processed Dataset. Skip Processing...
21:08:32	Loading Locally Stored Model from: ./model/
21:09:18	Exception: <class 'huggingface_hub.utils._validators.HFValidationError'>. Repo id must be in the form 'repo_name' or 'namespace/repo_name': './model/trained_yifanhua-threadripper21:08:30'. Use `repo_type` argument if needed.. Traceback: <traceback object at 0x7f064e128240>

Start -- Nov 10 21:10:48 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:10:48	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:10:50	Using Locally Processed Dataset. Skip Processing...
21:10:50	Loading Locally Stored Model from: ./model/
21:10:51	Using model: ./model/trained_yifanhua-threadripper17:46:04
21:10:51	Exception: <class 'huggingface_hub.utils._validators.HFValidationError'>. Repo id must be in the form 'repo_name' or 'namespace/repo_name': './model/trained_yifanhua-threadripper21:10:48'. Use `repo_type` argument if needed.. Traceback: <traceback object at 0x7f3dcffd6b40>

Start -- Nov 10 21:11:12 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:11:12	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:11:14	Using Locally Processed Dataset. Skip Processing...
21:11:14	Loading Locally Stored Model from: ./model/
21:11:16	Using model: ./model/trained_yifanhua-threadripper17:46:04


Start -- Nov 10 21:23:19 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:23:19	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:23:22	Using Locally Processed Dataset. Skip Processing...
21:23:22	Exception: <class 'NameError'>. name 'selected_model' is not defined. Traceback: <traceback object at 0x7fb9d05c8c00>

Start -- Nov 10 21:23:42 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:23:42	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:23:44	Using Locally Processed Dataset. Skip Processing...
21:23:44	Using model: ./model/trained_yifanhua-threadripper17:46:04


Start -- Nov 10 21:24:20 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:24:20	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:24:23	Using Locally Processed Dataset. Skip Processing...
21:24:23	Using model: ./model/trained_yifanhua-threadripper17:46:04


Start -- Nov 10 21:24:40 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:24:40	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:24:43	Using Locally Processed Dataset. Skip Processing...
21:24:43	Using model: ./model/trained_yifanhua-threadripper17:46:04


Start -- Nov 10 21:25:05 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:25:05	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:25:07	Using Locally Processed Dataset. Skip Processing...
21:25:07	Using model: ./model/trained_yifanhua-threadripper17:46:04


Start -- Nov 10 21:31:59 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:31:59	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:32:02	Using Locally Processed Dataset. Skip Processing...
21:32:02	Using model: ./model/trained_yifanhua-threadripper17:46:04


Start -- Nov 10 21:33:06 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:33:06	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:33:10	Using Locally Processed Dataset. Skip Processing...
21:33:10	Generating Seq2SeqTrainer Arguments
21:33:10	Generating Seq2SeqTrainer
21:33:10	Start Training...
21:33:11	Exception: <class 'RuntimeError'>. cuDNN error: CUDNN_STATUS_NOT_INITIALIZED. Traceback: <traceback object at 0x7f4a3c31cdc0>

Start -- Nov 10 21:34:38 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:34:38	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:34:40	Using Locally Processed Dataset. Skip Processing...
21:34:40	Generating Seq2SeqTrainer Arguments
21:34:40	Generating Seq2SeqTrainer
21:34:40	Start Training...
21:34:43	Exception: <class 'torch.cuda.OutOfMemoryError'>. CUDA out of memory. Tried to allocate 268.00 MiB. GPU 0 has a total capacty of 7.75 GiB of which 109.75 MiB is free. Including non-PyTorch memory, this process has 5.15 GiB memory in use. Of the allocated memory 4.51 GiB is allocated by PyTorch, and 503.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF. Traceback: <traceback object at 0x7f1c6e0c6140>

Start -- Nov 10 21:38:17 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:38:17	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:38:19	Using Locally Processed Dataset. Skip Processing...
21:38:19	Generating Seq2SeqTrainer Arguments
21:38:19	Generating Seq2SeqTrainer
21:38:19	Start Training...
21:38:20	Exception: <class 'torch.cuda.OutOfMemoryError'>. CUDA out of memory. Tried to allocate 274.00 MiB. GPU 0 has a total capacty of 7.75 GiB of which 311.38 MiB is free. Including non-PyTorch memory, this process has 4.58 GiB memory in use. Of the allocated memory 4.08 GiB is allocated by PyTorch, and 391.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF. Traceback: <traceback object at 0x7f81ea7d94c0>

Start -- Nov 10 21:39:25 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
21:39:25	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
21:39:27	Using Locally Processed Dataset. Skip Processing...
21:39:27	Generating Seq2SeqTrainer Arguments
21:39:27	Generating Seq2SeqTrainer
21:39:27	Start Training...
22:16:58	Start Saving the model...
22:17:05	Exception: <class 'RuntimeError'>. Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select). Traceback: <traceback object at 0x7f606a7c6ec0>

Start -- Nov 10 22:18:28 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:18:28	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:18:31	Using Locally Processed Dataset. Skip Processing...
22:18:34	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:18:35	Exception: <class 'RuntimeError'>. Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat). Traceback: <traceback object at 0x7fa94bf6ecc0>

Start -- Nov 10 22:19:14 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:19:14	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:19:16	Using Locally Processed Dataset. Skip Processing...
22:19:50	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:19:51	Exception: <class 'RuntimeError'>. Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select). Traceback: <traceback object at 0x7fa9c04cc4c0>

Start -- Nov 10 22:27:51 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:27:51	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:27:53	Using Locally Processed Dataset. Skip Processing...
22:27:56	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 22:28:09 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:28:09	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:28:11	Using Locally Processed Dataset. Skip Processing...
22:28:14	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 22:28:34 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:28:34	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:28:37	Using Locally Processed Dataset. Skip Processing...
22:28:39	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 22:31:52 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:31:52	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:31:54	Using Locally Processed Dataset. Skip Processing...
22:31:54	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 22:33:35 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:33:35	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:33:37	Using Locally Processed Dataset. Skip Processing...
22:33:45	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 22:34:16 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:34:16	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:34:18	Using Locally Processed Dataset. Skip Processing...
22:37:12	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 22:37:41 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:37:41	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:37:43	Using Locally Processed Dataset. Skip Processing...
22:39:21	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 22:39:39 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:39:39	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:39:41	Using Locally Processed Dataset. Skip Processing...
22:39:52	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 22:40:05 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:40:05	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:40:07	Using Locally Processed Dataset. Skip Processing...
22:40:08	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:40:09	Exception: ['Traceback (most recent call last):\n', '  File "/home/yifanhua/5455-term-project/src/main.py", line 374, in <module>\n    spectrogram = pretrained_model.generate_speech(inputs["input_ids"], speaker_embeddings)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context\n    return func(*args, **kwargs)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2824, in generate_speech\n    return _generate_speech(\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2480, in _generate_speech\n    encoder_out = model.speecht5.encoder(\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 1448, in forward\n    hidden_states = self.prenet(input_values)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 787, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward\n    return F.embedding(\n', '  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n', 'RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n']

Start -- Nov 10 22:43:07 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:43:07	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:43:09	Using Locally Processed Dataset. Skip Processing...
22:43:14	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:43:15	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 374, in <module>
    spectrogram = pretrained_model.generate_speech(inputs["input_ids"], speaker_embeddings)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2824, in generate_speech
    return _generate_speech(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2480, in _generate_speech
    encoder_out = model.speecht5.encoder(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 1448, in forward
    hidden_states = self.prenet(input_values)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 787, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)


Start -- Nov 10 22:46:38 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:46:38	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:46:40	Using Locally Processed Dataset. Skip Processing...
22:46:54	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:46:55	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 374, in <module>
    spectrogram = pretrained_model.generate_speech(inputs["input_ids"], speaker_embeddings)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2824, in generate_speech
    return _generate_speech(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2480, in _generate_speech
    encoder_out = model.speecht5.encoder(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 1448, in forward
    hidden_states = self.prenet(input_values)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 787, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)


Start -- Nov 10 22:47:34 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:47:34	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:47:36	Using Locally Processed Dataset. Skip Processing...
22:47:37	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:47:38	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 373, in <module>
    spectrogram = pretrained_model.generate_speech(inputs["input_ids"], speaker_embeddings)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2824, in generate_speech
    return _generate_speech(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2480, in _generate_speech
    encoder_out = model.speecht5.encoder(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 1448, in forward
    hidden_states = self.prenet(input_values)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 787, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)


Start -- Nov 10 22:48:28 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:48:28	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:48:30	Using Locally Processed Dataset. Skip Processing...
22:48:41	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:48:42	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 376, in <module>
    speech = vocoder(spectrogram)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 3208, in forward
    spectrogram = (spectrogram - self.mean) / self.scale
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!


Start -- Nov 10 22:52:35 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:52:35	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:52:37	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 225, in <module>
    pretrained_model, processor, tokenizer, speaker_model, data_collator, vocoder = init_global_variables()
  File "/home/yifanhua/5455-term-project/src/main.py", line 86, in init_global_variables
    _processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts").to(device)
AttributeError: 'SpeechT5Processor' object has no attribute 'to'


Start -- Nov 10 22:52:58 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:52:58	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:53:00	Using Locally Processed Dataset. Skip Processing...
22:53:02	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:53:04	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 376, in <module>
    speech = vocoder(spectrogram)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 3208, in forward
    spectrogram = (spectrogram - self.mean) / self.scale
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!


Start -- Nov 10 22:53:37 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:53:37	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:53:39	Using Locally Processed Dataset. Skip Processing...
22:53:40	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:53:41	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 376, in <module>
    speech = vocoder(spectrogram)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 3208, in forward
    spectrogram = (spectrogram - self.mean) / self.scale
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!


Start -- Nov 10 22:53:58 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:53:58	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:54:00	Using Locally Processed Dataset. Skip Processing...
22:54:00	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:54:02	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 380, in <module>
    Audio(speech.numpy(), rate=16000)
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.


Start -- Nov 10 22:55:16 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:55:16	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:55:18	Using Locally Processed Dataset. Skip Processing...
22:55:19	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
22:55:21	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 384, in <module>
    sf.write("output.wav", speech.numpy(), samplerate=16000)
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.


Start -- Nov 10 22:58:12 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
22:58:12	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
22:58:15	Using Locally Processed Dataset. Skip Processing...
22:58:44	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 23:04:34 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
23:04:34	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
23:04:36	Using Locally Processed Dataset. Skip Processing...
23:04:38	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
23:04:39	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 371, in <module>
    example = divided_dataset["test"][5]
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2803, in __getitem__
    return self._getitem(key)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2787, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 583, in query_table
    _check_valid_index_key(key, size)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 526, in _check_valid_index_key
    raise IndexError(f"Invalid key: {key} is out of bounds for size {size}")
IndexError: Invalid key: 5 is out of bounds for size 5


Start -- Nov 10 23:04:49 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
23:04:49	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
23:04:51	Using Locally Processed Dataset. Skip Processing...
23:04:53	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39

Start -- Nov 10 23:06:29 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
23:06:29	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
23:06:32	Using Locally Processed Dataset. Skip Processing...
23:06:34	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
23:06:35	Input text: I'm loading the model from the Hugging Face Hub!
23:06:35	Using speaker embedding: tensor([[-7.6836e-02,  6.3403e-02,  4.0562e-02,  3.8820e-02, -1.7536e-02,
          9.0452e-03, -4.9579e-02,  4.5229e-02,  4.2024e-02,  1.4980e-02,
         -7.4401e-02, -8.2151e-02,  4.0258e-02,  3.4921e-02,  3.0220e-02,
          4.3113e-02,  2.8215e-02,  1.8833e-02,  1.4011e-02,  1.5947e-02,
          3.4128e-02,  2.5922e-02, -1.3152e-02, -5.4478e-02, -6.3750e-02,
         -9.2914e-03, -6.7767e-02,  1.5682e-03,  4.6559e-02,  4.0730e-02,
         -3.0536e-03,  5.2462e-02,  4.2241e-02,  1.7429e-03,  2.8580e-02,
         -7.5006e-02,  3.1938e-02,  2.8833e-02,  1.4967e-02, -6.4432e-02,
          1.8939e-02, -9.6601e-03,  2.1530e-02,  5.3108e-02,  4.4103e-02,
         -1.0370e-01, -1.8632e-02,  1.2255e-02, -9.0650e-02,  4.7486e-02,
          2.5308e-02,  2.9944e-02,  1.1476e-02,  3.2961e-02, -8.1554e-02,
         -6.5184e-02,  1.4976e-02,  2.7341e-02,  3.2480e-02,  2.3878e-02,
          1.6458e-02, -7.3298e-03, -1.4582e-02,  1.5723e-02,  3.7580e-02,
          2.4833e-02,  4.5336e-02, -5.6571e-02, -6.9297e-02, -4.3971e-02,
          5.3361e-04,  9.0129e-03,  4.7100e-02,  1.2022e-02,  2.4132e-02,
          4.5285e-02,  8.8045e-03,  2.0786e-02, -5.7390e-02, -6.9695e-02,
         -6.2979e-02, -5.5661e-02, -7.1943e-02, -5.0074e-02, -3.2374e-02,
         -7.4352e-02, -5.9192e-02,  4.9010e-02,  2.1572e-02, -6.5620e-02,
          2.7758e-02, -9.1477e-02,  1.7411e-02, -7.2112e-02,  3.7737e-02,
         -6.9570e-03,  3.6025e-02,  3.9992e-02, -5.8816e-02, -7.7995e-02,
          1.7029e-02, -7.5107e-02, -7.2432e-02,  2.5058e-02,  3.2963e-02,
          2.0084e-02,  4.3422e-02,  6.3396e-02,  2.6535e-02,  2.6281e-02,
         -9.0686e-02,  1.0709e-02,  7.4889e-02,  5.9660e-03,  2.3868e-02,
          5.3681e-02, -6.9733e-02,  5.0336e-03, -5.4502e-02, -1.8466e-03,
          1.7699e-02, -5.9915e-02,  6.1621e-03,  4.6774e-02, -6.2205e-02,
          2.2998e-02, -7.8370e-02,  2.9577e-02,  2.5580e-02,  5.9209e-02,
          2.0903e-02,  3.4895e-02,  1.0720e-02,  6.7392e-02,  2.8271e-02,
         -8.2322e-02, -9.4090e-02,  2.9570e-02, -6.0319e-02,  8.3229e-04,
          3.6323e-02,  1.4722e-03,  1.1924e-02,  6.2906e-02, -6.4020e-02,
          3.6470e-02, -1.3211e-02,  1.3893e-02,  3.0392e-02, -8.4153e-02,
          4.6257e-02, -6.4347e-02, -8.6102e-02,  3.2333e-02,  1.8153e-02,
          1.8537e-02,  2.5648e-02, -8.1804e-02, -7.1273e-02,  5.8943e-02,
          9.4322e-03,  2.1041e-02,  2.3579e-02,  3.5110e-02,  2.9283e-03,
          3.5936e-03,  4.1134e-02,  2.3648e-02,  2.4587e-02,  3.7277e-02,
          1.4844e-02, -6.3547e-02,  6.7236e-03, -7.0230e-02, -3.8990e-02,
          2.2931e-02, -6.3326e-02,  1.6781e-02,  5.9014e-02, -7.7587e-02,
          2.7868e-02, -7.6592e-02,  3.3783e-02,  1.9375e-02, -7.5908e-02,
         -1.0387e-02,  1.4482e-02,  3.2370e-02, -6.6226e-02, -6.2260e-02,
          2.3353e-02,  2.7653e-02, -7.1155e-02,  2.6955e-02,  1.1666e-02,
          2.7088e-02,  1.8777e-02,  1.5794e-02,  3.1896e-02,  5.7146e-03,
          1.7183e-02,  3.0124e-02, -4.7382e-02,  1.3569e-02,  2.6958e-02,
          1.3499e-02,  3.0649e-02,  2.4900e-02,  3.9258e-02,  2.0793e-02,
          3.9824e-03, -7.0943e-02, -5.0493e-02,  4.8370e-02, -6.2684e-02,
          3.4115e-02,  3.9837e-02, -9.1817e-02,  2.9612e-02,  3.2944e-02,
         -6.8549e-02,  3.3213e-02, -7.0979e-02, -3.4610e-02, -7.1876e-02,
          5.6086e-02,  2.0452e-02,  7.0578e-02,  6.3566e-02,  2.9136e-02,
         -1.3794e-02,  6.8589e-02,  2.3369e-02,  1.6955e-02,  6.2187e-03,
          7.6584e-03,  4.0822e-02,  1.8875e-02,  3.2420e-03,  4.5718e-02,
         -5.8132e-02,  2.0638e-02, -9.0662e-02,  2.6839e-02, -6.8926e-02,
          2.6182e-02,  3.1046e-02,  2.3971e-02,  1.9273e-02,  3.9320e-02,
          5.2058e-02,  2.1656e-02,  8.4990e-03, -7.2728e-02,  5.7367e-03,
          4.7709e-02,  1.8558e-02,  4.2686e-02, -4.6847e-02,  5.7891e-02,
          1.9858e-02, -7.3581e-02,  4.9841e-02,  2.3763e-02, -9.7366e-05,
          3.2634e-02,  4.0977e-02,  4.1435e-02,  2.3259e-02,  2.1570e-02,
          2.7965e-02, -3.6707e-03,  6.4330e-03,  4.3431e-02, -8.1272e-02,
          2.1740e-02,  3.4477e-02,  3.4442e-02, -5.8742e-02, -8.1472e-02,
         -2.7763e-02,  3.0170e-02,  5.8892e-02, -7.6027e-02,  3.4409e-02,
          4.3539e-02, -1.2113e-02,  4.0763e-02, -6.8863e-02,  5.1888e-03,
          7.8995e-03,  6.7038e-03,  2.1326e-02, -1.5543e-02,  3.6723e-02,
          2.3226e-02,  2.9182e-02, -9.0791e-03,  3.0437e-02,  5.0427e-02,
         -6.9418e-02,  2.7464e-02,  2.2207e-02, -7.3056e-02,  1.6306e-02,
          3.0963e-02,  2.9017e-02,  2.0750e-02,  3.1957e-02,  2.9078e-02,
          4.5587e-02,  2.0261e-02,  5.0033e-03,  5.8378e-02, -8.4611e-02,
         -7.6075e-02, -1.9838e-02,  4.0487e-02, -6.1888e-02,  4.3652e-02,
         -6.2916e-02,  3.9619e-02, -5.2359e-02,  6.3863e-03,  4.9324e-02,
         -6.9716e-02,  4.2202e-02,  6.4274e-02, -6.0595e-02, -5.6750e-02,
         -6.0675e-02,  2.8630e-02,  2.9104e-02,  4.5632e-02, -8.1962e-02,
          4.5834e-02,  2.8655e-02,  1.9608e-02,  3.6816e-03,  4.6190e-02,
          2.9040e-02, -4.9038e-02,  3.5908e-02,  2.0035e-03, -9.5161e-02,
          1.3396e-02,  3.1781e-02,  2.4335e-02, -5.2164e-02, -7.2463e-02,
         -6.3275e-02,  2.3019e-02, -7.1583e-02,  1.1089e-01,  2.1324e-02,
         -4.8902e-02, -5.4751e-03,  5.0892e-02, -3.2968e-02,  1.6456e-03,
         -1.0364e-01,  1.1794e-02,  2.8560e-03,  3.1179e-02,  5.8883e-02,
         -8.6580e-03, -5.3970e-02,  1.7495e-02,  2.7266e-02,  2.2890e-02,
          5.9171e-03,  3.1879e-02, -5.4221e-02,  7.6534e-03, -1.3825e-02,
         -2.2423e-03,  3.0243e-02,  2.1296e-02,  3.1641e-02,  1.9973e-02,
         -8.4289e-02, -1.0444e-01,  2.8702e-02,  3.7857e-02,  2.3716e-02,
          2.9539e-02,  5.5774e-02, -6.6566e-02,  2.0514e-02,  3.5840e-02,
          1.6014e-02,  1.7473e-02, -4.7530e-02,  2.7591e-02,  2.4982e-02,
         -8.2298e-03,  5.0999e-02, -7.7798e-02,  3.4287e-02, -1.2068e-02,
          1.9090e-03, -5.1682e-02,  9.6981e-03,  2.2473e-02,  3.1747e-02,
          4.2606e-03,  2.0931e-03, -6.1320e-02, -5.2187e-02, -7.9547e-02,
         -4.8807e-02,  3.5517e-02,  3.4056e-02, -1.2789e-02,  4.4212e-02,
         -6.5526e-02,  1.5182e-02, -7.6318e-02, -1.3027e-01, -1.9935e-02,
         -7.3741e-02, -1.0807e-03,  2.6086e-02,  2.6132e-02, -5.8830e-02,
          3.4185e-02,  4.0558e-02,  3.2187e-02,  4.5877e-02, -5.3099e-02,
         -4.2563e-02, -1.2752e-02, -1.9676e-02,  3.6419e-02,  3.3966e-02,
          2.8699e-03,  3.8468e-02,  1.4768e-02,  2.2580e-02, -7.2594e-02,
         -4.6226e-03,  3.8273e-02,  1.4515e-02,  2.8593e-03,  7.1926e-03,
          4.0886e-02,  1.9179e-02,  5.4359e-03,  4.3910e-03, -1.1557e-02,
          3.2949e-02,  1.9718e-02,  3.7592e-02,  4.9951e-02,  4.5965e-02,
          3.4425e-03,  3.8280e-02, -6.6107e-02, -5.9034e-02,  4.0041e-02,
         -1.1044e-02,  3.2420e-03,  2.6370e-02,  3.4795e-02, -7.8728e-02,
          2.9791e-02,  2.5478e-02,  9.7442e-03,  5.1081e-02,  3.3058e-02,
         -2.0060e-03,  4.1954e-02,  2.4819e-03,  5.9872e-03, -5.1906e-02,
         -8.1537e-02,  1.6202e-02, -4.1698e-03,  2.7178e-02,  4.4018e-02,
         -6.4413e-02, -5.9531e-02,  8.8259e-04,  3.8542e-02,  2.8775e-02,
          3.4858e-02, -6.3876e-02,  2.6297e-02, -3.1551e-02, -1.0353e-02,
         -3.4075e-02,  2.4226e-02, -3.8576e-02, -2.3174e-02, -3.1889e-02,
          1.6949e-02, -4.8726e-02,  4.1621e-03, -6.3748e-02, -7.0101e-02,
          7.8505e-03,  4.0942e-02, -6.4920e-02,  3.1049e-02,  5.1092e-03,
         -1.3265e-02,  1.5715e-02,  4.5942e-02,  6.0792e-02, -5.3153e-02,
          3.5526e-02, -6.5377e-02]], device='cuda:0')

Start -- Nov 10 23:08:00 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
23:08:00	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
23:08:03	Using Locally Processed Dataset. Skip Processing...
23:08:03	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
23:08:04	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 11 09:49:03 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
09:49:03	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
09:49:05	Loading Remote Dataset: facebook/voxpopuli. Sub-collection: en_accented
09:49:32	Finish Loading Remote Dataset. Length of dataset: 8387
09:49:32	Start Setting Sampling Rate to 16 kHz...
09:49:32	Setting Sampling Rate Successfully
09:49:32	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 364, in <module>
    sort_speaker()
TypeError: sort_speaker() missing 1 required positional argument: '_dataset'


Start -- Nov 11 09:49:49 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
09:49:49	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
09:49:51	Loading Remote Dataset: facebook/voxpopuli. Sub-collection: en_accented
09:50:17	Finish Loading Remote Dataset. Length of dataset: 8387
09:50:17	Start Setting Sampling Rate to 16 kHz...
09:50:17	Setting Sampling Rate Successfully
09:50:18	Start Filtering Short Data
09:50:18	8358 data entries left after filtering
09:50:18	Start Preparing Dataset
09:50:39	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 09:53:21 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
09:53:21	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
09:53:23	Loading Remote Dataset: facebook/voxpopuli. Sub-collection: en_accented
09:53:49	Finish Loading Remote Dataset. Length of dataset: 8387
09:53:49	Start Setting Sampling Rate to 16 kHz...
09:53:49	Setting Sampling Rate Successfully
09:53:49	Start Filtering Short Data
09:53:49	8358 data entries left after filtering
09:53:49	Start Preparing Dataset
09:53:55	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 11:47:35 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
11:47:35	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
11:47:38	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
12:22:59	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 397, in <module>
    # Step 1A: Process & Preparing Dataset
  File "/home/yifanhua/5455-term-project/src/main.py", line 265, in load_remote_dataset
    _dataset = load_dataset(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/load.py", line 2166, in load_dataset
    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/builder.py", line 1190, in as_dataset
    datasets = map_nested(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 456, in map_nested
    return function(data_struct)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/builder.py", line 1220, in _build_single_dataset
    ds = self._as_dataset(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/builder.py", line 1294, in _as_dataset
    dataset_kwargs = ArrowReader(cache_dir, self.info).read(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_reader.py", line 240, in read
    files = self.get_file_instructions(name, instructions, split_infos)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_reader.py", line 213, in get_file_instructions
    file_instructions = make_file_instructions(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_reader.py", line 130, in make_file_instructions
    absolute_instructions = instruction.to_absolute(name2len)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_reader.py", line 653, in to_absolute
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_reader.py", line 653, in <listcomp>
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_reader.py", line 465, in _rel_to_abs_instr
    raise ValueError(f'Unknown split "{split}". Should be one of {list(name2len)}.')
ValueError: Unknown split "validated". Should be one of ['train', 'test', 'validation', 'other', 'invalidated'].


Start -- Nov 11 12:23:47 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:23:47	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:23:50	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
12:27:15	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 12:27:19 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:27:19	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:27:22	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
12:32:09	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 13:32:36 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
13:32:36	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
13:32:38	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
13:33:01	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 13:45:53 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
13:45:53	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
13:45:56	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
13:46:07	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 13:46:47 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
13:46:47	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
13:46:50	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
13:58:24	Finish Loading Remote Dataset. Length of dataset: 12135
13:58:24	Begin Sharding the dataset
13:58:24	Total 252 shards
13:58:24	Using Mozilla Common Voice. Additional Cleaning Needed
13:58:24	Starting Size of Mozilla Common Voice: 12135
13:59:05	Size after filtering accent: 8671
13:59:08	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 407, in <module>
    dataset = load_remote_dataset()
  File "/home/yifanhua/5455-term-project/src/main.py", line 281, in load_remote_dataset
    _dataset = list(executor.map(clean_mozilla_dataset(_dataset), _shard_dataset))
  File "/home/yifanhua/5455-term-project/src/main.py", line 250, in clean_mozilla_dataset
    _dataset = _dataset.filter(lambda entry: entry["gender"].isin(['female', 'male']))
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/fingerprint.py", line 511, in wrapper
    out = func(dataset, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3627, in filter
    indices = self.map(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3474, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3353, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 6238, in get_indices_from_mask_function
    function(example, indices[i], **fn_kwargs) if with_indices else function(example, **fn_kwargs)
  File "/home/yifanhua/5455-term-project/src/main.py", line 250, in <lambda>
    _dataset = _dataset.filter(lambda entry: entry["gender"].isin(['female', 'male']))
AttributeError: 'str' object has no attribute 'isin'


Start -- Nov 11 13:59:29 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
13:59:29	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
13:59:32	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
13:59:32	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 14:02:27 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
14:02:27	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
14:02:29	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
14:14:06	Finish Loading Remote Dataset. Length of dataset: 12135
14:14:06	Using Mozilla Common Voice. Additional Cleaning Needed
14:14:06	Starting Size of Mozilla Common Voice: 12135
14:14:45	Size after filtering accent: 8671
14:15:12	Size after filtering gender: 8637
14:15:38	Size after filtering voting: 8637
14:15:38	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 410, in <module>
    dataset = load_remote_dataset()
  File "/home/yifanhua/5455-term-project/src/main.py", line 285, in load_remote_dataset
    _dataset = clean_mozilla_dataset(_dataset)
  File "/home/yifanhua/5455-term-project/src/main.py", line 256, in clean_mozilla_dataset
    _dataset = _dataset.map(lambda entry: entry['sentence'].str.lower())
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3450, in _map_single
    example = apply_function_on_filtered_inputs(example, i, offset=offset)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3353, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/yifanhua/5455-term-project/src/main.py", line 256, in <lambda>
    _dataset = _dataset.map(lambda entry: entry['sentence'].str.lower())
AttributeError: 'str' object has no attribute 'str'


Start -- Nov 11 14:17:45 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
14:17:45	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
14:17:47	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
14:29:20	Finish Loading Remote Dataset. Length of dataset: 5
14:29:20	Using Mozilla Common Voice. Additional Cleaning Needed
14:29:20	Starting Size of Mozilla Common Voice: 5
14:30:28	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 410, in <module>
    dataset = load_remote_dataset()
  File "/home/yifanhua/5455-term-project/src/main.py", line 285, in load_remote_dataset
    _dataset = clean_mozilla_dataset(_dataset)
  File "/home/yifanhua/5455-term-project/src/main.py", line 247, in clean_mozilla_dataset
    _dataset = _dataset.filter(lambda entry: True if entry["accent"] else False)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/dataset_dict.py", line 960, in filter
    {
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/dataset_dict.py", line 961, in <dictcomp>
    k: dataset.filter(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/fingerprint.py", line 511, in wrapper
    out = func(dataset, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3627, in filter
    indices = self.map(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3474, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3353, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 6236, in get_indices_from_mask_function
    example = {key: batch[key][i] for key in batch}
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 6236, in <dictcomp>
    example = {key: batch[key][i] for key in batch}
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 272, in __getitem__
    value = self.format(key)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 375, in format
    return self.formatter.format_column(self.pa_table.select([key]))
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 442, in format_column
    column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 218, in decode_column
    return self.features.decode_column(column, column_name) if self.features else column
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/features/features.py", line 1925, in decode_column
    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/features/features.py", line 1925, in <listcomp>
    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/features/features.py", line 1325, in decode_nested_example
    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/features/audio.py", line 187, in decode_example
    array, sampling_rate = sf.read(f)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/soundfile.py", line 285, in read
    with SoundFile(file, 'r', samplerate, channels,
  File "/home/yifanhua/.local/lib/python3.10/site-packages/soundfile.py", line 658, in __init__
    self._file = self._open(file, mode_int, closefd)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/soundfile.py", line 1216, in _open
    raise LibsndfileError(err, prefix="Error opening {0!r}: ".format(self.name))
soundfile.LibsndfileError: Error opening <_io.BufferedReader name='/home/yifanhua/.cache/huggingface/datasets/downloads/extracted/a62e68ef77daee3e83f95f22b1da75e786c3ec6a96e2f0e5c5288ea4b46ce202/clips/14dadaeff069012e7c585d95f43a504b52a592ea07ff1871caebd698ca80efbfa8a9bffb448fc93ff976516da5c4ee2841852095cca944ef56dc0abdf107fb0d.mp3'>: Format not recognised.


Start -- Nov 11 14:36:54 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
14:36:54	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
14:37:02	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
14:48:36	Finish Loading Remote Dataset. Length of dataset: 5
14:48:36	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 412, in <module>
    if constants.download_remote_dataset:
  File "/home/yifanhua/5455-term-project/src/main.py", line 286, in load_remote_dataset
    # TODO: Remove next line, we dont need to save dataset now. It's just for speeding up the process of debugging
AttributeError: 'NoneType' object has no attribute 'save_to_disk'


Start -- Nov 11 14:52:16 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
14:52:16	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
14:52:18	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
15:03:55	Finish Loading Remote Dataset. Length of dataset: 12135
15:03:56	Using Mozilla Common Voice. Additional Cleaning Needed
15:03:56	Starting Size of Mozilla Common Voice: 12135
15:04:35	Size after filtering accent: 8671
15:05:01	Size after filtering gender: 8637
15:05:28	Size after filtering voting: 8637
15:05:28	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 413, in <module>
    dataset = load_remote_dataset()
  File "/home/yifanhua/5455-term-project/src/main.py", line 288, in load_remote_dataset
    _dataset = clean_mozilla_dataset(_dataset)
  File "/home/yifanhua/5455-term-project/src/main.py", line 256, in clean_mozilla_dataset
    _dataset = _dataset.map(lambda entry: entry['sentence'].lower())
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3450, in _map_single
    example = apply_function_on_filtered_inputs(example, i, offset=offset)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3364, in apply_function_on_filtered_inputs
    validate_function_output(processed_inputs, indices)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3309, in validate_function_output
    raise TypeError(
TypeError: Provided `function` which is applied to all elements of table returns a variable of type <class 'str'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.


Start -- Nov 11 17:10:50 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
17:10:50	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
17:10:52	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
17:11:01	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 17:11:05 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
17:11:05	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
17:11:08	Using Locally Processed Dataset. Skip Processing...
17:11:08	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
17:11:09	Input text: I'm loading the model from the Hugging Face Hub!
17:11:09	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 440, in <module>
    speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0).to(device)
KeyError: 'speaker_embeddings'


Start -- Nov 11 17:11:49 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
17:11:49	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
17:11:52	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
17:23:25	Finish Loading Remote Dataset. Length of dataset: 12135
17:23:26	Using Mozilla Common Voice. Additional Cleaning Needed
17:23:26	Starting Size of Mozilla Common Voice: 12135
17:24:05	Size after filtering accent: 8671
17:24:32	Size after filtering gender: 8637
17:24:58	Size after filtering voting: 8637
17:24:59	Finish normalizing input text
17:24:59	Finish Cleaning Dataset. Length of dataset: 8637
17:24:59	Start Setting Sampling Rate to 16 kHz...
17:24:59	Setting Sampling Rate Successfully
17:24:59	Start Filtering Short Data
17:24:59	8635 data entries left after filtering
17:24:59	Start Preparing Dataset
17:27:40	Preparing dataset finished successfully
17:27:40	Start Filtering Long Data
17:27:40	8635 data entries left after filtering
17:27:40	save_processed_dataset is True. Saving processed dataset to dir: ./data/
17:27:41	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
17:27:42	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 11 19:11:38 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
19:11:38	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
19:11:40	Using Locally Processed Dataset. Skip Processing...
19:11:40	Using model: ./model/trained_yifanhua-threadripper_Nov10_21:39
19:11:41	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 11 19:21:50 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
19:21:50	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
19:21:53	Using Locally Processed Dataset. Skip Processing...
19:21:53	Generating Seq2SeqTrainer Arguments
19:21:53	Generating Seq2SeqTrainer
19:21:53	Start Training...
19:44:36	Start Saving the model...
19:44:41	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 11 19:57:56 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
19:57:56	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
19:57:59	Using Locally Processed Dataset. Skip Processing...
19:57:59	Generating Seq2SeqTrainer Arguments
19:57:59	Generating Seq2SeqTrainer
19:57:59	Start Training...
19:58:24	KeyboardInterrupt detected. Exiting...

Start -- Nov 11 19:58:37 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
19:58:37	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
19:58:39	Using Locally Processed Dataset. Skip Processing...
19:58:39	Generating Seq2SeqTrainer Arguments
19:58:39	Generating Seq2SeqTrainer
19:58:39	Start Training...
20:26:44	Start Saving the model...
20:26:49	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 13 15:54:00 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
15:54:00	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
15:54:03	Using Locally Processed Dataset. Skip Processing...
15:54:03	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 415, in <module>
    sort_speaker(dataset)
  File "/home/yifanhua/5455-term-project/src/main.py", line 313, in sort_speaker
    for speaker_id in _dataset["speaker_id"]:
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2803, in __getitem__
    return self._getitem(key)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2787, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 580, in query_table
    _check_valid_column_key(key, table.column_names)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 520, in _check_valid_column_key
    raise KeyError(f"Column {key} not in the dataset. Current columns in the dataset: {columns}")
KeyError: "Column speaker_id not in the dataset. Current columns in the dataset: ['input_ids', 'labels', 'speaker_embeddings']"


Start -- Nov 13 15:55:06 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
15:55:06	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
15:55:08	Using Locally Processed Dataset. Skip Processing...
15:55:08	Generating Seq2SeqTrainer Arguments
15:55:08	Generating Seq2SeqTrainer
15:55:08	Start Training...
15:55:33	KeyboardInterrupt detected. Exiting...

Start -- Nov 13 15:55:40 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
15:55:40	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
15:55:42	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
16:07:36	Finish Loading Remote Dataset. Length of dataset: 12135
16:07:36	Using Mozilla Common Voice. Additional Cleaning Needed
16:07:36	Starting Size of Mozilla Common Voice: 12135
16:08:18	Size after filtering accent: 8671
16:08:45	Size after filtering gender: 8637
16:09:12	Size after filtering voting: 8637
16:09:13	Finish normalizing input text
16:09:13	Finish Cleaning Dataset. Length of dataset: 8637
16:09:13	Start Setting Sampling Rate to 16 kHz...
16:09:13	Setting Sampling Rate Successfully
16:09:13	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 406, in <module>
    sort_speaker(dataset)
  File "/home/yifanhua/5455-term-project/src/main.py", line 313, in sort_speaker
    for speaker_id in _dataset["speaker_id"]:
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2803, in __getitem__
    return self._getitem(key)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2787, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 580, in query_table
    _check_valid_column_key(key, table.column_names)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 520, in _check_valid_column_key
    raise KeyError(f"Column {key} not in the dataset. Current columns in the dataset: {columns}")
KeyError: "Column speaker_id not in the dataset. Current columns in the dataset: ['client_id', 'path', 'audio', 'normalized_text', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment']"


Start -- Nov 13 18:47:37 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
18:47:37	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
18:47:39	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
18:59:17	Finish Loading Remote Dataset. Length of dataset: 12135
18:59:17	Using Mozilla Common Voice. Additional Cleaning Needed
18:59:17	Starting Size of Mozilla Common Voice: 12135
18:59:57	Size after filtering accent: 8671
19:00:24	Size after filtering gender: 8637
19:00:50	Size after filtering voting: 8637
19:00:51	Finish normalizing input text
19:00:51	Finish Cleaning Dataset. Length of dataset: 8637
19:00:51	Start Setting Sampling Rate to 16 kHz...
19:00:51	Setting Sampling Rate Successfully
19:00:52	Start Filtering Short Data
19:00:52	8635 data entries left after filtering
19:00:52	Start Preparing Dataset
19:03:39	Preparing dataset finished successfully
19:03:39	Start Filtering Long Data
19:03:39	8635 data entries left after filtering
19:03:39	save_processed_dataset is True. Saving processed dataset to dir: ./data/
19:03:40	Generating Seq2SeqTrainer Arguments
19:03:40	Generating Seq2SeqTrainer
19:03:41	Start Training...
19:19:45	KeyboardInterrupt detected. Exiting...

Start -- Nov 13 19:19:50 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
19:19:50	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
19:19:52	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
19:20:04	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 439, in <module>
    dataset = load_remote_dataset()
  File "/home/yifanhua/5455-term-project/src/main.py", line 272, in load_remote_dataset
    _dataset = load_dataset(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/load.py", line 2153, in load_dataset
    builder_instance.download_and_prepare(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/builder.py", line 954, in download_and_prepare
    self._download_and_prepare(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/builder.py", line 1717, in _download_and_prepare
    super()._download_and_prepare(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/datasets/builder.py", line 1027, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File "/home/yifanhua/.cache/huggingface/modules/datasets_modules/datasets/mozilla-foundation--common_voice_1_0/f01470c32b0b93cfcd4baaa7725f5ec0285549a5d086fab27151c3260e5c8035/common_voice_1_0.py", line 159, in _split_generators
    archive_path = dl_manager.download(self._get_bundle_url(self.config.name, bundle_url_template))
  File "/home/yifanhua/.cache/huggingface/modules/datasets_modules/datasets/mozilla-foundation--common_voice_1_0/f01470c32b0b93cfcd4baaa7725f5ec0285549a5d086fab27151c3260e5c8035/common_voice_1_0.py", line 135, in _get_bundle_url
    response = requests.get(f"{_API_URL}/bucket/dataset/{path}", timeout=10.0).json()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 900, in json
    return complexjson.loads(self.text, **kwargs)
  File "/usr/lib/python3.10/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.10/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.10/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 2 column 1 (char 1)


Start -- Nov 13 19:26:45 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
19:26:45	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
19:26:47	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
19:38:23	Finish Loading Remote Dataset. Length of dataset: 12135
19:38:23	Using Mozilla Common Voice. Additional Cleaning Needed
19:38:23	Starting Size of Mozilla Common Voice: 12135
19:39:02	Size after filtering accent: 8671
19:39:29	Size after filtering gender: 8637
19:39:55	Size after filtering voting: 8637
19:39:56	Finish normalizing input text
19:39:56	Finish Cleaning Dataset. Length of dataset: 8637
19:39:56	Start Setting Sampling Rate to 16 kHz...
19:39:56	Setting Sampling Rate Successfully
19:39:57	Start Filtering Short Data
19:39:57	8635 data entries left after filtering
19:39:57	Start Preparing Dataset
19:42:42	Preparing dataset finished successfully
19:42:42	Start Filtering Long Data
19:42:42	8635 data entries left after filtering
19:42:42	save_processed_dataset is True. Saving processed dataset to dir: ./data/
19:42:44	Generating Seq2SeqTrainer Arguments
19:42:44	Generating Seq2SeqTrainer
19:42:44	Start Training...
20:18:29	Start Saving the model...
20:18:34	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 14 09:13:00 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
09:13:00	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
09:13:05	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
09:24:58	Finish Loading Remote Dataset. Length of dataset: 12135
09:24:58	Using Mozilla Common Voice. Additional Cleaning Needed
09:24:58	Starting Size of Mozilla Common Voice: 12135
09:25:40	Size after filtering accent: 8671
09:26:07	Size after filtering gender: 8637
09:26:34	Size after filtering voting: 8637
09:26:35	Finish normalizing input text
09:26:35	Finish Cleaning Dataset. Length of dataset: 8637
09:26:35	Start Setting Sampling Rate to 16 kHz...
09:26:35	Setting Sampling Rate Successfully
09:26:36	Start Filtering Short Data
09:26:36	8635 data entries left after filtering
09:26:36	Start Preparing Dataset
09:29:20	Preparing dataset finished successfully
09:29:20	Start Filtering Long Data
09:29:20	8635 data entries left after filtering
09:29:20	save_processed_dataset is True. Saving processed dataset to dir: ./data/
09:29:21	Generating Seq2SeqTrainer Arguments
09:29:21	Generating Seq2SeqTrainer
09:29:21	Exception: Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 169, in _new_conn
    conn = connection.create_connection(
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 73, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "/usr/lib/python3.10/socket.py", line 955, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 700, in urlopen
    httplib_response = self._make_request(
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 383, in _make_request
    self._validate_conn(conn)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 1017, in _validate_conn
    conn.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 353, in connect
    conn = self._new_conn()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 181, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f77eb955060>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 439, in send
    resp = conn.urlopen(
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 756, in urlopen
    retries = retries.increment(
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 574, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/repos/create (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f77eb955060>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 461, in <module>
    trainer = generate_trainer(training_args, pretrained_model, divided_dataset, data_collator, tokenizer)
  File "/home/yifanhua/5455-term-project/src/main.py", line 425, in generate_trainer
    return Seq2SeqTrainer(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 56, in __init__
    super().__init__(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/trainer.py", line 536, in __init__
    self.init_hf_repo()
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3480, in init_hf_repo
    repo_url = create_repo(repo_name, token=self.args.hub_token, private=self.args.hub_private_repo, exist_ok=True)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py", line 3053, in create_repo
    r = get_session().post(path, headers=headers, json=json)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 592, in post
    return self.request('POST', url, data=data, json=json, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 544, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 657, in send
    r = adapter.send(request, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/repos/create (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f77eb955060>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))"), '(Request ID: 066c2541-fa49-48d2-8041-8d560bde5683)')


Start -- Nov 14 10:10:48 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:10:48	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:10:50	Using Locally Processed Dataset. Skip Processing...
10:10:50	Generating Seq2SeqTrainer Arguments
10:10:50	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 467, in <module>
    training_args = generate_train_arguments()
  File "/home/yifanhua/5455-term-project/src/main.py", line 406, in generate_train_arguments
    return Seq2SeqTrainingArguments(
TypeError: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'log'


Start -- Nov 14 10:11:02 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:11:02	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:11:04	Using Locally Processed Dataset. Skip Processing...
10:11:04	Generating Seq2SeqTrainer Arguments
10:11:04	Generating Seq2SeqTrainer
10:11:04	Start Training...
10:14:38	KeyboardInterrupt detected. Exiting...

Start -- Nov 14 10:18:54 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:18:54	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:18:56	Using Locally Processed Dataset. Skip Processing...
10:18:56	Generating Seq2SeqTrainer Arguments
10:18:56	Generating Seq2SeqTrainer
10:18:56	Start Training...
10:25:02	KeyboardInterrupt detected. Exiting...

Start -- Nov 14 10:25:05 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:25:05	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:25:07	Using Locally Processed Dataset. Skip Processing...
10:25:07	Generating Seq2SeqTrainer Arguments
10:25:07	Generating Seq2SeqTrainer
10:25:07	Start Training...
10:25:15	KeyboardInterrupt detected. Exiting...

Start -- Nov 14 10:25:19 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:25:19	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:25:21	Using Locally Processed Dataset. Skip Processing...
10:25:21	Generating Seq2SeqTrainer Arguments
10:25:21	Generating Seq2SeqTrainer
10:25:21	Start Training...
10:26:20	KeyboardInterrupt detected. Exiting...

Start -- Nov 14 10:26:24 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:26:24	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
loading configuration file config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/config.json
Model config SpeechT5Config {
  "activation_dropout": 0.1,
  "apply_spec_augment": true,
  "architectures": [
    "SpeechT5ForTextToSpeech"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "conv_bias": false,
  "conv_dim": [
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "conv_kernel": [
    10,
    3,
    3,
    3,
    3,
    2,
    2
  ],
  "conv_stride": [
    5,
    2,
    2,
    2,
    2,
    2,
    2
  ],
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.1,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.1,
  "encoder_layers": 12,
  "encoder_max_relative_position": 160,
  "eos_token_id": 2,
  "feat_extract_activation": "gelu",
  "feat_extract_norm": "group",
  "feat_proj_dropout": 0.0,
  "guided_attention_loss_num_heads": 2,
  "guided_attention_loss_scale": 10.0,
  "guided_attention_loss_sigma": 0.4,
  "hidden_act": "gelu",
  "hidden_dropout": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "is_encoder_decoder": true,
  "layer_norm_eps": 1e-05,
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 1876,
  "max_speech_positions": 1876,
  "max_text_positions": 600,
  "model_type": "speecht5",
  "num_conv_pos_embedding_groups": 16,
  "num_conv_pos_embeddings": 128,
  "num_feat_extract_layers": 7,
  "num_mel_bins": 80,
  "pad_token_id": 1,
  "positional_dropout": 0.1,
  "reduction_factor": 2,
  "scale_embedding": false,
  "speaker_embedding_dim": 512,
  "speech_decoder_postnet_dropout": 0.5,
  "speech_decoder_postnet_kernel": 5,
  "speech_decoder_postnet_layers": 5,
  "speech_decoder_postnet_units": 256,
  "speech_decoder_prenet_dropout": 0.5,
  "speech_decoder_prenet_layers": 2,
  "speech_decoder_prenet_units": 256,
  "torch_dtype": "float32",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "use_guided_attention_loss": true,
  "vocab_size": 81
}

loading weights file pytorch_model.bin from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/pytorch_model.bin
Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "eos_token_id": 2,
  "max_length": 1876,
  "pad_token_id": 1
}

All model checkpoint weights were used when initializing SpeechT5ForTextToSpeech.

All the weights of SpeechT5ForTextToSpeech were initialized from the model checkpoint at microsoft/speecht5_tts.
If your task is similar to the task the model of the checkpoint was trained on, you can already use SpeechT5ForTextToSpeech for predictions without further training.
Generation config file not found, using a generation config created from the model config.
loading configuration file preprocessor_config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/preprocessor_config.json
Feature extractor SpeechT5FeatureExtractor {
  "do_normalize": false,
  "feature_extractor_type": "SpeechT5FeatureExtractor",
  "feature_size": 1,
  "fmax": 7600,
  "fmin": 80,
  "frame_signal_scale": 1.0,
  "hop_length": 16,
  "mel_floor": 1e-10,
  "num_mel_bins": 80,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "SpeechT5Processor",
  "reduction_factor": 2,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "win_function": "hann_window",
  "win_length": 64
}

loading file spm_char.model from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/spm_char.model
loading file added_tokens.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/added_tokens.json
loading file special_tokens_map.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/tokenizer_config.json
loading file tokenizer.json from cache at None
loading configuration file config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_hifigan/snapshots/bb6f429406e86a9992357a972c0698b22043307d/config.json
Model config SpeechT5HifiGanConfig {
  "architectures": [
    "SpeechT5HifiGan"
  ],
  "initializer_range": 0.01,
  "leaky_relu_slope": 0.1,
  "model_in_dim": 80,
  "model_type": "hifigan",
  "normalize_before": true,
  "resblock_dilation_sizes": [
    [
      1,
      3,
      5
    ],
    [
      1,
      3,
      5
    ],
    [
      1,
      3,
      5
    ]
  ],
  "resblock_kernel_sizes": [
    3,
    7,
    11
  ],
  "sampling_rate": 16000,
  "torch_dtype": "float32",
  "transformers_version": "4.36.0.dev0",
  "upsample_initial_channel": 512,
  "upsample_kernel_sizes": [
    8,
    8,
    8,
    8
  ],
  "upsample_rates": [
    4,
    4,
    4,
    4
  ]
}

loading weights file pytorch_model.bin from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_hifigan/snapshots/bb6f429406e86a9992357a972c0698b22043307d/pytorch_model.bin
All model checkpoint weights were used when initializing SpeechT5HifiGan.

All the weights of SpeechT5HifiGan were initialized from the model checkpoint at microsoft/speecht5_hifigan.
If your task is similar to the task the model of the checkpoint was trained on, you can already use SpeechT5HifiGan for predictions without further training.
10:26:26	Using Locally Processed Dataset. Skip Processing...
10:26:26	Generating Seq2SeqTrainer Arguments
PyTorch: setting up devices
10:26:26	Generating Seq2SeqTrainer
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
10:26:26	Start Training...
Currently training with a batch size of: 8
***** Running training *****
  Num examples = 2,000
  Num Epochs = 32
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 4,000
  Number of trainable parameters = 144,431,684
10:27:11	KeyboardInterrupt detected. Exiting...

Start -- Nov 14 10:27:14 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:27:14	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
loading configuration file config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/config.json
Model config SpeechT5Config {
  "activation_dropout": 0.1,
  "apply_spec_augment": true,
  "architectures": [
    "SpeechT5ForTextToSpeech"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "conv_bias": false,
  "conv_dim": [
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "conv_kernel": [
    10,
    3,
    3,
    3,
    3,
    2,
    2
  ],
  "conv_stride": [
    5,
    2,
    2,
    2,
    2,
    2,
    2
  ],
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.1,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.1,
  "encoder_layers": 12,
  "encoder_max_relative_position": 160,
  "eos_token_id": 2,
  "feat_extract_activation": "gelu",
  "feat_extract_norm": "group",
  "feat_proj_dropout": 0.0,
  "guided_attention_loss_num_heads": 2,
  "guided_attention_loss_scale": 10.0,
  "guided_attention_loss_sigma": 0.4,
  "hidden_act": "gelu",
  "hidden_dropout": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "is_encoder_decoder": true,
  "layer_norm_eps": 1e-05,
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 1876,
  "max_speech_positions": 1876,
  "max_text_positions": 600,
  "model_type": "speecht5",
  "num_conv_pos_embedding_groups": 16,
  "num_conv_pos_embeddings": 128,
  "num_feat_extract_layers": 7,
  "num_mel_bins": 80,
  "pad_token_id": 1,
  "positional_dropout": 0.1,
  "reduction_factor": 2,
  "scale_embedding": false,
  "speaker_embedding_dim": 512,
  "speech_decoder_postnet_dropout": 0.5,
  "speech_decoder_postnet_kernel": 5,
  "speech_decoder_postnet_layers": 5,
  "speech_decoder_postnet_units": 256,
  "speech_decoder_prenet_dropout": 0.5,
  "speech_decoder_prenet_layers": 2,
  "speech_decoder_prenet_units": 256,
  "torch_dtype": "float32",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "use_guided_attention_loss": true,
  "vocab_size": 81
}

loading weights file pytorch_model.bin from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/pytorch_model.bin
Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "eos_token_id": 2,
  "max_length": 1876,
  "pad_token_id": 1
}

All model checkpoint weights were used when initializing SpeechT5ForTextToSpeech.

All the weights of SpeechT5ForTextToSpeech were initialized from the model checkpoint at microsoft/speecht5_tts.
If your task is similar to the task the model of the checkpoint was trained on, you can already use SpeechT5ForTextToSpeech for predictions without further training.
Generation config file not found, using a generation config created from the model config.
loading configuration file preprocessor_config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/preprocessor_config.json
Feature extractor SpeechT5FeatureExtractor {
  "do_normalize": false,
  "feature_extractor_type": "SpeechT5FeatureExtractor",
  "feature_size": 1,
  "fmax": 7600,
  "fmin": 80,
  "frame_signal_scale": 1.0,
  "hop_length": 16,
  "mel_floor": 1e-10,
  "num_mel_bins": 80,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "SpeechT5Processor",
  "reduction_factor": 2,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "win_function": "hann_window",
  "win_length": 64
}

loading file spm_char.model from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/spm_char.model
loading file added_tokens.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/added_tokens.json
loading file special_tokens_map.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/tokenizer_config.json
loading file tokenizer.json from cache at None
loading configuration file config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_hifigan/snapshots/bb6f429406e86a9992357a972c0698b22043307d/config.json
Model config SpeechT5HifiGanConfig {
  "architectures": [
    "SpeechT5HifiGan"
  ],
  "initializer_range": 0.01,
  "leaky_relu_slope": 0.1,
  "model_in_dim": 80,
  "model_type": "hifigan",
  "normalize_before": true,
  "resblock_dilation_sizes": [
    [
      1,
      3,
      5
    ],
    [
      1,
      3,
      5
    ],
    [
      1,
      3,
      5
    ]
  ],
  "resblock_kernel_sizes": [
    3,
    7,
    11
  ],
  "sampling_rate": 16000,
  "torch_dtype": "float32",
  "transformers_version": "4.36.0.dev0",
  "upsample_initial_channel": 512,
  "upsample_kernel_sizes": [
    8,
    8,
    8,
    8
  ],
  "upsample_rates": [
    4,
    4,
    4,
    4
  ]
}

loading weights file pytorch_model.bin from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_hifigan/snapshots/bb6f429406e86a9992357a972c0698b22043307d/pytorch_model.bin
All model checkpoint weights were used when initializing SpeechT5HifiGan.

All the weights of SpeechT5HifiGan were initialized from the model checkpoint at microsoft/speecht5_hifigan.
If your task is similar to the task the model of the checkpoint was trained on, you can already use SpeechT5HifiGan for predictions without further training.
10:27:17	Using Locally Processed Dataset. Skip Processing...
10:27:17	Generating Seq2SeqTrainer Arguments
PyTorch: setting up devices
10:27:17	Generating Seq2SeqTrainer
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
10:27:17	Start Training...
Currently training with a batch size of: 8
***** Running training *****
  Num examples = 2,000
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 100
  Number of trainable parameters = 144,431,684
***** Running Evaluation *****
  Num examples = 5
  Batch size = 8
Saving model checkpoint to ./speecht5_tts/checkpoint-100
Configuration saved in ./speecht5_tts/checkpoint-100/config.json
Configuration saved in ./speecht5_tts/checkpoint-100/generation_config.json
Model weights saved in ./speecht5_tts/checkpoint-100/pytorch_model.bin
tokenizer config file saved in ./speecht5_tts/checkpoint-100/tokenizer_config.json
Special tokens file saved in ./speecht5_tts/checkpoint-100/special_tokens_map.json
added tokens file saved in ./speecht5_tts/checkpoint-100/added_tokens.json
tokenizer config file saved in ./speecht5_tts/tokenizer_config.json
Special tokens file saved in ./speecht5_tts/special_tokens_map.json
added tokens file saved in ./speecht5_tts/added_tokens.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./speecht5_tts/checkpoint-100 (score: 0.6336874961853027).
Waiting for the current checkpoint push to be finished, this might take a couple of minutes.
10:28:32	Start Saving the model...
Saving model checkpoint to ./model/trained_yifanhua-threadripper_Nov14_10:27
Configuration saved in ./model/trained_yifanhua-threadripper_Nov14_10:27/config.json
Configuration saved in ./model/trained_yifanhua-threadripper_Nov14_10:27/generation_config.json
Model weights saved in ./model/trained_yifanhua-threadripper_Nov14_10:27/pytorch_model.bin
tokenizer config file saved in ./model/trained_yifanhua-threadripper_Nov14_10:27/tokenizer_config.json
Special tokens file saved in ./model/trained_yifanhua-threadripper_Nov14_10:27/special_tokens_map.json
added tokens file saved in ./model/trained_yifanhua-threadripper_Nov14_10:27/added_tokens.json
Saving model checkpoint to ./speecht5_tts/
Configuration saved in ./speecht5_tts/config.json
Configuration saved in ./speecht5_tts/generation_config.json
Model weights saved in ./speecht5_tts/pytorch_model.bin
tokenizer config file saved in ./speecht5_tts/tokenizer_config.json
Special tokens file saved in ./speecht5_tts/special_tokens_map.json
added tokens file saved in ./speecht5_tts/added_tokens.json
Dropping the following result as it does not have all the necessary fields:
{'dataset': {'name': 'common_voice_1_0', 'type': 'common_voice_1_0', 'config': 'en', 'split': 'train', 'args': 'en'}}
Saving model checkpoint to ./speecht5_tts/
Configuration saved in ./speecht5_tts/config.json
Configuration saved in ./speecht5_tts/generation_config.json
Model weights saved in ./speecht5_tts/pytorch_model.bin
tokenizer config file saved in ./speecht5_tts/tokenizer_config.json
Special tokens file saved in ./speecht5_tts/special_tokens_map.json
added tokens file saved in ./speecht5_tts/added_tokens.json
Dropping the following result as it does not have all the necessary fields:
{'dataset': {'name': 'Common Voice', 'type': 'mozilla-foundation/common_voice_1_0', 'config': 'en', 'split': 'train', 'args': 'config: en'}}
10:28:38	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 14 10:32:36 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:32:36	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
loading configuration file config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/config.json
Model config SpeechT5Config {
  "activation_dropout": 0.1,
  "apply_spec_augment": true,
  "architectures": [
    "SpeechT5ForTextToSpeech"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "conv_bias": false,
  "conv_dim": [
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "conv_kernel": [
    10,
    3,
    3,
    3,
    3,
    2,
    2
  ],
  "conv_stride": [
    5,
    2,
    2,
    2,
    2,
    2,
    2
  ],
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.1,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.1,
  "encoder_layers": 12,
  "encoder_max_relative_position": 160,
  "eos_token_id": 2,
  "feat_extract_activation": "gelu",
  "feat_extract_norm": "group",
  "feat_proj_dropout": 0.0,
  "guided_attention_loss_num_heads": 2,
  "guided_attention_loss_scale": 10.0,
  "guided_attention_loss_sigma": 0.4,
  "hidden_act": "gelu",
  "hidden_dropout": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "is_encoder_decoder": true,
  "layer_norm_eps": 1e-05,
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 1876,
  "max_speech_positions": 1876,
  "max_text_positions": 600,
  "model_type": "speecht5",
  "num_conv_pos_embedding_groups": 16,
  "num_conv_pos_embeddings": 128,
  "num_feat_extract_layers": 7,
  "num_mel_bins": 80,
  "pad_token_id": 1,
  "positional_dropout": 0.1,
  "reduction_factor": 2,
  "scale_embedding": false,
  "speaker_embedding_dim": 512,
  "speech_decoder_postnet_dropout": 0.5,
  "speech_decoder_postnet_kernel": 5,
  "speech_decoder_postnet_layers": 5,
  "speech_decoder_postnet_units": 256,
  "speech_decoder_prenet_dropout": 0.5,
  "speech_decoder_prenet_layers": 2,
  "speech_decoder_prenet_units": 256,
  "torch_dtype": "float32",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "use_guided_attention_loss": true,
  "vocab_size": 81
}

loading weights file pytorch_model.bin from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/pytorch_model.bin
Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "eos_token_id": 2,
  "max_length": 1876,
  "pad_token_id": 1
}

All model checkpoint weights were used when initializing SpeechT5ForTextToSpeech.

All the weights of SpeechT5ForTextToSpeech were initialized from the model checkpoint at microsoft/speecht5_tts.
If your task is similar to the task the model of the checkpoint was trained on, you can already use SpeechT5ForTextToSpeech for predictions without further training.
Generation config file not found, using a generation config created from the model config.
loading configuration file preprocessor_config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/preprocessor_config.json
Feature extractor SpeechT5FeatureExtractor {
  "do_normalize": false,
  "feature_extractor_type": "SpeechT5FeatureExtractor",
  "feature_size": 1,
  "fmax": 7600,
  "fmin": 80,
  "frame_signal_scale": 1.0,
  "hop_length": 16,
  "mel_floor": 1e-10,
  "num_mel_bins": 80,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "SpeechT5Processor",
  "reduction_factor": 2,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "win_function": "hann_window",
  "win_length": 64
}

loading file spm_char.model from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/spm_char.model
loading file added_tokens.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/added_tokens.json
loading file special_tokens_map.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_tts/snapshots/30fcde30f19b87502b8435427b5f5068e401d5f6/tokenizer_config.json
loading file tokenizer.json from cache at None
loading configuration file config.json from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_hifigan/snapshots/bb6f429406e86a9992357a972c0698b22043307d/config.json
Model config SpeechT5HifiGanConfig {
  "architectures": [
    "SpeechT5HifiGan"
  ],
  "initializer_range": 0.01,
  "leaky_relu_slope": 0.1,
  "model_in_dim": 80,
  "model_type": "hifigan",
  "normalize_before": true,
  "resblock_dilation_sizes": [
    [
      1,
      3,
      5
    ],
    [
      1,
      3,
      5
    ],
    [
      1,
      3,
      5
    ]
  ],
  "resblock_kernel_sizes": [
    3,
    7,
    11
  ],
  "sampling_rate": 16000,
  "torch_dtype": "float32",
  "transformers_version": "4.36.0.dev0",
  "upsample_initial_channel": 512,
  "upsample_kernel_sizes": [
    8,
    8,
    8,
    8
  ],
  "upsample_rates": [
    4,
    4,
    4,
    4
  ]
}

loading weights file pytorch_model.bin from cache at /home/yifanhua/.cache/huggingface/hub/models--microsoft--speecht5_hifigan/snapshots/bb6f429406e86a9992357a972c0698b22043307d/pytorch_model.bin
All model checkpoint weights were used when initializing SpeechT5HifiGan.

All the weights of SpeechT5HifiGan were initialized from the model checkpoint at microsoft/speecht5_hifigan.
If your task is similar to the task the model of the checkpoint was trained on, you can already use SpeechT5HifiGan for predictions without further training.
10:32:38	Using Locally Processed Dataset. Skip Processing...
10:32:38	Generating Seq2SeqTrainer Arguments
10:32:38	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 468, in <module>
    training_args = generate_train_arguments()
  File "/home/yifanhua/5455-term-project/src/main.py", line 408, in generate_train_arguments
    return Seq2SeqTrainingArguments(
TypeError: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'callbacks'


Start -- Nov 14 10:56:17 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:56:17	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:56:19	Using Locally Processed Dataset. Skip Processing...
10:56:19	Generating Seq2SeqTrainer Arguments
10:56:19	Generating Seq2SeqTrainer
10:56:20	Start Training...
10:57:33	Start Saving the model...
10:57:38	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 14 12:24:04 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:24:04	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:24:07	Using Locally Processed Dataset. Skip Processing...
12:24:07	Generating Seq2SeqTrainer Arguments
12:24:07	Generating Seq2SeqTrainer
12:24:07	Start Training...
12:25:16	Start Saving the model...
12:25:21	Input text: I'm loading the model from the Hugging Face Hub!


Start -- Nov 14 12:30:50 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:30:50	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:30:53	Using Locally Processed Dataset. Skip Processing...
12:30:53	Generating Seq2SeqTrainer Arguments
12:30:53	Generating Seq2SeqTrainer
12:30:53	Start Training...
12:32:04	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 471, in <module>
    log_msg(trainer.state.log_history)
  File "/home/yifanhua/5455-term-project/src/main.py", line 32, in log_msg
    msg = time.strftime("%H:%M:%S", time.localtime()) + '\t' + m if include_time else m
TypeError: can only concatenate str (not "dict") to str


Start -- Nov 14 12:33:09 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:33:09	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:33:12	Using Locally Processed Dataset. Skip Processing...
12:33:12	Generating Seq2SeqTrainer Arguments
12:33:12	Generating Seq2SeqTrainer
12:33:12	Start Training...
12:34:28	{'loss': 0.9859, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.04, 'step': 5}
12:34:28	{'loss': 0.8889, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.08, 'step': 10}
12:34:28	{'loss': 0.9105, 'learning_rate': 8.6e-06, 'epoch': 0.12, 'step': 15}
12:34:28	{'loss': 0.8511, 'learning_rate': 8.1e-06, 'epoch': 0.16, 'step': 20}
12:34:28	{'loss': 0.8137, 'learning_rate': 7.7e-06, 'epoch': 0.2, 'step': 25}
12:34:28	{'loss': 0.8476, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.24, 'step': 30}
12:34:28	{'loss': 0.8663, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.28, 'step': 35}
12:34:28	{'loss': 0.8475, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.32, 'step': 40}
12:34:28	{'loss': 0.8243, 'learning_rate': 5.7e-06, 'epoch': 0.36, 'step': 45}
12:34:28	{'loss': 0.8349, 'learning_rate': 5.2e-06, 'epoch': 0.4, 'step': 50}
12:34:28	{'loss': 0.7796, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.44, 'step': 55}
12:34:28	{'loss': 0.7706, 'learning_rate': 4.3e-06, 'epoch': 0.48, 'step': 60}
12:34:28	{'loss': 0.8132, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.52, 'step': 65}
12:34:28	{'loss': 0.8094, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.56, 'step': 70}
12:34:28	{'loss': 0.7413, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.6, 'step': 75}
12:34:28	{'loss': 0.8326, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.64, 'step': 80}
12:34:28	{'loss': 0.7873, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.68, 'step': 85}
12:34:28	{'loss': 0.789, 'learning_rate': 1.3e-06, 'epoch': 0.72, 'step': 90}
12:34:28	{'loss': 0.8452, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.76, 'step': 95}
12:34:28	{'loss': 0.8092, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.8, 'step': 100}
12:34:28	{'eval_loss': 0.744220495223999, 'eval_runtime': 0.0774, 'eval_samples_per_second': 64.586, 'eval_steps_per_second': 12.917, 'epoch': 0.8, 'step': 100}
12:34:28	{'train_runtime': 47.6549, 'train_samples_per_second': 33.575, 'train_steps_per_second': 2.098, 'total_flos': 114345191087424.0, 'train_loss': 0.8323853802680969, 'epoch': 0.8, 'step': 100}
12:34:28	Start Saving the model...
12:34:33	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 14 12:34:59 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:34:59	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:35:02	Using Locally Processed Dataset. Skip Processing...
12:35:02	Generating Seq2SeqTrainer Arguments
12:35:02	Generating Seq2SeqTrainer
12:35:02	Start Training...
{'loss': 0.9372, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.04, 'step': 5}
{'loss': 0.9539, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.08, 'step': 10}
{'loss': 0.9213, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.12, 'step': 15}
{'loss': 0.874, 'learning_rate': 8.2e-06, 'epoch': 0.16, 'step': 20}
{'loss': 0.8497, 'learning_rate': 7.7e-06, 'epoch': 0.2, 'step': 25}
{'loss': 0.8623, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.24, 'step': 30}
{'loss': 0.7895, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.28, 'step': 35}
{'loss': 0.8644, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.32, 'step': 40}
{'loss': 0.8408, 'learning_rate': 5.7e-06, 'epoch': 0.36, 'step': 45}
{'loss': 0.7933, 'learning_rate': 5.2e-06, 'epoch': 0.4, 'step': 50}
{'loss': 0.7975, 'learning_rate': 4.7e-06, 'epoch': 0.44, 'step': 55}
{'loss': 0.7626, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.48, 'step': 60}
{'loss': 0.8135, 'learning_rate': 3.7e-06, 'epoch': 0.52, 'step': 65}
{'loss': 0.7836, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.56, 'step': 70}
{'loss': 0.762, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.6, 'step': 75}
{'loss': 0.8379, 'learning_rate': 2.2e-06, 'epoch': 0.64, 'step': 80}
{'loss': 0.738, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.68, 'step': 85}
{'loss': 0.7998, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.72, 'step': 90}
{'loss': 0.7802, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.76, 'step': 95}
{'loss': 0.7681, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.8, 'step': 100}
{'eval_loss': 0.6385095715522766, 'eval_runtime': 0.0999, 'eval_samples_per_second': 50.062, 'eval_steps_per_second': 10.012, 'epoch': 0.8, 'step': 100}
{'train_runtime': 47.1112, 'train_samples_per_second': 33.962, 'train_steps_per_second': 2.123, 'total_flos': 115758656456256.0, 'train_loss': 0.8264809226989747, 'epoch': 0.8, 'step': 100}
12:36:15	Start Saving the model...
12:36:23	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 14 13:36:01 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
13:36:01	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
13:36:03	Using Locally Processed Dataset. Skip Processing...
13:37:29	Using model: ./model/trained_yifanhua-threadripper_Nov14_12:24
13:37:30	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 14 13:38:25 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
13:38:25	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
13:38:28	Using Locally Processed Dataset. Skip Processing...
13:38:35	Using model: ./model/trained_yifanhua-threadripper_Nov13_19:26
13:38:36	Input text: I'm loading the model from the Hugging Face Hub!
18:02:05	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
18:13:36	Finish Loading Remote Dataset. Length of dataset: 12135
18:13:36	Using Mozilla Common Voice. Additional Cleaning Needed
18:13:36	Starting Size of Mozilla Common Voice: 12135
18:14:17	Size after filtering accent: 8671
18:14:44	Size after filtering gender: 8637
18:15:09	Size after filtering voting: 8637
18:15:10	Finish normalizing input text
18:15:10	Finish Cleaning Dataset. Length of dataset: 8637
18:15:10	Start Setting Sampling Rate to 16 kHz...
18:15:10	Setting Sampling Rate Successfully
19:19:17	Loading Remote Dataset: mozilla-foundation/common_voice_1_0. Sub-collection: en
19:30:51	Finish Loading Remote Dataset. Length of dataset: 12135
19:30:51	Using Mozilla Common Voice. Additional Cleaning Needed
19:30:51	Starting Size of Mozilla Common Voice: 12135
19:31:32	Size after filtering accent: 8671
19:31:58	Size after filtering gender: 8637
19:32:23	Size after filtering voting: 8637
19:32:24	Finish normalizing input text
19:32:24	Finish Cleaning Dataset. Length of dataset: 8637
19:32:24	Start Setting Sampling Rate to 16 kHz...
19:32:24	Setting Sampling Rate Successfully
19:32:25	Start Setting Sampling Rate to 16 kHz...
19:32:25	Setting Sampling Rate Successfully
20:28:01	Start Setting Sampling Rate to 16 kHz...
20:28:01	Setting Sampling Rate Successfully
20:45:18	Start Setting Sampling Rate to 16 kHz...
20:45:18	Setting Sampling Rate Successfully
20:46:56	Start Setting Sampling Rate to 16 kHz...
20:46:56	Setting Sampling Rate Successfully
20:47:55	Start Setting Sampling Rate to 16 kHz...
20:47:55	Setting Sampling Rate Successfully
20:48:28	Start Setting Sampling Rate to 16 kHz...
20:48:28	Setting Sampling Rate Successfully
20:55:09	Start Setting Sampling Rate to 16 kHz...
20:55:09	Setting Sampling Rate Successfully
20:55:58	Start Setting Sampling Rate to 16 kHz...
20:55:58	Setting Sampling Rate Successfully
20:56:39	Start Setting Sampling Rate to 16 kHz...
20:56:39	Setting Sampling Rate Successfully
20:57:10	Start Setting Sampling Rate to 16 kHz...
20:57:10	Setting Sampling Rate Successfully
20:58:09	Start Setting Sampling Rate to 16 kHz...
20:58:09	Setting Sampling Rate Successfully
20:59:50	Start Setting Sampling Rate to 16 kHz...
20:59:50	Setting Sampling Rate Successfully
21:00:30	Start Setting Sampling Rate to 16 kHz...
21:00:30	Setting Sampling Rate Successfully
21:02:35	Start Setting Sampling Rate to 16 kHz...
21:02:35	Setting Sampling Rate Successfully
21:05:19	Start Setting Sampling Rate to 16 kHz...
21:05:19	Setting Sampling Rate Successfully
21:09:08	Start Setting Sampling Rate to 16 kHz...
21:09:08	Setting Sampling Rate Successfully
21:10:09	Start Setting Sampling Rate to 16 kHz...
21:10:09	Setting Sampling Rate Successfully
21:10:25	Start Setting Sampling Rate to 16 kHz...
21:10:25	Setting Sampling Rate Successfully
21:12:37	Start Setting Sampling Rate to 16 kHz...
21:12:37	Setting Sampling Rate Successfully
21:13:49	Start Setting Sampling Rate to 16 kHz...
21:13:49	Setting Sampling Rate Successfully
21:21:56	Start Setting Sampling Rate to 16 kHz...
21:21:56	Setting Sampling Rate Successfully
21:22:05	Start Setting Sampling Rate to 16 kHz...
21:22:05	Setting Sampling Rate Successfully
21:24:42	Start Setting Sampling Rate to 16 kHz...
21:24:42	Setting Sampling Rate Successfully
21:25:03	Start Setting Sampling Rate to 16 kHz...
21:25:03	Setting Sampling Rate Successfully
21:25:52	Start Setting Sampling Rate to 16 kHz...
21:25:52	Setting Sampling Rate Successfully
21:27:08	Start Setting Sampling Rate to 16 kHz...
21:27:08	Setting Sampling Rate Successfully
21:27:48	Start Setting Sampling Rate to 16 kHz...
21:27:48	Setting Sampling Rate Successfully
21:30:01	Start Setting Sampling Rate to 16 kHz...
21:30:01	Setting Sampling Rate Successfully
07:40:06	Start Setting Sampling Rate to 16 kHz...
07:40:06	Setting Sampling Rate Successfully
07:40:26	Start Setting Sampling Rate to 16 kHz...
07:40:26	Setting Sampling Rate Successfully

Start -- Nov 15 09:53:25 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
09:53:25	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
09:53:28	Using Locally Processed Dataset. Skip Processing...
09:53:30	Using model: ./model/trained_yifanhua-threadripper_Nov11_19:58
09:53:32	Using model: ./model/trained_yifanhua-threadripper_Nov11_19:58
09:53:33	Input text: I'm loading the model from the Hugging Face Hub!
09:53:33	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 516, in <module>
    spectrogram = pretrained_model.generate_speech(inputs["input_ids"], speaker_embeddings).to(device)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2824, in generate_speech
    return _generate_speech(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2509, in _generate_speech
    decoder_hidden_states = model.speecht5.decoder.prenet(output_sequence, speaker_embeddings)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 698, in forward
    inputs_embeds = torch.cat([inputs_embeds, speaker_embeddings], dim=-1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)


Start -- Nov 15 10:01:03 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:01:03	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:01:05	Using Locally Processed Dataset. Skip Processing...
10:01:08	Using model: ./model/trained_yifanhua-threadripper_Nov11_19:58
10:01:09	Input text: I'm loading the model from the Hugging Face Hub!
10:01:10	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 517, in <module>
    spectrogram = pretrained_model.generate_speech(inputs["input_ids"], speaker_embeddings).to(device)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2824, in generate_speech
    return _generate_speech(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2509, in _generate_speech
    decoder_hidden_states = model.speecht5.decoder.prenet(output_sequence, speaker_embeddings)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 698, in forward
    inputs_embeds = torch.cat([inputs_embeds, speaker_embeddings], dim=-1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)


Start -- Nov 15 10:02:48 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:02:48	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:02:51	Using Locally Processed Dataset. Skip Processing...
10:02:51	Using model: ./model/trained_yifanhua-threadripper_Nov11_19:58
10:02:52	Input text: I'm loading the model from the Hugging Face Hub!
10:02:52	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 517, in <module>
    spectrogram = pretrained_model.generate_speech(inputs["input_ids"], speaker_embeddings).to(device)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2824, in generate_speech
    return _generate_speech(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2509, in _generate_speech
    decoder_hidden_states = model.speecht5.decoder.prenet(output_sequence, speaker_embeddings)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 698, in forward
    inputs_embeds = torch.cat([inputs_embeds, speaker_embeddings], dim=-1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)


Start -- Nov 15 10:05:38 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:05:38	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:05:41	Using Locally Processed Dataset. Skip Processing...
10:05:42	Using model: ./model/trained_yifanhua-threadripper_Nov14_12:24
10:05:43	Input text: I'm loading the model from the Hugging Face Hub!

Start -- Nov 15 10:17:16 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:17:16	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:17:20	Using Locally Processed Dataset. Skip Processing...
10:17:20	Generating Seq2SeqTrainer Arguments
10:17:20	Generating Seq2SeqTrainer
10:17:20	Start Training...
10:18:29	KeyboardInterrupt detected. Exiting...

Start -- Nov 15 10:18:47 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:18:47	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:18:49	Using Locally Processed Dataset. Skip Processing...
10:18:49	Generating Seq2SeqTrainer Arguments
10:18:49	Generating Seq2SeqTrainer
10:18:50	Start Training...
10:19:11	KeyboardInterrupt detected. Exiting...

Start -- Nov 15 10:19:16 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:19:16	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:19:18	Using Locally Processed Dataset. Skip Processing...
10:19:18	Generating Seq2SeqTrainer Arguments
10:19:18	Generating Seq2SeqTrainer
10:19:18	Start Training...
10:19:54	KeyboardInterrupt detected. Exiting...

Start -- Nov 15 10:20:00 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:20:00	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:20:02	Using Locally Processed Dataset. Skip Processing...
10:20:02	Generating Seq2SeqTrainer Arguments
10:20:02	Generating Seq2SeqTrainer
10:20:02	Start Training...
10:22:33	KeyboardInterrupt detected. Exiting...

Start -- Nov 15 10:26:42 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:26:42	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:26:44	Using Locally Processed Dataset. Skip Processing...
10:26:44	Generating Seq2SeqTrainer Arguments
10:26:44	Generating Seq2SeqTrainer
10:26:44	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 506, in <module>
    log_msg(["Using Training Argument: ", json.dumps(training_args, indent=2)])
  File "/usr/lib/python3.10/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/usr/lib/python3.10/json/encoder.py", line 201, in encode
    chunks = list(chunks)
  File "/usr/lib/python3.10/json/encoder.py", line 438, in _iterencode
    o = _default(o)
  File "/usr/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type Seq2SeqTrainingArguments is not JSON serializable


Start -- Nov 15 10:27:04 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
10:27:04	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
10:27:07	Using Locally Processed Dataset. Skip Processing...
10:27:07	Generating Seq2SeqTrainer Arguments
10:27:07	Generating Seq2SeqTrainer
10:27:07	Using Training Argument: 
10:27:07	{
  "output_dir": "./speecht5_tts/",
  "overwrite_output_dir": false,
  "do_train": false,
  "do_eval": true,
  "do_predict": false,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 16,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 2,
  "eval_accumulation_steps": null,
  "eval_delay": 0,
  "learning_rate": 1e-05,
  "weight_decay": 0.0,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "num_train_epochs": 3.0,
  "max_steps": 5000,
  "lr_scheduler_type": "linear",
  "lr_scheduler_kwargs": {},
  "warmup_ratio": 0.0,
  "warmup_steps": 500,
  "log_level": "passive",
  "log_level_replica": "warning",
  "log_on_each_node": true,
  "logging_dir": "./speecht5_tts/runs/Nov15_10-27-07_yifanhua-threadripper",
  "logging_strategy": "steps",
  "logging_first_step": false,
  "logging_steps": 5,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": null,
  "save_safetensors": true,
  "save_on_each_node": false,
  "no_cuda": false,
  "use_cpu": false,
  "use_mps_device": false,
  "seed": 42,
  "data_seed": null,
  "jit_mode_eval": false,
  "use_ipex": false,
  "bf16": false,
  "fp16": true,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": 0,
  "ddp_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 0,
  "past_index": -1,
  "run_name": "./speecht5_tts/",
  "disable_tqdm": false,
  "remove_unused_columns": true,
  "label_names": [
    "labels"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "loss",
  "greater_is_better": false,
  "ignore_data_skip": false,
  "fsdp": [],
  "fsdp_min_num_params": 0,
  "fsdp_config": {
    "min_num_params": 0,
    "xla": false,
    "xla_fsdp_grad_ckpt": false
  },
  "fsdp_transformer_layer_cls_to_wrap": null,
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_torch",
  "optim_args": null,
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [
    "tensorboard"
  ],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "ddp_broadcast_buffers": null,
  "dataloader_pin_memory": true,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": true,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "hub_private_repo": false,
  "hub_always_push": false,
  "gradient_checkpointing": true,
  "gradient_checkpointing_kwargs": null,
  "include_inputs_for_metrics": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "mp_parameters": "",
  "auto_find_batch_size": false,
  "full_determinism": false,
  "torchdynamo": null,
  "ray_scope": "last",
  "ddp_timeout": 1800,
  "torch_compile": false,
  "torch_compile_backend": null,
  "torch_compile_mode": null,
  "dispatch_batches": null,
  "split_batches": false,
  "include_tokens_per_second": false,
  "neftune_noise_alpha": null,
  "sortish_sampler": false,
  "predict_with_generate": false,
  "generation_max_length": null,
  "generation_num_beams": null,
  "generation_config": null
}
10:27:07	Start Training...
{'loss': 1.0094, 'learning_rate': 8e-08, 'epoch': 0.08, 'step': 5}
{'loss': 1.0783, 'learning_rate': 1.8e-07, 'epoch': 0.16, 'step': 10}
{'loss': 1.0468, 'learning_rate': 2.8e-07, 'epoch': 0.24, 'step': 15}
{'loss': 1.0816, 'learning_rate': 3.6e-07, 'epoch': 0.32, 'step': 20}
{'loss': 0.9796, 'learning_rate': 4.6000000000000004e-07, 'epoch': 0.4, 'step': 25}
{'loss': 1.095, 'learning_rate': 5.6e-07, 'epoch': 0.48, 'step': 30}
{'loss': 1.0258, 'learning_rate': 6.6e-07, 'epoch': 0.56, 'step': 35}
{'loss': 1.1419, 'learning_rate': 7.6e-07, 'epoch': 0.64, 'step': 40}
{'loss': 0.9892, 'learning_rate': 8.6e-07, 'epoch': 0.72, 'step': 45}
{'loss': 1.0089, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.8, 'step': 50}
{'loss': 1.0073, 'learning_rate': 1.06e-06, 'epoch': 0.88, 'step': 55}
{'loss': 0.9298, 'learning_rate': 1.1600000000000001e-06, 'epoch': 0.96, 'step': 60}
{'loss': 1.0275, 'learning_rate': 1.26e-06, 'epoch': 1.04, 'step': 65}
{'loss': 0.9491, 'learning_rate': 1.3600000000000001e-06, 'epoch': 1.12, 'step': 70}
{'loss': 0.8826, 'learning_rate': 1.46e-06, 'epoch': 1.2, 'step': 75}
{'loss': 0.9587, 'learning_rate': 1.56e-06, 'epoch': 1.28, 'step': 80}
{'loss': 0.94, 'learning_rate': 1.6600000000000002e-06, 'epoch': 1.36, 'step': 85}
{'loss': 0.8889, 'learning_rate': 1.76e-06, 'epoch': 1.44, 'step': 90}
{'loss': 0.8196, 'learning_rate': 1.8600000000000002e-06, 'epoch': 1.52, 'step': 95}
{'loss': 0.9578, 'learning_rate': 1.9600000000000003e-06, 'epoch': 1.6, 'step': 100}
{'loss': 0.9022, 'learning_rate': 2.06e-06, 'epoch': 1.68, 'step': 105}
{'loss': 0.8501, 'learning_rate': 2.16e-06, 'epoch': 1.76, 'step': 110}
{'loss': 0.8769, 'learning_rate': 2.2600000000000004e-06, 'epoch': 1.84, 'step': 115}
{'loss': 0.8633, 'learning_rate': 2.3600000000000003e-06, 'epoch': 1.92, 'step': 120}
{'loss': 0.8799, 'learning_rate': 2.46e-06, 'epoch': 2.0, 'step': 125}
{'loss': 0.8043, 'learning_rate': 2.56e-06, 'epoch': 2.08, 'step': 130}
{'loss': 0.8551, 'learning_rate': 2.6600000000000004e-06, 'epoch': 2.16, 'step': 135}
{'loss': 0.8411, 'learning_rate': 2.7600000000000003e-06, 'epoch': 2.24, 'step': 140}
{'loss': 0.8251, 'learning_rate': 2.86e-06, 'epoch': 2.32, 'step': 145}
{'loss': 0.8576, 'learning_rate': 2.96e-06, 'epoch': 2.4, 'step': 150}
{'loss': 0.8614, 'learning_rate': 3.0600000000000003e-06, 'epoch': 2.48, 'step': 155}
{'loss': 0.7479, 'learning_rate': 3.1600000000000002e-06, 'epoch': 2.56, 'step': 160}
{'loss': 0.8869, 'learning_rate': 3.2600000000000006e-06, 'epoch': 2.64, 'step': 165}
{'loss': 0.8558, 'learning_rate': 3.3600000000000004e-06, 'epoch': 2.72, 'step': 170}
{'loss': 0.8188, 'learning_rate': 3.46e-06, 'epoch': 2.8, 'step': 175}
{'loss': 0.8403, 'learning_rate': 3.5600000000000002e-06, 'epoch': 2.88, 'step': 180}
{'loss': 0.7694, 'learning_rate': 3.66e-06, 'epoch': 2.96, 'step': 185}
{'loss': 0.7537, 'learning_rate': 3.7600000000000004e-06, 'epoch': 3.04, 'step': 190}
{'loss': 0.8158, 'learning_rate': 3.86e-06, 'epoch': 3.12, 'step': 195}
{'loss': 0.7423, 'learning_rate': 3.96e-06, 'epoch': 3.2, 'step': 200}
{'loss': 0.7999, 'learning_rate': 4.060000000000001e-06, 'epoch': 3.28, 'step': 205}
{'loss': 0.7474, 'learning_rate': 4.16e-06, 'epoch': 3.36, 'step': 210}
{'loss': 0.7543, 'learning_rate': 4.26e-06, 'epoch': 3.44, 'step': 215}
{'loss': 0.7664, 'learning_rate': 4.360000000000001e-06, 'epoch': 3.52, 'step': 220}
{'loss': 0.7693, 'learning_rate': 4.4600000000000005e-06, 'epoch': 3.6, 'step': 225}
{'loss': 0.8139, 'learning_rate': 4.56e-06, 'epoch': 3.68, 'step': 230}
{'loss': 0.7671, 'learning_rate': 4.66e-06, 'epoch': 3.76, 'step': 235}
{'loss': 0.7311, 'learning_rate': 4.76e-06, 'epoch': 3.84, 'step': 240}
{'loss': 0.7361, 'learning_rate': 4.86e-06, 'epoch': 3.92, 'step': 245}
{'loss': 0.762, 'learning_rate': 4.960000000000001e-06, 'epoch': 4.0, 'step': 250}
{'loss': 0.7162, 'learning_rate': 5.060000000000001e-06, 'epoch': 4.08, 'step': 255}
{'loss': 0.7542, 'learning_rate': 5.1600000000000006e-06, 'epoch': 4.16, 'step': 260}
{'loss': 0.7399, 'learning_rate': 5.2600000000000005e-06, 'epoch': 4.24, 'step': 265}
{'loss': 0.7579, 'learning_rate': 5.36e-06, 'epoch': 4.32, 'step': 270}
{'loss': 0.7509, 'learning_rate': 5.460000000000001e-06, 'epoch': 4.4, 'step': 275}
{'loss': 0.7231, 'learning_rate': 5.560000000000001e-06, 'epoch': 4.48, 'step': 280}
{'loss': 0.707, 'learning_rate': 5.66e-06, 'epoch': 4.56, 'step': 285}
{'loss': 0.7197, 'learning_rate': 5.76e-06, 'epoch': 4.64, 'step': 290}
{'loss': 0.7283, 'learning_rate': 5.86e-06, 'epoch': 4.72, 'step': 295}
{'loss': 0.6542, 'learning_rate': 5.9600000000000005e-06, 'epoch': 4.8, 'step': 300}
{'loss': 0.704, 'learning_rate': 6.0600000000000004e-06, 'epoch': 4.88, 'step': 305}
{'loss': 0.6824, 'learning_rate': 6.16e-06, 'epoch': 4.96, 'step': 310}
{'loss': 0.7516, 'learning_rate': 6.26e-06, 'epoch': 5.04, 'step': 315}
{'loss': 0.6652, 'learning_rate': 6.360000000000001e-06, 'epoch': 5.12, 'step': 320}
{'loss': 0.6041, 'learning_rate': 6.460000000000001e-06, 'epoch': 5.2, 'step': 325}
{'loss': 0.6429, 'learning_rate': 6.560000000000001e-06, 'epoch': 5.28, 'step': 330}
{'loss': 0.6416, 'learning_rate': 6.660000000000001e-06, 'epoch': 5.36, 'step': 335}
{'loss': 0.6078, 'learning_rate': 6.760000000000001e-06, 'epoch': 5.44, 'step': 340}
{'loss': 0.6988, 'learning_rate': 6.860000000000001e-06, 'epoch': 5.52, 'step': 345}
{'loss': 0.621, 'learning_rate': 6.96e-06, 'epoch': 5.6, 'step': 350}
{'loss': 0.6559, 'learning_rate': 7.06e-06, 'epoch': 5.68, 'step': 355}
{'loss': 0.6201, 'learning_rate': 7.16e-06, 'epoch': 5.76, 'step': 360}
{'loss': 0.5708, 'learning_rate': 7.260000000000001e-06, 'epoch': 5.84, 'step': 365}
{'loss': 0.5809, 'learning_rate': 7.360000000000001e-06, 'epoch': 5.92, 'step': 370}
{'loss': 0.6458, 'learning_rate': 7.4600000000000006e-06, 'epoch': 6.0, 'step': 375}
{'loss': 0.6229, 'learning_rate': 7.5600000000000005e-06, 'epoch': 6.08, 'step': 380}
{'loss': 0.5928, 'learning_rate': 7.660000000000001e-06, 'epoch': 6.16, 'step': 385}
{'loss': 0.6299, 'learning_rate': 7.76e-06, 'epoch': 6.24, 'step': 390}
{'loss': 0.5927, 'learning_rate': 7.860000000000001e-06, 'epoch': 6.32, 'step': 395}
{'loss': 0.6117, 'learning_rate': 7.960000000000002e-06, 'epoch': 6.4, 'step': 400}
{'loss': 0.561, 'learning_rate': 8.06e-06, 'epoch': 6.48, 'step': 405}
{'loss': 0.573, 'learning_rate': 8.16e-06, 'epoch': 6.56, 'step': 410}
{'loss': 0.6438, 'learning_rate': 8.26e-06, 'epoch': 6.64, 'step': 415}
{'loss': 0.6039, 'learning_rate': 8.36e-06, 'epoch': 6.72, 'step': 420}
{'loss': 0.6672, 'learning_rate': 8.46e-06, 'epoch': 6.8, 'step': 425}
{'loss': 0.6286, 'learning_rate': 8.560000000000001e-06, 'epoch': 6.88, 'step': 430}
{'loss': 0.5586, 'learning_rate': 8.66e-06, 'epoch': 6.96, 'step': 435}
{'loss': 0.6141, 'learning_rate': 8.76e-06, 'epoch': 7.04, 'step': 440}
{'loss': 0.5769, 'learning_rate': 8.860000000000002e-06, 'epoch': 7.12, 'step': 445}
{'loss': 0.5592, 'learning_rate': 8.96e-06, 'epoch': 7.2, 'step': 450}
{'loss': 0.5609, 'learning_rate': 9.060000000000001e-06, 'epoch': 7.28, 'step': 455}
{'loss': 0.6154, 'learning_rate': 9.16e-06, 'epoch': 7.36, 'step': 460}
{'loss': 0.5762, 'learning_rate': 9.260000000000001e-06, 'epoch': 7.44, 'step': 465}
{'loss': 0.5884, 'learning_rate': 9.360000000000002e-06, 'epoch': 7.52, 'step': 470}
{'loss': 0.5842, 'learning_rate': 9.460000000000001e-06, 'epoch': 7.6, 'step': 475}
{'loss': 0.5686, 'learning_rate': 9.56e-06, 'epoch': 7.68, 'step': 480}
{'loss': 0.6147, 'learning_rate': 9.66e-06, 'epoch': 7.76, 'step': 485}
{'loss': 0.5709, 'learning_rate': 9.760000000000001e-06, 'epoch': 7.84, 'step': 490}
{'loss': 0.5259, 'learning_rate': 9.86e-06, 'epoch': 7.92, 'step': 495}
{'loss': 0.5845, 'learning_rate': 9.960000000000001e-06, 'epoch': 8.0, 'step': 500}
{'loss': 0.5475, 'learning_rate': 9.993333333333333e-06, 'epoch': 8.08, 'step': 505}
{'loss': 0.5391, 'learning_rate': 9.982222222222224e-06, 'epoch': 8.16, 'step': 510}
{'loss': 0.5679, 'learning_rate': 9.97111111111111e-06, 'epoch': 8.24, 'step': 515}
{'loss': 0.5869, 'learning_rate': 9.960000000000001e-06, 'epoch': 8.32, 'step': 520}
{'loss': 0.5836, 'learning_rate': 9.94888888888889e-06, 'epoch': 8.4, 'step': 525}
{'loss': 0.5761, 'learning_rate': 9.937777777777779e-06, 'epoch': 8.48, 'step': 530}
{'loss': 0.5645, 'learning_rate': 9.926666666666668e-06, 'epoch': 8.56, 'step': 535}
{'loss': 0.6004, 'learning_rate': 9.915555555555556e-06, 'epoch': 8.64, 'step': 540}
{'loss': 0.5524, 'learning_rate': 9.904444444444445e-06, 'epoch': 8.72, 'step': 545}
{'loss': 0.6087, 'learning_rate': 9.893333333333334e-06, 'epoch': 8.8, 'step': 550}
{'loss': 0.5488, 'learning_rate': 9.882222222222223e-06, 'epoch': 8.88, 'step': 555}
{'loss': 0.5756, 'learning_rate': 9.871111111111112e-06, 'epoch': 8.96, 'step': 560}
{'loss': 0.5542, 'learning_rate': 9.86e-06, 'epoch': 9.04, 'step': 565}
{'loss': 0.5749, 'learning_rate': 9.84888888888889e-06, 'epoch': 9.12, 'step': 570}
{'loss': 0.5319, 'learning_rate': 9.837777777777778e-06, 'epoch': 9.2, 'step': 575}
{'loss': 0.5309, 'learning_rate': 9.826666666666667e-06, 'epoch': 9.28, 'step': 580}
{'loss': 0.5309, 'learning_rate': 9.815555555555556e-06, 'epoch': 9.36, 'step': 585}
{'loss': 0.5534, 'learning_rate': 9.804444444444444e-06, 'epoch': 9.44, 'step': 590}
{'loss': 0.5536, 'learning_rate': 9.793333333333333e-06, 'epoch': 9.52, 'step': 595}
{'loss': 0.5426, 'learning_rate': 9.782222222222222e-06, 'epoch': 9.6, 'step': 600}
{'loss': 0.566, 'learning_rate': 9.771111111111113e-06, 'epoch': 9.68, 'step': 605}
{'loss': 0.5696, 'learning_rate': 9.760000000000001e-06, 'epoch': 9.76, 'step': 610}
{'loss': 0.5441, 'learning_rate': 9.74888888888889e-06, 'epoch': 9.84, 'step': 615}
{'loss': 0.5413, 'learning_rate': 9.737777777777779e-06, 'epoch': 9.92, 'step': 620}
{'loss': 0.5241, 'learning_rate': 9.726666666666668e-06, 'epoch': 10.0, 'step': 625}
{'loss': 0.6377, 'learning_rate': 9.715555555555557e-06, 'epoch': 10.08, 'step': 630}
{'loss': 0.5559, 'learning_rate': 9.704444444444445e-06, 'epoch': 10.16, 'step': 635}
{'loss': 0.5226, 'learning_rate': 9.693333333333334e-06, 'epoch': 10.24, 'step': 640}
{'loss': 0.5758, 'learning_rate': 9.682222222222223e-06, 'epoch': 10.32, 'step': 645}
{'loss': 0.5273, 'learning_rate': 9.671111111111112e-06, 'epoch': 10.4, 'step': 650}
{'loss': 0.5304, 'learning_rate': 9.66e-06, 'epoch': 10.48, 'step': 655}
{'loss': 0.5287, 'learning_rate': 9.64888888888889e-06, 'epoch': 10.56, 'step': 660}
{'loss': 0.5699, 'learning_rate': 9.637777777777778e-06, 'epoch': 10.64, 'step': 665}
{'loss': 0.5511, 'learning_rate': 9.626666666666667e-06, 'epoch': 10.72, 'step': 670}
{'loss': 0.5393, 'learning_rate': 9.615555555555558e-06, 'epoch': 10.8, 'step': 675}
{'loss': 0.5194, 'learning_rate': 9.604444444444445e-06, 'epoch': 10.88, 'step': 680}
{'loss': 0.5307, 'learning_rate': 9.593333333333335e-06, 'epoch': 10.96, 'step': 685}
{'loss': 0.531, 'learning_rate': 9.582222222222222e-06, 'epoch': 11.04, 'step': 690}
{'loss': 0.5053, 'learning_rate': 9.571111111111113e-06, 'epoch': 11.12, 'step': 695}
{'loss': 0.533, 'learning_rate': 9.56e-06, 'epoch': 11.2, 'step': 700}
{'loss': 0.5478, 'learning_rate': 9.54888888888889e-06, 'epoch': 11.28, 'step': 705}
{'loss': 0.5202, 'learning_rate': 9.537777777777778e-06, 'epoch': 11.36, 'step': 710}
{'loss': 0.5292, 'learning_rate': 9.526666666666668e-06, 'epoch': 11.44, 'step': 715}
{'loss': 0.5227, 'learning_rate': 9.515555555555557e-06, 'epoch': 11.52, 'step': 720}
{'loss': 0.5348, 'learning_rate': 9.504444444444446e-06, 'epoch': 11.6, 'step': 725}
{'loss': 0.5199, 'learning_rate': 9.493333333333334e-06, 'epoch': 11.68, 'step': 730}
{'loss': 0.5308, 'learning_rate': 9.482222222222223e-06, 'epoch': 11.76, 'step': 735}
{'loss': 0.5164, 'learning_rate': 9.471111111111112e-06, 'epoch': 11.84, 'step': 740}
{'loss': 0.5098, 'learning_rate': 9.460000000000001e-06, 'epoch': 11.92, 'step': 745}
{'loss': 0.5252, 'learning_rate': 9.44888888888889e-06, 'epoch': 12.0, 'step': 750}
{'loss': 0.5061, 'learning_rate': 9.437777777777779e-06, 'epoch': 12.08, 'step': 755}
{'loss': 0.5178, 'learning_rate': 9.426666666666667e-06, 'epoch': 12.16, 'step': 760}
{'loss': 0.5767, 'learning_rate': 9.415555555555556e-06, 'epoch': 12.24, 'step': 765}
{'loss': 0.5173, 'learning_rate': 9.404444444444445e-06, 'epoch': 12.32, 'step': 770}
{'loss': 0.5362, 'learning_rate': 9.393333333333334e-06, 'epoch': 12.4, 'step': 775}
{'loss': 0.5073, 'learning_rate': 9.382222222222223e-06, 'epoch': 12.48, 'step': 780}
{'loss': 0.5443, 'learning_rate': 9.371111111111111e-06, 'epoch': 12.56, 'step': 785}
{'loss': 0.5364, 'learning_rate': 9.360000000000002e-06, 'epoch': 12.64, 'step': 790}
{'loss': 0.5548, 'learning_rate': 9.348888888888889e-06, 'epoch': 12.72, 'step': 795}
{'loss': 0.5315, 'learning_rate': 9.33777777777778e-06, 'epoch': 12.8, 'step': 800}
{'loss': 0.526, 'learning_rate': 9.326666666666667e-06, 'epoch': 12.88, 'step': 805}
{'loss': 0.5609, 'learning_rate': 9.315555555555557e-06, 'epoch': 12.96, 'step': 810}
{'loss': 0.5229, 'learning_rate': 9.304444444444444e-06, 'epoch': 13.04, 'step': 815}
{'loss': 0.5274, 'learning_rate': 9.293333333333335e-06, 'epoch': 13.12, 'step': 820}
{'loss': 0.5616, 'learning_rate': 9.282222222222222e-06, 'epoch': 13.2, 'step': 825}
{'loss': 0.5078, 'learning_rate': 9.271111111111112e-06, 'epoch': 13.28, 'step': 830}
{'loss': 0.5416, 'learning_rate': 9.260000000000001e-06, 'epoch': 13.36, 'step': 835}
{'loss': 0.5648, 'learning_rate': 9.24888888888889e-06, 'epoch': 13.44, 'step': 840}
{'loss': 0.5203, 'learning_rate': 9.237777777777779e-06, 'epoch': 13.52, 'step': 845}
{'loss': 0.5218, 'learning_rate': 9.226666666666668e-06, 'epoch': 13.6, 'step': 850}
{'loss': 0.5005, 'learning_rate': 9.215555555555556e-06, 'epoch': 13.68, 'step': 855}
{'loss': 0.5242, 'learning_rate': 9.204444444444445e-06, 'epoch': 13.76, 'step': 860}
{'loss': 0.5237, 'learning_rate': 9.193333333333334e-06, 'epoch': 13.84, 'step': 865}
{'loss': 0.5176, 'learning_rate': 9.182222222222223e-06, 'epoch': 13.92, 'step': 870}
{'loss': 0.52, 'learning_rate': 9.171111111111112e-06, 'epoch': 14.0, 'step': 875}
{'loss': 0.5111, 'learning_rate': 9.16e-06, 'epoch': 14.08, 'step': 880}
{'loss': 0.5312, 'learning_rate': 9.14888888888889e-06, 'epoch': 14.16, 'step': 885}
{'loss': 0.5279, 'learning_rate': 9.137777777777778e-06, 'epoch': 14.24, 'step': 890}
{'loss': 0.5317, 'learning_rate': 9.126666666666667e-06, 'epoch': 14.32, 'step': 895}
{'loss': 0.5556, 'learning_rate': 9.115555555555556e-06, 'epoch': 14.4, 'step': 900}
{'loss': 0.5314, 'learning_rate': 9.104444444444444e-06, 'epoch': 14.48, 'step': 905}
{'loss': 0.5199, 'learning_rate': 9.093333333333333e-06, 'epoch': 14.56, 'step': 910}
{'loss': 0.5053, 'learning_rate': 9.082222222222224e-06, 'epoch': 14.64, 'step': 915}
{'loss': 0.5323, 'learning_rate': 9.07111111111111e-06, 'epoch': 14.72, 'step': 920}
{'loss': 0.5179, 'learning_rate': 9.060000000000001e-06, 'epoch': 14.8, 'step': 925}
{'loss': 0.5059, 'learning_rate': 9.048888888888888e-06, 'epoch': 14.88, 'step': 930}
{'loss': 0.5095, 'learning_rate': 9.037777777777779e-06, 'epoch': 14.96, 'step': 935}
{'loss': 0.5192, 'learning_rate': 9.026666666666666e-06, 'epoch': 15.04, 'step': 940}
{'loss': 0.501, 'learning_rate': 9.015555555555557e-06, 'epoch': 15.12, 'step': 945}
{'loss': 0.5175, 'learning_rate': 9.004444444444445e-06, 'epoch': 15.2, 'step': 950}
{'loss': 0.5259, 'learning_rate': 8.993333333333334e-06, 'epoch': 15.28, 'step': 955}
{'loss': 0.4989, 'learning_rate': 8.982222222222223e-06, 'epoch': 15.36, 'step': 960}
{'loss': 0.4956, 'learning_rate': 8.971111111111112e-06, 'epoch': 15.44, 'step': 965}
{'loss': 0.5311, 'learning_rate': 8.96e-06, 'epoch': 15.52, 'step': 970}
{'loss': 0.5063, 'learning_rate': 8.94888888888889e-06, 'epoch': 15.6, 'step': 975}
{'loss': 0.5027, 'learning_rate': 8.937777777777778e-06, 'epoch': 15.68, 'step': 980}
{'loss': 0.5223, 'learning_rate': 8.926666666666669e-06, 'epoch': 15.76, 'step': 985}
{'loss': 0.5122, 'learning_rate': 8.915555555555556e-06, 'epoch': 15.84, 'step': 990}
{'loss': 0.5121, 'learning_rate': 8.904444444444446e-06, 'epoch': 15.92, 'step': 995}
{'loss': 0.5383, 'learning_rate': 8.893333333333333e-06, 'epoch': 16.0, 'step': 1000}
{'eval_loss': 0.4643670618534088, 'eval_runtime': 0.0946, 'eval_samples_per_second': 52.866, 'eval_steps_per_second': 10.573, 'epoch': 16.0, 'step': 1000}
{'loss': 0.5261, 'learning_rate': 8.882222222222224e-06, 'epoch': 16.08, 'step': 1005}
{'loss': 0.4991, 'learning_rate': 8.871111111111111e-06, 'epoch': 16.16, 'step': 1010}
{'loss': 0.5159, 'learning_rate': 8.860000000000002e-06, 'epoch': 16.24, 'step': 1015}
{'loss': 0.5061, 'learning_rate': 8.848888888888889e-06, 'epoch': 16.32, 'step': 1020}
{'loss': 0.4941, 'learning_rate': 8.83777777777778e-06, 'epoch': 16.4, 'step': 1025}
{'loss': 0.5585, 'learning_rate': 8.826666666666668e-06, 'epoch': 16.48, 'step': 1030}
{'loss': 0.5104, 'learning_rate': 8.815555555555557e-06, 'epoch': 16.56, 'step': 1035}
{'loss': 0.5534, 'learning_rate': 8.804444444444446e-06, 'epoch': 16.64, 'step': 1040}
{'loss': 0.5064, 'learning_rate': 8.793333333333334e-06, 'epoch': 16.72, 'step': 1045}
{'loss': 0.5147, 'learning_rate': 8.782222222222223e-06, 'epoch': 16.8, 'step': 1050}
{'loss': 0.5023, 'learning_rate': 8.771111111111112e-06, 'epoch': 16.88, 'step': 1055}
{'loss': 0.5271, 'learning_rate': 8.76e-06, 'epoch': 16.96, 'step': 1060}
{'loss': 0.4923, 'learning_rate': 8.74888888888889e-06, 'epoch': 17.04, 'step': 1065}
{'loss': 0.5036, 'learning_rate': 8.737777777777778e-06, 'epoch': 17.12, 'step': 1070}
{'loss': 0.4863, 'learning_rate': 8.726666666666667e-06, 'epoch': 17.2, 'step': 1075}
{'loss': 0.5131, 'learning_rate': 8.715555555555556e-06, 'epoch': 17.28, 'step': 1080}
{'loss': 0.4973, 'learning_rate': 8.704444444444445e-06, 'epoch': 17.36, 'step': 1085}
{'loss': 0.5004, 'learning_rate': 8.693333333333334e-06, 'epoch': 17.44, 'step': 1090}
{'loss': 0.5424, 'learning_rate': 8.682222222222222e-06, 'epoch': 17.52, 'step': 1095}
{'loss': 0.5239, 'learning_rate': 8.671111111111113e-06, 'epoch': 17.6, 'step': 1100}
{'loss': 0.6449, 'learning_rate': 8.66e-06, 'epoch': 17.68, 'step': 1105}
{'loss': 0.5347, 'learning_rate': 8.64888888888889e-06, 'epoch': 17.76, 'step': 1110}
{'loss': 0.4875, 'learning_rate': 8.637777777777778e-06, 'epoch': 17.84, 'step': 1115}
{'loss': 0.5275, 'learning_rate': 8.626666666666668e-06, 'epoch': 17.92, 'step': 1120}
{'loss': 0.5181, 'learning_rate': 8.615555555555555e-06, 'epoch': 18.0, 'step': 1125}
{'loss': 0.517, 'learning_rate': 8.604444444444446e-06, 'epoch': 18.08, 'step': 1130}
{'loss': 0.5363, 'learning_rate': 8.593333333333333e-06, 'epoch': 18.16, 'step': 1135}
{'loss': 0.4894, 'learning_rate': 8.582222222222223e-06, 'epoch': 18.24, 'step': 1140}
{'loss': 0.4956, 'learning_rate': 8.571111111111112e-06, 'epoch': 18.32, 'step': 1145}
{'loss': 0.4815, 'learning_rate': 8.560000000000001e-06, 'epoch': 18.4, 'step': 1150}
{'loss': 0.4852, 'learning_rate': 8.54888888888889e-06, 'epoch': 18.48, 'step': 1155}
{'loss': 0.5243, 'learning_rate': 8.537777777777779e-06, 'epoch': 18.56, 'step': 1160}
{'loss': 0.4945, 'learning_rate': 8.526666666666667e-06, 'epoch': 18.64, 'step': 1165}
{'loss': 0.4901, 'learning_rate': 8.515555555555556e-06, 'epoch': 18.72, 'step': 1170}
{'loss': 0.4965, 'learning_rate': 8.504444444444445e-06, 'epoch': 18.8, 'step': 1175}
{'loss': 0.4953, 'learning_rate': 8.493333333333334e-06, 'epoch': 18.88, 'step': 1180}
{'loss': 0.509, 'learning_rate': 8.482222222222223e-06, 'epoch': 18.96, 'step': 1185}
{'loss': 0.4998, 'learning_rate': 8.471111111111112e-06, 'epoch': 19.04, 'step': 1190}
{'loss': 0.4971, 'learning_rate': 8.46e-06, 'epoch': 19.12, 'step': 1195}
{'loss': 0.5004, 'learning_rate': 8.448888888888889e-06, 'epoch': 19.2, 'step': 1200}
{'loss': 0.4984, 'learning_rate': 8.437777777777778e-06, 'epoch': 19.28, 'step': 1205}
{'loss': 0.5389, 'learning_rate': 8.426666666666667e-06, 'epoch': 19.36, 'step': 1210}
{'loss': 0.4968, 'learning_rate': 8.415555555555556e-06, 'epoch': 19.44, 'step': 1215}
{'loss': 0.5128, 'learning_rate': 8.404444444444444e-06, 'epoch': 19.52, 'step': 1220}
{'loss': 0.5187, 'learning_rate': 8.393333333333335e-06, 'epoch': 19.6, 'step': 1225}
{'loss': 0.5026, 'learning_rate': 8.382222222222222e-06, 'epoch': 19.68, 'step': 1230}
{'loss': 0.4852, 'learning_rate': 8.371111111111112e-06, 'epoch': 19.76, 'step': 1235}
{'loss': 0.4798, 'learning_rate': 8.36e-06, 'epoch': 19.84, 'step': 1240}
{'loss': 0.5023, 'learning_rate': 8.34888888888889e-06, 'epoch': 19.92, 'step': 1245}
{'loss': 0.5249, 'learning_rate': 8.337777777777777e-06, 'epoch': 20.0, 'step': 1250}
{'loss': 0.4792, 'learning_rate': 8.326666666666668e-06, 'epoch': 20.08, 'step': 1255}
{'loss': 0.4934, 'learning_rate': 8.315555555555557e-06, 'epoch': 20.16, 'step': 1260}
{'loss': 0.4915, 'learning_rate': 8.304444444444445e-06, 'epoch': 20.24, 'step': 1265}
{'loss': 0.5818, 'learning_rate': 8.293333333333334e-06, 'epoch': 20.32, 'step': 1270}
{'loss': 0.5135, 'learning_rate': 8.282222222222223e-06, 'epoch': 20.4, 'step': 1275}
{'loss': 0.5154, 'learning_rate': 8.271111111111112e-06, 'epoch': 20.48, 'step': 1280}
{'loss': 0.4885, 'learning_rate': 8.26e-06, 'epoch': 20.56, 'step': 1285}
{'loss': 0.5448, 'learning_rate': 8.24888888888889e-06, 'epoch': 20.64, 'step': 1290}
{'loss': 0.4857, 'learning_rate': 8.237777777777778e-06, 'epoch': 20.72, 'step': 1295}
{'loss': 0.4962, 'learning_rate': 8.226666666666667e-06, 'epoch': 20.8, 'step': 1300}
{'loss': 0.5002, 'learning_rate': 8.215555555555557e-06, 'epoch': 20.88, 'step': 1305}
{'loss': 0.5212, 'learning_rate': 8.204444444444445e-06, 'epoch': 20.96, 'step': 1310}
{'loss': 0.4953, 'learning_rate': 8.193333333333335e-06, 'epoch': 21.04, 'step': 1315}
{'loss': 0.4955, 'learning_rate': 8.182222222222222e-06, 'epoch': 21.12, 'step': 1320}
{'loss': 0.5056, 'learning_rate': 8.171111111111113e-06, 'epoch': 21.2, 'step': 1325}
{'loss': 0.4772, 'learning_rate': 8.16e-06, 'epoch': 21.28, 'step': 1330}
{'loss': 0.5024, 'learning_rate': 8.14888888888889e-06, 'epoch': 21.36, 'step': 1335}
{'loss': 0.5094, 'learning_rate': 8.137777777777779e-06, 'epoch': 21.44, 'step': 1340}
{'loss': 0.5026, 'learning_rate': 8.126666666666668e-06, 'epoch': 21.52, 'step': 1345}
{'loss': 0.4961, 'learning_rate': 8.115555555555557e-06, 'epoch': 21.6, 'step': 1350}
{'loss': 0.4977, 'learning_rate': 8.104444444444446e-06, 'epoch': 21.68, 'step': 1355}
{'loss': 0.4961, 'learning_rate': 8.093333333333334e-06, 'epoch': 21.76, 'step': 1360}
{'loss': 0.4971, 'learning_rate': 8.082222222222223e-06, 'epoch': 21.84, 'step': 1365}
{'loss': 0.4867, 'learning_rate': 8.071111111111112e-06, 'epoch': 21.92, 'step': 1370}
{'loss': 0.4868, 'learning_rate': 8.06e-06, 'epoch': 22.0, 'step': 1375}
{'loss': 0.4812, 'learning_rate': 8.04888888888889e-06, 'epoch': 22.08, 'step': 1380}
{'loss': 0.4776, 'learning_rate': 8.037777777777778e-06, 'epoch': 22.16, 'step': 1385}
{'loss': 0.4782, 'learning_rate': 8.026666666666667e-06, 'epoch': 22.24, 'step': 1390}
{'loss': 0.5011, 'learning_rate': 8.015555555555556e-06, 'epoch': 22.32, 'step': 1395}
{'loss': 0.4852, 'learning_rate': 8.004444444444445e-06, 'epoch': 22.4, 'step': 1400}
{'loss': 0.4847, 'learning_rate': 7.993333333333334e-06, 'epoch': 22.48, 'step': 1405}
{'loss': 0.496, 'learning_rate': 7.982222222222224e-06, 'epoch': 22.56, 'step': 1410}
{'loss': 0.5048, 'learning_rate': 7.971111111111111e-06, 'epoch': 22.64, 'step': 1415}
{'loss': 0.5428, 'learning_rate': 7.960000000000002e-06, 'epoch': 22.72, 'step': 1420}
{'loss': 0.491, 'learning_rate': 7.948888888888889e-06, 'epoch': 22.8, 'step': 1425}
{'loss': 0.5016, 'learning_rate': 7.93777777777778e-06, 'epoch': 22.88, 'step': 1430}
{'loss': 0.5045, 'learning_rate': 7.926666666666666e-06, 'epoch': 22.96, 'step': 1435}
{'loss': 0.4952, 'learning_rate': 7.915555555555557e-06, 'epoch': 23.04, 'step': 1440}
{'loss': 0.4907, 'learning_rate': 7.904444444444444e-06, 'epoch': 23.12, 'step': 1445}
{'loss': 0.4695, 'learning_rate': 7.893333333333335e-06, 'epoch': 23.2, 'step': 1450}
{'loss': 0.4898, 'learning_rate': 7.882222222222223e-06, 'epoch': 23.28, 'step': 1455}
{'loss': 0.5078, 'learning_rate': 7.871111111111112e-06, 'epoch': 23.36, 'step': 1460}
{'loss': 0.4916, 'learning_rate': 7.860000000000001e-06, 'epoch': 23.44, 'step': 1465}
{'loss': 0.5061, 'learning_rate': 7.84888888888889e-06, 'epoch': 23.52, 'step': 1470}
{'loss': 0.5004, 'learning_rate': 7.837777777777779e-06, 'epoch': 23.6, 'step': 1475}
{'loss': 0.5083, 'learning_rate': 7.826666666666667e-06, 'epoch': 23.68, 'step': 1480}
{'loss': 0.4925, 'learning_rate': 7.815555555555556e-06, 'epoch': 23.76, 'step': 1485}
{'loss': 0.4908, 'learning_rate': 7.804444444444445e-06, 'epoch': 23.84, 'step': 1490}
{'loss': 0.4935, 'learning_rate': 7.793333333333334e-06, 'epoch': 23.92, 'step': 1495}
{'loss': 0.4884, 'learning_rate': 7.782222222222223e-06, 'epoch': 24.0, 'step': 1500}
{'loss': 0.5068, 'learning_rate': 7.771111111111111e-06, 'epoch': 24.08, 'step': 1505}
{'loss': 0.5076, 'learning_rate': 7.76e-06, 'epoch': 24.16, 'step': 1510}
{'loss': 0.5057, 'learning_rate': 7.748888888888889e-06, 'epoch': 24.24, 'step': 1515}
{'loss': 0.4922, 'learning_rate': 7.737777777777778e-06, 'epoch': 24.32, 'step': 1520}
{'loss': 0.4804, 'learning_rate': 7.726666666666667e-06, 'epoch': 24.4, 'step': 1525}
{'loss': 0.4881, 'learning_rate': 7.715555555555555e-06, 'epoch': 24.48, 'step': 1530}
{'loss': 0.5201, 'learning_rate': 7.704444444444446e-06, 'epoch': 24.56, 'step': 1535}
{'loss': 0.4844, 'learning_rate': 7.693333333333333e-06, 'epoch': 24.64, 'step': 1540}
{'loss': 0.518, 'learning_rate': 7.682222222222224e-06, 'epoch': 24.72, 'step': 1545}
{'loss': 0.4823, 'learning_rate': 7.67111111111111e-06, 'epoch': 24.8, 'step': 1550}
{'loss': 0.4827, 'learning_rate': 7.660000000000001e-06, 'epoch': 24.88, 'step': 1555}
{'loss': 0.5174, 'learning_rate': 7.648888888888888e-06, 'epoch': 24.96, 'step': 1560}
{'loss': 0.4942, 'learning_rate': 7.637777777777779e-06, 'epoch': 25.04, 'step': 1565}
{'loss': 0.4883, 'learning_rate': 7.626666666666668e-06, 'epoch': 25.12, 'step': 1570}
{'loss': 0.4743, 'learning_rate': 7.6155555555555564e-06, 'epoch': 25.2, 'step': 1575}
{'loss': 0.497, 'learning_rate': 7.604444444444445e-06, 'epoch': 25.28, 'step': 1580}
{'loss': 0.4895, 'learning_rate': 7.593333333333334e-06, 'epoch': 25.36, 'step': 1585}
{'loss': 0.485, 'learning_rate': 7.582222222222223e-06, 'epoch': 25.44, 'step': 1590}
{'loss': 0.4887, 'learning_rate': 7.571111111111112e-06, 'epoch': 25.52, 'step': 1595}
{'loss': 0.5042, 'learning_rate': 7.5600000000000005e-06, 'epoch': 25.6, 'step': 1600}
{'loss': 0.5028, 'learning_rate': 7.54888888888889e-06, 'epoch': 25.68, 'step': 1605}
{'loss': 0.5189, 'learning_rate': 7.537777777777778e-06, 'epoch': 25.76, 'step': 1610}
{'loss': 0.482, 'learning_rate': 7.526666666666668e-06, 'epoch': 25.84, 'step': 1615}
{'loss': 0.5265, 'learning_rate': 7.515555555555556e-06, 'epoch': 25.92, 'step': 1620}
{'loss': 0.4834, 'learning_rate': 7.504444444444445e-06, 'epoch': 26.0, 'step': 1625}
{'loss': 0.4981, 'learning_rate': 7.493333333333333e-06, 'epoch': 26.08, 'step': 1630}
{'loss': 0.5065, 'learning_rate': 7.482222222222223e-06, 'epoch': 26.16, 'step': 1635}
{'loss': 0.5085, 'learning_rate': 7.471111111111111e-06, 'epoch': 26.24, 'step': 1640}
{'loss': 0.4798, 'learning_rate': 7.4600000000000006e-06, 'epoch': 26.32, 'step': 1645}
{'loss': 0.5273, 'learning_rate': 7.44888888888889e-06, 'epoch': 26.4, 'step': 1650}
{'loss': 0.5326, 'learning_rate': 7.437777777777778e-06, 'epoch': 26.48, 'step': 1655}
{'loss': 0.5183, 'learning_rate': 7.426666666666668e-06, 'epoch': 26.56, 'step': 1660}
{'loss': 0.497, 'learning_rate': 7.415555555555556e-06, 'epoch': 26.64, 'step': 1665}
{'loss': 0.4995, 'learning_rate': 7.4044444444444455e-06, 'epoch': 26.72, 'step': 1670}
{'loss': 0.4927, 'learning_rate': 7.393333333333333e-06, 'epoch': 26.8, 'step': 1675}
{'loss': 0.5044, 'learning_rate': 7.382222222222223e-06, 'epoch': 26.88, 'step': 1680}
{'loss': 0.4878, 'learning_rate': 7.371111111111112e-06, 'epoch': 26.96, 'step': 1685}
{'loss': 0.4853, 'learning_rate': 7.360000000000001e-06, 'epoch': 27.04, 'step': 1690}
{'loss': 0.5051, 'learning_rate': 7.3488888888888895e-06, 'epoch': 27.12, 'step': 1695}
{'loss': 0.4713, 'learning_rate': 7.337777777777778e-06, 'epoch': 27.2, 'step': 1700}
{'loss': 0.5039, 'learning_rate': 7.326666666666667e-06, 'epoch': 27.28, 'step': 1705}
{'loss': 0.4982, 'learning_rate': 7.315555555555556e-06, 'epoch': 27.36, 'step': 1710}
{'loss': 0.4969, 'learning_rate': 7.304444444444445e-06, 'epoch': 27.44, 'step': 1715}
{'loss': 0.5043, 'learning_rate': 7.2933333333333335e-06, 'epoch': 27.52, 'step': 1720}
{'loss': 0.4944, 'learning_rate': 7.282222222222222e-06, 'epoch': 27.6, 'step': 1725}
{'loss': 0.4685, 'learning_rate': 7.271111111111112e-06, 'epoch': 27.68, 'step': 1730}
{'loss': 0.4816, 'learning_rate': 7.260000000000001e-06, 'epoch': 27.76, 'step': 1735}
{'loss': 0.4806, 'learning_rate': 7.24888888888889e-06, 'epoch': 27.84, 'step': 1740}
{'loss': 0.4747, 'learning_rate': 7.237777777777778e-06, 'epoch': 27.92, 'step': 1745}
{'loss': 0.476, 'learning_rate': 7.226666666666667e-06, 'epoch': 28.0, 'step': 1750}
{'loss': 0.4718, 'learning_rate': 7.215555555555556e-06, 'epoch': 28.08, 'step': 1755}
{'loss': 0.4802, 'learning_rate': 7.204444444444445e-06, 'epoch': 28.16, 'step': 1760}
{'loss': 0.4782, 'learning_rate': 7.1933333333333345e-06, 'epoch': 28.24, 'step': 1765}
{'loss': 0.4893, 'learning_rate': 7.1822222222222224e-06, 'epoch': 28.32, 'step': 1770}
{'loss': 0.501, 'learning_rate': 7.171111111111112e-06, 'epoch': 28.4, 'step': 1775}
{'loss': 0.4716, 'learning_rate': 7.16e-06, 'epoch': 28.48, 'step': 1780}
{'loss': 0.5466, 'learning_rate': 7.14888888888889e-06, 'epoch': 28.56, 'step': 1785}
{'loss': 0.5091, 'learning_rate': 7.137777777777778e-06, 'epoch': 28.64, 'step': 1790}
{'loss': 0.4871, 'learning_rate': 7.126666666666667e-06, 'epoch': 28.72, 'step': 1795}
{'loss': 0.4681, 'learning_rate': 7.115555555555557e-06, 'epoch': 28.8, 'step': 1800}
{'loss': 0.505, 'learning_rate': 7.104444444444445e-06, 'epoch': 28.88, 'step': 1805}
{'loss': 0.5329, 'learning_rate': 7.093333333333335e-06, 'epoch': 28.96, 'step': 1810}
{'loss': 0.4758, 'learning_rate': 7.0822222222222226e-06, 'epoch': 29.04, 'step': 1815}
{'loss': 0.4846, 'learning_rate': 7.071111111111112e-06, 'epoch': 29.12, 'step': 1820}
{'loss': 0.4889, 'learning_rate': 7.06e-06, 'epoch': 29.2, 'step': 1825}
{'loss': 0.4685, 'learning_rate': 7.04888888888889e-06, 'epoch': 29.28, 'step': 1830}
{'loss': 0.4874, 'learning_rate': 7.037777777777778e-06, 'epoch': 29.36, 'step': 1835}
{'loss': 0.4899, 'learning_rate': 7.0266666666666674e-06, 'epoch': 29.44, 'step': 1840}
{'loss': 0.4735, 'learning_rate': 7.015555555555556e-06, 'epoch': 29.52, 'step': 1845}
{'loss': 0.5107, 'learning_rate': 7.004444444444445e-06, 'epoch': 29.6, 'step': 1850}
{'loss': 0.4721, 'learning_rate': 6.993333333333334e-06, 'epoch': 29.68, 'step': 1855}
{'loss': 0.497, 'learning_rate': 6.982222222222223e-06, 'epoch': 29.76, 'step': 1860}
{'loss': 0.4861, 'learning_rate': 6.9711111111111115e-06, 'epoch': 29.84, 'step': 1865}
{'loss': 0.5053, 'learning_rate': 6.96e-06, 'epoch': 29.92, 'step': 1870}
{'loss': 0.4892, 'learning_rate': 6.948888888888889e-06, 'epoch': 30.0, 'step': 1875}
{'loss': 0.4812, 'learning_rate': 6.937777777777779e-06, 'epoch': 30.08, 'step': 1880}
{'loss': 0.4939, 'learning_rate': 6.926666666666667e-06, 'epoch': 30.16, 'step': 1885}
{'loss': 0.4749, 'learning_rate': 6.915555555555556e-06, 'epoch': 30.24, 'step': 1890}
{'loss': 0.4764, 'learning_rate': 6.904444444444444e-06, 'epoch': 30.32, 'step': 1895}
{'loss': 0.4609, 'learning_rate': 6.893333333333334e-06, 'epoch': 30.4, 'step': 1900}
{'loss': 0.4967, 'learning_rate': 6.882222222222223e-06, 'epoch': 30.48, 'step': 1905}
{'loss': 0.4815, 'learning_rate': 6.871111111111112e-06, 'epoch': 30.56, 'step': 1910}
{'loss': 0.4807, 'learning_rate': 6.860000000000001e-06, 'epoch': 30.64, 'step': 1915}
{'loss': 0.4673, 'learning_rate': 6.848888888888889e-06, 'epoch': 30.72, 'step': 1920}
{'loss': 0.4778, 'learning_rate': 6.837777777777779e-06, 'epoch': 30.8, 'step': 1925}
{'loss': 0.4677, 'learning_rate': 6.826666666666667e-06, 'epoch': 30.88, 'step': 1930}
{'loss': 0.4996, 'learning_rate': 6.8155555555555565e-06, 'epoch': 30.96, 'step': 1935}
{'loss': 0.4791, 'learning_rate': 6.8044444444444444e-06, 'epoch': 31.04, 'step': 1940}
{'loss': 0.4869, 'learning_rate': 6.793333333333334e-06, 'epoch': 31.12, 'step': 1945}
{'loss': 0.484, 'learning_rate': 6.782222222222222e-06, 'epoch': 31.2, 'step': 1950}
{'loss': 0.4737, 'learning_rate': 6.771111111111112e-06, 'epoch': 31.28, 'step': 1955}
{'loss': 0.4643, 'learning_rate': 6.760000000000001e-06, 'epoch': 31.36, 'step': 1960}
{'loss': 0.4861, 'learning_rate': 6.748888888888889e-06, 'epoch': 31.44, 'step': 1965}
{'loss': 0.4935, 'learning_rate': 6.737777777777779e-06, 'epoch': 31.52, 'step': 1970}
{'loss': 0.4801, 'learning_rate': 6.726666666666667e-06, 'epoch': 31.6, 'step': 1975}
{'loss': 0.4952, 'learning_rate': 6.7155555555555566e-06, 'epoch': 31.68, 'step': 1980}
{'loss': 0.4979, 'learning_rate': 6.7044444444444445e-06, 'epoch': 31.76, 'step': 1985}
{'loss': 0.4611, 'learning_rate': 6.693333333333334e-06, 'epoch': 31.84, 'step': 1990}
{'loss': 0.4701, 'learning_rate': 6.682222222222223e-06, 'epoch': 31.92, 'step': 1995}
{'loss': 0.5126, 'learning_rate': 6.671111111111112e-06, 'epoch': 32.0, 'step': 2000}
{'eval_loss': 0.4857853055000305, 'eval_runtime': 0.0792, 'eval_samples_per_second': 63.118, 'eval_steps_per_second': 12.624, 'epoch': 32.0, 'step': 2000}
{'loss': 0.5268, 'learning_rate': 6.660000000000001e-06, 'epoch': 32.08, 'step': 2005}
{'loss': 0.5078, 'learning_rate': 6.648888888888889e-06, 'epoch': 32.16, 'step': 2010}
{'loss': 0.4818, 'learning_rate': 6.637777777777778e-06, 'epoch': 32.24, 'step': 2015}
{'loss': 0.4617, 'learning_rate': 6.626666666666667e-06, 'epoch': 32.32, 'step': 2020}
{'loss': 0.5173, 'learning_rate': 6.615555555555556e-06, 'epoch': 32.4, 'step': 2025}
{'loss': 0.4867, 'learning_rate': 6.604444444444445e-06, 'epoch': 32.48, 'step': 2030}
{'loss': 0.4745, 'learning_rate': 6.5933333333333335e-06, 'epoch': 32.56, 'step': 2035}
{'loss': 0.4669, 'learning_rate': 6.582222222222223e-06, 'epoch': 32.64, 'step': 2040}
{'loss': 0.4948, 'learning_rate': 6.571111111111111e-06, 'epoch': 32.72, 'step': 2045}
{'loss': 0.4618, 'learning_rate': 6.560000000000001e-06, 'epoch': 32.8, 'step': 2050}
{'loss': 0.4759, 'learning_rate': 6.548888888888889e-06, 'epoch': 32.88, 'step': 2055}
{'loss': 0.49, 'learning_rate': 6.537777777777778e-06, 'epoch': 32.96, 'step': 2060}
{'loss': 0.4807, 'learning_rate': 6.526666666666666e-06, 'epoch': 33.04, 'step': 2065}
{'loss': 0.5242, 'learning_rate': 6.515555555555556e-06, 'epoch': 33.12, 'step': 2070}
{'loss': 0.4567, 'learning_rate': 6.504444444444446e-06, 'epoch': 33.2, 'step': 2075}
{'loss': 0.5024, 'learning_rate': 6.4933333333333336e-06, 'epoch': 33.28, 'step': 2080}
{'loss': 0.5303, 'learning_rate': 6.482222222222223e-06, 'epoch': 33.36, 'step': 2085}
{'loss': 0.4653, 'learning_rate': 6.471111111111111e-06, 'epoch': 33.44, 'step': 2090}
{'loss': 0.4897, 'learning_rate': 6.460000000000001e-06, 'epoch': 33.52, 'step': 2095}
{'loss': 0.4568, 'learning_rate': 6.448888888888889e-06, 'epoch': 33.6, 'step': 2100}
{'loss': 0.4785, 'learning_rate': 6.4377777777777784e-06, 'epoch': 33.68, 'step': 2105}
{'loss': 0.5023, 'learning_rate': 6.426666666666668e-06, 'epoch': 33.76, 'step': 2110}
{'loss': 0.4757, 'learning_rate': 6.415555555555556e-06, 'epoch': 33.84, 'step': 2115}
{'loss': 0.4673, 'learning_rate': 6.404444444444446e-06, 'epoch': 33.92, 'step': 2120}
{'loss': 0.535, 'learning_rate': 6.393333333333334e-06, 'epoch': 34.0, 'step': 2125}
{'loss': 0.4735, 'learning_rate': 6.382222222222223e-06, 'epoch': 34.08, 'step': 2130}
{'loss': 0.4915, 'learning_rate': 6.371111111111111e-06, 'epoch': 34.16, 'step': 2135}
{'loss': 0.4596, 'learning_rate': 6.360000000000001e-06, 'epoch': 34.24, 'step': 2140}
{'loss': 0.4909, 'learning_rate': 6.348888888888889e-06, 'epoch': 34.32, 'step': 2145}
{'loss': 0.4945, 'learning_rate': 6.3377777777777786e-06, 'epoch': 34.4, 'step': 2150}
{'loss': 0.528, 'learning_rate': 6.326666666666667e-06, 'epoch': 34.48, 'step': 2155}
{'loss': 0.4891, 'learning_rate': 6.315555555555556e-06, 'epoch': 34.56, 'step': 2160}
{'loss': 0.475, 'learning_rate': 6.304444444444445e-06, 'epoch': 34.64, 'step': 2165}
{'loss': 0.4939, 'learning_rate': 6.293333333333334e-06, 'epoch': 34.72, 'step': 2170}
{'loss': 0.4926, 'learning_rate': 6.282222222222223e-06, 'epoch': 34.8, 'step': 2175}
{'loss': 0.5109, 'learning_rate': 6.271111111111111e-06, 'epoch': 34.88, 'step': 2180}
{'loss': 0.4811, 'learning_rate': 6.26e-06, 'epoch': 34.96, 'step': 2185}
{'loss': 0.4721, 'learning_rate': 6.24888888888889e-06, 'epoch': 35.04, 'step': 2190}
{'loss': 0.4677, 'learning_rate': 6.237777777777778e-06, 'epoch': 35.12, 'step': 2195}
{'loss': 0.4707, 'learning_rate': 6.2266666666666675e-06, 'epoch': 35.2, 'step': 2200}
{'loss': 0.4806, 'learning_rate': 6.2155555555555554e-06, 'epoch': 35.28, 'step': 2205}
{'loss': 0.4623, 'learning_rate': 6.204444444444445e-06, 'epoch': 35.36, 'step': 2210}
{'loss': 0.5318, 'learning_rate': 6.193333333333333e-06, 'epoch': 35.44, 'step': 2215}
{'loss': 0.4946, 'learning_rate': 6.182222222222223e-06, 'epoch': 35.52, 'step': 2220}
{'loss': 0.4807, 'learning_rate': 6.171111111111112e-06, 'epoch': 35.6, 'step': 2225}
{'loss': 0.4773, 'learning_rate': 6.16e-06, 'epoch': 35.68, 'step': 2230}
{'loss': 0.4839, 'learning_rate': 6.14888888888889e-06, 'epoch': 35.76, 'step': 2235}
{'loss': 0.4883, 'learning_rate': 6.137777777777778e-06, 'epoch': 35.84, 'step': 2240}
{'loss': 0.4933, 'learning_rate': 6.126666666666668e-06, 'epoch': 35.92, 'step': 2245}
{'loss': 0.461, 'learning_rate': 6.1155555555555555e-06, 'epoch': 36.0, 'step': 2250}
{'loss': 0.5061, 'learning_rate': 6.104444444444445e-06, 'epoch': 36.08, 'step': 2255}
{'loss': 0.4881, 'learning_rate': 6.093333333333333e-06, 'epoch': 36.16, 'step': 2260}
{'loss': 0.4691, 'learning_rate': 6.082222222222223e-06, 'epoch': 36.24, 'step': 2265}
{'loss': 0.5153, 'learning_rate': 6.0711111111111125e-06, 'epoch': 36.32, 'step': 2270}
{'loss': 0.4735, 'learning_rate': 6.0600000000000004e-06, 'epoch': 36.4, 'step': 2275}
{'loss': 0.4955, 'learning_rate': 6.04888888888889e-06, 'epoch': 36.48, 'step': 2280}
{'loss': 0.4684, 'learning_rate': 6.037777777777778e-06, 'epoch': 36.56, 'step': 2285}
{'loss': 0.4874, 'learning_rate': 6.026666666666668e-06, 'epoch': 36.64, 'step': 2290}
{'loss': 0.5007, 'learning_rate': 6.015555555555556e-06, 'epoch': 36.72, 'step': 2295}
{'loss': 0.4615, 'learning_rate': 6.004444444444445e-06, 'epoch': 36.8, 'step': 2300}
{'loss': 0.4617, 'learning_rate': 5.993333333333334e-06, 'epoch': 36.88, 'step': 2305}
{'loss': 0.4808, 'learning_rate': 5.982222222222223e-06, 'epoch': 36.96, 'step': 2310}
{'loss': 0.4475, 'learning_rate': 5.971111111111112e-06, 'epoch': 37.04, 'step': 2315}
{'loss': 0.4805, 'learning_rate': 5.9600000000000005e-06, 'epoch': 37.12, 'step': 2320}
{'loss': 0.4844, 'learning_rate': 5.948888888888889e-06, 'epoch': 37.2, 'step': 2325}
{'loss': 0.5089, 'learning_rate': 5.937777777777778e-06, 'epoch': 37.28, 'step': 2330}
{'loss': 0.4727, 'learning_rate': 5.926666666666667e-06, 'epoch': 37.36, 'step': 2335}
{'loss': 0.4883, 'learning_rate': 5.915555555555556e-06, 'epoch': 37.44, 'step': 2340}
{'loss': 0.5446, 'learning_rate': 5.9044444444444446e-06, 'epoch': 37.52, 'step': 2345}
{'loss': 0.4558, 'learning_rate': 5.893333333333334e-06, 'epoch': 37.6, 'step': 2350}
{'loss': 0.4785, 'learning_rate': 5.882222222222222e-06, 'epoch': 37.68, 'step': 2355}
{'loss': 0.4844, 'learning_rate': 5.871111111111112e-06, 'epoch': 37.76, 'step': 2360}
{'loss': 0.4727, 'learning_rate': 5.86e-06, 'epoch': 37.84, 'step': 2365}
{'loss': 0.501, 'learning_rate': 5.8488888888888895e-06, 'epoch': 37.92, 'step': 2370}
{'loss': 0.5085, 'learning_rate': 5.837777777777777e-06, 'epoch': 38.0, 'step': 2375}
{'loss': 0.4768, 'learning_rate': 5.826666666666667e-06, 'epoch': 38.08, 'step': 2380}
{'loss': 0.4845, 'learning_rate': 5.815555555555557e-06, 'epoch': 38.16, 'step': 2385}
{'loss': 0.4983, 'learning_rate': 5.804444444444445e-06, 'epoch': 38.24, 'step': 2390}
{'loss': 0.4732, 'learning_rate': 5.793333333333334e-06, 'epoch': 38.32, 'step': 2395}
{'loss': 0.4794, 'learning_rate': 5.782222222222222e-06, 'epoch': 38.4, 'step': 2400}
{'loss': 0.4797, 'learning_rate': 5.771111111111112e-06, 'epoch': 38.48, 'step': 2405}
{'loss': 0.4989, 'learning_rate': 5.76e-06, 'epoch': 38.56, 'step': 2410}
{'loss': 0.4686, 'learning_rate': 5.7488888888888896e-06, 'epoch': 38.64, 'step': 2415}
{'loss': 0.4844, 'learning_rate': 5.737777777777778e-06, 'epoch': 38.72, 'step': 2420}
{'loss': 0.4813, 'learning_rate': 5.726666666666667e-06, 'epoch': 38.8, 'step': 2425}
{'loss': 0.4898, 'learning_rate': 5.715555555555557e-06, 'epoch': 38.88, 'step': 2430}
{'loss': 0.4903, 'learning_rate': 5.704444444444445e-06, 'epoch': 38.96, 'step': 2435}
{'loss': 0.4847, 'learning_rate': 5.6933333333333344e-06, 'epoch': 39.04, 'step': 2440}
{'loss': 0.464, 'learning_rate': 5.682222222222222e-06, 'epoch': 39.12, 'step': 2445}
{'loss': 0.4983, 'learning_rate': 5.671111111111112e-06, 'epoch': 39.2, 'step': 2450}
{'loss': 0.4847, 'learning_rate': 5.66e-06, 'epoch': 39.28, 'step': 2455}
{'loss': 0.4635, 'learning_rate': 5.64888888888889e-06, 'epoch': 39.36, 'step': 2460}
{'loss': 0.4822, 'learning_rate': 5.6377777777777785e-06, 'epoch': 39.44, 'step': 2465}
{'loss': 0.4836, 'learning_rate': 5.626666666666667e-06, 'epoch': 39.52, 'step': 2470}
{'loss': 0.4882, 'learning_rate': 5.615555555555556e-06, 'epoch': 39.6, 'step': 2475}
{'loss': 0.5093, 'learning_rate': 5.604444444444445e-06, 'epoch': 39.68, 'step': 2480}
{'loss': 0.4649, 'learning_rate': 5.593333333333334e-06, 'epoch': 39.76, 'step': 2485}
{'loss': 0.4566, 'learning_rate': 5.5822222222222225e-06, 'epoch': 39.84, 'step': 2490}
{'loss': 0.4844, 'learning_rate': 5.571111111111111e-06, 'epoch': 39.92, 'step': 2495}
{'loss': 0.4836, 'learning_rate': 5.560000000000001e-06, 'epoch': 40.0, 'step': 2500}
{'loss': 0.5045, 'learning_rate': 5.548888888888889e-06, 'epoch': 40.08, 'step': 2505}
{'loss': 0.5381, 'learning_rate': 5.537777777777779e-06, 'epoch': 40.16, 'step': 2510}
{'loss': 0.4916, 'learning_rate': 5.5266666666666666e-06, 'epoch': 40.24, 'step': 2515}
{'loss': 0.4666, 'learning_rate': 5.515555555555556e-06, 'epoch': 40.32, 'step': 2520}
{'loss': 0.4621, 'learning_rate': 5.504444444444444e-06, 'epoch': 40.4, 'step': 2525}
{'loss': 0.4616, 'learning_rate': 5.493333333333334e-06, 'epoch': 40.48, 'step': 2530}
{'loss': 0.5248, 'learning_rate': 5.4822222222222235e-06, 'epoch': 40.56, 'step': 2535}
{'loss': 0.4617, 'learning_rate': 5.4711111111111114e-06, 'epoch': 40.64, 'step': 2540}
{'loss': 0.4931, 'learning_rate': 5.460000000000001e-06, 'epoch': 40.72, 'step': 2545}
{'loss': 0.46, 'learning_rate': 5.448888888888889e-06, 'epoch': 40.8, 'step': 2550}
{'loss': 0.5007, 'learning_rate': 5.437777777777779e-06, 'epoch': 40.88, 'step': 2555}
{'loss': 0.4965, 'learning_rate': 5.426666666666667e-06, 'epoch': 40.96, 'step': 2560}
{'loss': 0.475, 'learning_rate': 5.415555555555556e-06, 'epoch': 41.04, 'step': 2565}
{'loss': 0.4851, 'learning_rate': 5.404444444444444e-06, 'epoch': 41.12, 'step': 2570}
{'loss': 0.4856, 'learning_rate': 5.393333333333334e-06, 'epoch': 41.2, 'step': 2575}
{'loss': 0.4592, 'learning_rate': 5.382222222222223e-06, 'epoch': 41.28, 'step': 2580}
{'loss': 0.4682, 'learning_rate': 5.3711111111111115e-06, 'epoch': 41.36, 'step': 2585}
{'loss': 0.4671, 'learning_rate': 5.36e-06, 'epoch': 41.44, 'step': 2590}
{'loss': 0.4837, 'learning_rate': 5.348888888888889e-06, 'epoch': 41.52, 'step': 2595}
{'loss': 0.4623, 'learning_rate': 5.337777777777779e-06, 'epoch': 41.6, 'step': 2600}
{'loss': 0.4766, 'learning_rate': 5.326666666666667e-06, 'epoch': 41.68, 'step': 2605}
{'loss': 0.4825, 'learning_rate': 5.3155555555555564e-06, 'epoch': 41.76, 'step': 2610}
{'loss': 0.4956, 'learning_rate': 5.304444444444445e-06, 'epoch': 41.84, 'step': 2615}
{'loss': 0.4546, 'learning_rate': 5.293333333333334e-06, 'epoch': 41.92, 'step': 2620}
{'loss': 0.4794, 'learning_rate': 5.282222222222223e-06, 'epoch': 42.0, 'step': 2625}
{'loss': 0.4868, 'learning_rate': 5.271111111111112e-06, 'epoch': 42.08, 'step': 2630}
{'loss': 0.4672, 'learning_rate': 5.2600000000000005e-06, 'epoch': 42.16, 'step': 2635}
{'loss': 0.4895, 'learning_rate': 5.248888888888889e-06, 'epoch': 42.24, 'step': 2640}
{'loss': 0.5049, 'learning_rate': 5.237777777777778e-06, 'epoch': 42.32, 'step': 2645}
{'loss': 0.4901, 'learning_rate': 5.226666666666667e-06, 'epoch': 42.4, 'step': 2650}
{'loss': 0.4632, 'learning_rate': 5.215555555555556e-06, 'epoch': 42.48, 'step': 2655}
{'loss': 0.4756, 'learning_rate': 5.204444444444445e-06, 'epoch': 42.56, 'step': 2660}
{'loss': 0.4863, 'learning_rate': 5.193333333333333e-06, 'epoch': 42.64, 'step': 2665}
{'loss': 0.4728, 'learning_rate': 5.182222222222223e-06, 'epoch': 42.72, 'step': 2670}
{'loss': 0.4644, 'learning_rate': 5.171111111111111e-06, 'epoch': 42.8, 'step': 2675}
{'loss': 0.475, 'learning_rate': 5.1600000000000006e-06, 'epoch': 42.88, 'step': 2680}
{'loss': 0.5164, 'learning_rate': 5.1488888888888885e-06, 'epoch': 42.96, 'step': 2685}
{'loss': 0.479, 'learning_rate': 5.137777777777778e-06, 'epoch': 43.04, 'step': 2690}
{'loss': 0.462, 'learning_rate': 5.126666666666668e-06, 'epoch': 43.12, 'step': 2695}
{'loss': 0.4664, 'learning_rate': 5.115555555555556e-06, 'epoch': 43.2, 'step': 2700}
{'loss': 0.5024, 'learning_rate': 5.1044444444444455e-06, 'epoch': 43.28, 'step': 2705}
{'loss': 0.518, 'learning_rate': 5.093333333333333e-06, 'epoch': 43.36, 'step': 2710}
{'loss': 0.4951, 'learning_rate': 5.082222222222223e-06, 'epoch': 43.44, 'step': 2715}
{'loss': 0.4809, 'learning_rate': 5.071111111111111e-06, 'epoch': 43.52, 'step': 2720}
{'loss': 0.4633, 'learning_rate': 5.060000000000001e-06, 'epoch': 43.6, 'step': 2725}
{'loss': 0.4836, 'learning_rate': 5.0488888888888895e-06, 'epoch': 43.68, 'step': 2730}
{'loss': 0.4857, 'learning_rate': 5.037777777777778e-06, 'epoch': 43.76, 'step': 2735}
{'loss': 0.4762, 'learning_rate': 5.026666666666667e-06, 'epoch': 43.84, 'step': 2740}
{'loss': 0.4941, 'learning_rate': 5.015555555555556e-06, 'epoch': 43.92, 'step': 2745}
{'loss': 0.4654, 'learning_rate': 5.004444444444445e-06, 'epoch': 44.0, 'step': 2750}
{'loss': 0.4725, 'learning_rate': 4.9933333333333335e-06, 'epoch': 44.08, 'step': 2755}
{'loss': 0.4597, 'learning_rate': 4.982222222222222e-06, 'epoch': 44.16, 'step': 2760}
{'loss': 0.4799, 'learning_rate': 4.971111111111111e-06, 'epoch': 44.24, 'step': 2765}
{'loss': 0.4735, 'learning_rate': 4.960000000000001e-06, 'epoch': 44.32, 'step': 2770}
{'loss': 0.4655, 'learning_rate': 4.94888888888889e-06, 'epoch': 44.4, 'step': 2775}
{'loss': 0.4675, 'learning_rate': 4.937777777777778e-06, 'epoch': 44.48, 'step': 2780}
{'loss': 0.4745, 'learning_rate': 4.926666666666667e-06, 'epoch': 44.56, 'step': 2785}
{'loss': 0.4638, 'learning_rate': 4.915555555555556e-06, 'epoch': 44.64, 'step': 2790}
{'loss': 0.4661, 'learning_rate': 4.904444444444445e-06, 'epoch': 44.72, 'step': 2795}
{'loss': 0.4557, 'learning_rate': 4.893333333333334e-06, 'epoch': 44.8, 'step': 2800}
{'loss': 0.4753, 'learning_rate': 4.8822222222222224e-06, 'epoch': 44.88, 'step': 2805}
{'loss': 0.4685, 'learning_rate': 4.871111111111111e-06, 'epoch': 44.96, 'step': 2810}
{'loss': 0.4746, 'learning_rate': 4.86e-06, 'epoch': 45.04, 'step': 2815}
{'loss': 0.4558, 'learning_rate': 4.848888888888889e-06, 'epoch': 45.12, 'step': 2820}
{'loss': 0.4858, 'learning_rate': 4.837777777777778e-06, 'epoch': 45.2, 'step': 2825}
{'loss': 0.4667, 'learning_rate': 4.826666666666667e-06, 'epoch': 45.28, 'step': 2830}
{'loss': 0.4747, 'learning_rate': 4.815555555555556e-06, 'epoch': 45.36, 'step': 2835}
{'loss': 0.4865, 'learning_rate': 4.804444444444445e-06, 'epoch': 45.44, 'step': 2840}
{'loss': 0.4519, 'learning_rate': 4.793333333333334e-06, 'epoch': 45.52, 'step': 2845}
{'loss': 0.4675, 'learning_rate': 4.7822222222222226e-06, 'epoch': 45.6, 'step': 2850}
{'loss': 0.4568, 'learning_rate': 4.771111111111111e-06, 'epoch': 45.68, 'step': 2855}
{'loss': 0.4781, 'learning_rate': 4.76e-06, 'epoch': 45.76, 'step': 2860}
{'loss': 0.471, 'learning_rate': 4.74888888888889e-06, 'epoch': 45.84, 'step': 2865}
{'loss': 0.4906, 'learning_rate': 4.737777777777779e-06, 'epoch': 45.92, 'step': 2870}
{'loss': 0.4663, 'learning_rate': 4.7266666666666674e-06, 'epoch': 46.0, 'step': 2875}
{'loss': 0.466, 'learning_rate': 4.715555555555556e-06, 'epoch': 46.08, 'step': 2880}
{'loss': 0.4756, 'learning_rate': 4.704444444444445e-06, 'epoch': 46.16, 'step': 2885}
{'loss': 0.4594, 'learning_rate': 4.693333333333334e-06, 'epoch': 46.24, 'step': 2890}
{'loss': 0.4663, 'learning_rate': 4.682222222222223e-06, 'epoch': 46.32, 'step': 2895}
{'loss': 0.4793, 'learning_rate': 4.6711111111111115e-06, 'epoch': 46.4, 'step': 2900}
{'loss': 0.469, 'learning_rate': 4.66e-06, 'epoch': 46.48, 'step': 2905}
{'loss': 0.4714, 'learning_rate': 4.648888888888889e-06, 'epoch': 46.56, 'step': 2910}
{'loss': 0.4857, 'learning_rate': 4.637777777777778e-06, 'epoch': 46.64, 'step': 2915}
{'loss': 0.4807, 'learning_rate': 4.626666666666667e-06, 'epoch': 46.72, 'step': 2920}
{'loss': 0.4546, 'learning_rate': 4.6155555555555555e-06, 'epoch': 46.8, 'step': 2925}
{'loss': 0.4574, 'learning_rate': 4.604444444444444e-06, 'epoch': 46.88, 'step': 2930}
{'loss': 0.4539, 'learning_rate': 4.593333333333333e-06, 'epoch': 46.96, 'step': 2935}
{'loss': 0.4754, 'learning_rate': 4.582222222222223e-06, 'epoch': 47.04, 'step': 2940}
{'loss': 0.4482, 'learning_rate': 4.571111111111112e-06, 'epoch': 47.12, 'step': 2945}
{'loss': 0.4713, 'learning_rate': 4.56e-06, 'epoch': 47.2, 'step': 2950}
{'loss': 0.4796, 'learning_rate': 4.548888888888889e-06, 'epoch': 47.28, 'step': 2955}
{'loss': 0.46, 'learning_rate': 4.537777777777778e-06, 'epoch': 47.36, 'step': 2960}
{'loss': 0.4706, 'learning_rate': 4.526666666666667e-06, 'epoch': 47.44, 'step': 2965}
{'loss': 0.4585, 'learning_rate': 4.515555555555556e-06, 'epoch': 47.52, 'step': 2970}
{'loss': 0.4578, 'learning_rate': 4.504444444444444e-06, 'epoch': 47.6, 'step': 2975}
{'loss': 0.4576, 'learning_rate': 4.493333333333333e-06, 'epoch': 47.68, 'step': 2980}
{'loss': 0.4631, 'learning_rate': 4.482222222222223e-06, 'epoch': 47.76, 'step': 2985}
{'loss': 0.5388, 'learning_rate': 4.471111111111112e-06, 'epoch': 47.84, 'step': 2990}
{'loss': 0.464, 'learning_rate': 4.4600000000000005e-06, 'epoch': 47.92, 'step': 2995}
{'loss': 0.4753, 'learning_rate': 4.448888888888889e-06, 'epoch': 48.0, 'step': 3000}
{'eval_loss': 0.4889301657676697, 'eval_runtime': 0.0729, 'eval_samples_per_second': 68.547, 'eval_steps_per_second': 13.709, 'epoch': 48.0, 'step': 3000}
{'loss': 0.4506, 'learning_rate': 4.437777777777778e-06, 'epoch': 48.08, 'step': 3005}
{'loss': 0.4575, 'learning_rate': 4.426666666666667e-06, 'epoch': 48.16, 'step': 3010}
{'loss': 0.4759, 'learning_rate': 4.415555555555556e-06, 'epoch': 48.24, 'step': 3015}
{'loss': 0.4852, 'learning_rate': 4.404444444444445e-06, 'epoch': 48.32, 'step': 3020}
{'loss': 0.4853, 'learning_rate': 4.393333333333334e-06, 'epoch': 48.4, 'step': 3025}
{'loss': 0.46, 'learning_rate': 4.382222222222223e-06, 'epoch': 48.48, 'step': 3030}
{'loss': 0.4428, 'learning_rate': 4.371111111111112e-06, 'epoch': 48.56, 'step': 3035}
{'loss': 0.4752, 'learning_rate': 4.360000000000001e-06, 'epoch': 48.64, 'step': 3040}
{'loss': 0.4722, 'learning_rate': 4.348888888888889e-06, 'epoch': 48.72, 'step': 3045}
{'loss': 0.4644, 'learning_rate': 4.337777777777778e-06, 'epoch': 48.8, 'step': 3050}
{'loss': 0.47, 'learning_rate': 4.326666666666667e-06, 'epoch': 48.88, 'step': 3055}
{'loss': 0.4575, 'learning_rate': 4.315555555555556e-06, 'epoch': 48.96, 'step': 3060}
{'loss': 0.4632, 'learning_rate': 4.304444444444445e-06, 'epoch': 49.04, 'step': 3065}
{'loss': 0.4704, 'learning_rate': 4.2933333333333334e-06, 'epoch': 49.12, 'step': 3070}
{'loss': 0.464, 'learning_rate': 4.282222222222222e-06, 'epoch': 49.2, 'step': 3075}
{'loss': 0.4486, 'learning_rate': 4.271111111111111e-06, 'epoch': 49.28, 'step': 3080}
{'loss': 0.4563, 'learning_rate': 4.26e-06, 'epoch': 49.36, 'step': 3085}
{'loss': 0.4707, 'learning_rate': 4.248888888888889e-06, 'epoch': 49.44, 'step': 3090}
{'loss': 0.4892, 'learning_rate': 4.2377777777777775e-06, 'epoch': 49.52, 'step': 3095}
{'loss': 0.4622, 'learning_rate': 4.226666666666667e-06, 'epoch': 49.6, 'step': 3100}
{'loss': 0.494, 'learning_rate': 4.215555555555556e-06, 'epoch': 49.68, 'step': 3105}
{'loss': 0.4842, 'learning_rate': 4.204444444444445e-06, 'epoch': 49.76, 'step': 3110}
{'loss': 0.4603, 'learning_rate': 4.1933333333333336e-06, 'epoch': 49.84, 'step': 3115}
{'loss': 0.4642, 'learning_rate': 4.182222222222222e-06, 'epoch': 49.92, 'step': 3120}
{'loss': 0.4738, 'learning_rate': 4.171111111111111e-06, 'epoch': 50.0, 'step': 3125}
{'loss': 0.4889, 'learning_rate': 4.16e-06, 'epoch': 50.08, 'step': 3130}
{'loss': 0.4975, 'learning_rate': 4.148888888888889e-06, 'epoch': 50.16, 'step': 3135}
{'loss': 0.4731, 'learning_rate': 4.1377777777777784e-06, 'epoch': 50.24, 'step': 3140}
{'loss': 0.4664, 'learning_rate': 4.126666666666667e-06, 'epoch': 50.32, 'step': 3145}
{'loss': 0.4517, 'learning_rate': 4.115555555555556e-06, 'epoch': 50.4, 'step': 3150}
{'loss': 0.4674, 'learning_rate': 4.104444444444445e-06, 'epoch': 50.48, 'step': 3155}
{'loss': 0.4663, 'learning_rate': 4.093333333333334e-06, 'epoch': 50.56, 'step': 3160}
{'loss': 0.4764, 'learning_rate': 4.0822222222222225e-06, 'epoch': 50.64, 'step': 3165}
{'loss': 0.4598, 'learning_rate': 4.071111111111111e-06, 'epoch': 50.72, 'step': 3170}
{'loss': 0.4796, 'learning_rate': 4.060000000000001e-06, 'epoch': 50.8, 'step': 3175}
{'loss': 0.4601, 'learning_rate': 4.04888888888889e-06, 'epoch': 50.88, 'step': 3180}
{'loss': 0.4644, 'learning_rate': 4.0377777777777786e-06, 'epoch': 50.96, 'step': 3185}
{'loss': 0.4407, 'learning_rate': 4.026666666666667e-06, 'epoch': 51.04, 'step': 3190}
{'loss': 0.4737, 'learning_rate': 4.015555555555556e-06, 'epoch': 51.12, 'step': 3195}
{'loss': 0.532, 'learning_rate': 4.004444444444445e-06, 'epoch': 51.2, 'step': 3200}
{'loss': 0.4651, 'learning_rate': 3.993333333333334e-06, 'epoch': 51.28, 'step': 3205}
{'loss': 0.4711, 'learning_rate': 3.982222222222223e-06, 'epoch': 51.36, 'step': 3210}
{'loss': 0.478, 'learning_rate': 3.971111111111111e-06, 'epoch': 51.44, 'step': 3215}
{'loss': 0.4511, 'learning_rate': 3.96e-06, 'epoch': 51.52, 'step': 3220}
{'loss': 0.4651, 'learning_rate': 3.948888888888889e-06, 'epoch': 51.6, 'step': 3225}
{'loss': 0.476, 'learning_rate': 3.937777777777778e-06, 'epoch': 51.68, 'step': 3230}
{'loss': 0.4614, 'learning_rate': 3.926666666666667e-06, 'epoch': 51.76, 'step': 3235}
{'loss': 0.4816, 'learning_rate': 3.9155555555555554e-06, 'epoch': 51.84, 'step': 3240}
{'loss': 0.4942, 'learning_rate': 3.904444444444444e-06, 'epoch': 51.92, 'step': 3245}
{'loss': 0.4735, 'learning_rate': 3.893333333333333e-06, 'epoch': 52.0, 'step': 3250}
{'loss': 0.4925, 'learning_rate': 3.882222222222223e-06, 'epoch': 52.08, 'step': 3255}
{'loss': 0.449, 'learning_rate': 3.8711111111111115e-06, 'epoch': 52.16, 'step': 3260}
{'loss': 0.46, 'learning_rate': 3.86e-06, 'epoch': 52.24, 'step': 3265}
{'loss': 0.4884, 'learning_rate': 3.848888888888889e-06, 'epoch': 52.32, 'step': 3270}
{'loss': 0.4898, 'learning_rate': 3.837777777777778e-06, 'epoch': 52.4, 'step': 3275}
{'loss': 0.4836, 'learning_rate': 3.826666666666667e-06, 'epoch': 52.48, 'step': 3280}
{'loss': 0.4512, 'learning_rate': 3.8155555555555555e-06, 'epoch': 52.56, 'step': 3285}
{'loss': 0.4663, 'learning_rate': 3.8044444444444443e-06, 'epoch': 52.64, 'step': 3290}
{'loss': 0.4679, 'learning_rate': 3.793333333333334e-06, 'epoch': 52.72, 'step': 3295}
{'loss': 0.452, 'learning_rate': 3.782222222222223e-06, 'epoch': 52.8, 'step': 3300}
{'loss': 0.5056, 'learning_rate': 3.7711111111111116e-06, 'epoch': 52.88, 'step': 3305}
{'loss': 0.4777, 'learning_rate': 3.7600000000000004e-06, 'epoch': 52.96, 'step': 3310}
{'loss': 0.4994, 'learning_rate': 3.7488888888888892e-06, 'epoch': 53.04, 'step': 3315}
{'loss': 0.4546, 'learning_rate': 3.737777777777778e-06, 'epoch': 53.12, 'step': 3320}
{'loss': 0.4773, 'learning_rate': 3.726666666666667e-06, 'epoch': 53.2, 'step': 3325}
{'loss': 0.4738, 'learning_rate': 3.7155555555555557e-06, 'epoch': 53.28, 'step': 3330}
{'loss': 0.4531, 'learning_rate': 3.704444444444445e-06, 'epoch': 53.36, 'step': 3335}
{'loss': 0.4669, 'learning_rate': 3.6933333333333337e-06, 'epoch': 53.44, 'step': 3340}
{'loss': 0.4574, 'learning_rate': 3.6822222222222225e-06, 'epoch': 53.52, 'step': 3345}
{'loss': 0.444, 'learning_rate': 3.6711111111111113e-06, 'epoch': 53.6, 'step': 3350}
{'loss': 0.4535, 'learning_rate': 3.66e-06, 'epoch': 53.68, 'step': 3355}
{'loss': 0.4565, 'learning_rate': 3.648888888888889e-06, 'epoch': 53.76, 'step': 3360}
{'loss': 0.5278, 'learning_rate': 3.6377777777777777e-06, 'epoch': 53.84, 'step': 3365}
{'loss': 0.4555, 'learning_rate': 3.6266666666666674e-06, 'epoch': 53.92, 'step': 3370}
{'loss': 0.4844, 'learning_rate': 3.615555555555556e-06, 'epoch': 54.0, 'step': 3375}
{'loss': 0.4847, 'learning_rate': 3.604444444444445e-06, 'epoch': 54.08, 'step': 3380}
{'loss': 0.4741, 'learning_rate': 3.593333333333334e-06, 'epoch': 54.16, 'step': 3385}
{'loss': 0.4572, 'learning_rate': 3.5822222222222226e-06, 'epoch': 54.24, 'step': 3390}
{'loss': 0.4761, 'learning_rate': 3.5711111111111114e-06, 'epoch': 54.32, 'step': 3395}
{'loss': 0.4806, 'learning_rate': 3.5600000000000002e-06, 'epoch': 54.4, 'step': 3400}
{'loss': 0.473, 'learning_rate': 3.548888888888889e-06, 'epoch': 54.48, 'step': 3405}
{'loss': 0.4754, 'learning_rate': 3.5377777777777783e-06, 'epoch': 54.56, 'step': 3410}
{'loss': 0.4849, 'learning_rate': 3.526666666666667e-06, 'epoch': 54.64, 'step': 3415}
{'loss': 0.4563, 'learning_rate': 3.515555555555556e-06, 'epoch': 54.72, 'step': 3420}
{'loss': 0.4791, 'learning_rate': 3.5044444444444447e-06, 'epoch': 54.8, 'step': 3425}
{'loss': 0.4524, 'learning_rate': 3.4933333333333335e-06, 'epoch': 54.88, 'step': 3430}
{'loss': 0.4468, 'learning_rate': 3.4822222222222223e-06, 'epoch': 54.96, 'step': 3435}
{'loss': 0.4755, 'learning_rate': 3.471111111111111e-06, 'epoch': 55.04, 'step': 3440}
{'loss': 0.4479, 'learning_rate': 3.46e-06, 'epoch': 55.12, 'step': 3445}
{'loss': 0.4877, 'learning_rate': 3.4488888888888896e-06, 'epoch': 55.2, 'step': 3450}
{'loss': 0.4622, 'learning_rate': 3.4377777777777784e-06, 'epoch': 55.28, 'step': 3455}
{'loss': 0.4919, 'learning_rate': 3.426666666666667e-06, 'epoch': 55.36, 'step': 3460}
{'loss': 0.4534, 'learning_rate': 3.415555555555556e-06, 'epoch': 55.44, 'step': 3465}
{'loss': 0.4655, 'learning_rate': 3.404444444444445e-06, 'epoch': 55.52, 'step': 3470}
{'loss': 0.4482, 'learning_rate': 3.3933333333333336e-06, 'epoch': 55.6, 'step': 3475}
{'loss': 0.4541, 'learning_rate': 3.3822222222222224e-06, 'epoch': 55.68, 'step': 3480}
{'loss': 0.4457, 'learning_rate': 3.371111111111111e-06, 'epoch': 55.76, 'step': 3485}
{'loss': 0.4861, 'learning_rate': 3.3600000000000004e-06, 'epoch': 55.84, 'step': 3490}
{'loss': 0.4637, 'learning_rate': 3.3488888888888892e-06, 'epoch': 55.92, 'step': 3495}
{'loss': 0.4636, 'learning_rate': 3.337777777777778e-06, 'epoch': 56.0, 'step': 3500}
{'loss': 0.4768, 'learning_rate': 3.326666666666667e-06, 'epoch': 56.08, 'step': 3505}
{'loss': 0.4967, 'learning_rate': 3.3155555555555557e-06, 'epoch': 56.16, 'step': 3510}
{'loss': 0.4829, 'learning_rate': 3.3044444444444445e-06, 'epoch': 56.24, 'step': 3515}
{'loss': 0.482, 'learning_rate': 3.2933333333333333e-06, 'epoch': 56.32, 'step': 3520}
{'loss': 0.4661, 'learning_rate': 3.282222222222223e-06, 'epoch': 56.4, 'step': 3525}
{'loss': 0.4474, 'learning_rate': 3.2711111111111117e-06, 'epoch': 56.48, 'step': 3530}
{'loss': 0.4943, 'learning_rate': 3.2600000000000006e-06, 'epoch': 56.56, 'step': 3535}
{'loss': 0.463, 'learning_rate': 3.2488888888888894e-06, 'epoch': 56.64, 'step': 3540}
{'loss': 0.4838, 'learning_rate': 3.237777777777778e-06, 'epoch': 56.72, 'step': 3545}
{'loss': 0.4684, 'learning_rate': 3.226666666666667e-06, 'epoch': 56.8, 'step': 3550}
{'loss': 0.4785, 'learning_rate': 3.2155555555555558e-06, 'epoch': 56.88, 'step': 3555}
{'loss': 0.4696, 'learning_rate': 3.2044444444444446e-06, 'epoch': 56.96, 'step': 3560}
{'loss': 0.4522, 'learning_rate': 3.193333333333334e-06, 'epoch': 57.04, 'step': 3565}
{'loss': 0.454, 'learning_rate': 3.1822222222222226e-06, 'epoch': 57.12, 'step': 3570}
{'loss': 0.4664, 'learning_rate': 3.1711111111111114e-06, 'epoch': 57.2, 'step': 3575}
{'loss': 0.4638, 'learning_rate': 3.1600000000000002e-06, 'epoch': 57.28, 'step': 3580}
{'loss': 0.4689, 'learning_rate': 3.148888888888889e-06, 'epoch': 57.36, 'step': 3585}
{'loss': 0.5178, 'learning_rate': 3.137777777777778e-06, 'epoch': 57.44, 'step': 3590}
{'loss': 0.4692, 'learning_rate': 3.1266666666666667e-06, 'epoch': 57.52, 'step': 3595}
{'loss': 0.4583, 'learning_rate': 3.1155555555555555e-06, 'epoch': 57.6, 'step': 3600}
{'loss': 0.4556, 'learning_rate': 3.104444444444445e-06, 'epoch': 57.68, 'step': 3605}
{'loss': 0.4856, 'learning_rate': 3.093333333333334e-06, 'epoch': 57.76, 'step': 3610}
{'loss': 0.4787, 'learning_rate': 3.0822222222222227e-06, 'epoch': 57.84, 'step': 3615}
{'loss': 0.4754, 'learning_rate': 3.0711111111111115e-06, 'epoch': 57.92, 'step': 3620}
{'loss': 0.4633, 'learning_rate': 3.0600000000000003e-06, 'epoch': 58.0, 'step': 3625}
{'loss': 0.474, 'learning_rate': 3.048888888888889e-06, 'epoch': 58.08, 'step': 3630}
{'loss': 0.46, 'learning_rate': 3.037777777777778e-06, 'epoch': 58.16, 'step': 3635}
{'loss': 0.456, 'learning_rate': 3.0266666666666668e-06, 'epoch': 58.24, 'step': 3640}
{'loss': 0.4532, 'learning_rate': 3.015555555555556e-06, 'epoch': 58.32, 'step': 3645}
{'loss': 0.4852, 'learning_rate': 3.004444444444445e-06, 'epoch': 58.4, 'step': 3650}
{'loss': 0.4908, 'learning_rate': 2.9933333333333336e-06, 'epoch': 58.48, 'step': 3655}
{'loss': 0.4524, 'learning_rate': 2.9822222222222224e-06, 'epoch': 58.56, 'step': 3660}
{'loss': 0.468, 'learning_rate': 2.9711111111111112e-06, 'epoch': 58.64, 'step': 3665}
{'loss': 0.5117, 'learning_rate': 2.96e-06, 'epoch': 58.72, 'step': 3670}
{'loss': 0.454, 'learning_rate': 2.948888888888889e-06, 'epoch': 58.8, 'step': 3675}
{'loss': 0.4639, 'learning_rate': 2.937777777777778e-06, 'epoch': 58.88, 'step': 3680}
{'loss': 0.4512, 'learning_rate': 2.9266666666666673e-06, 'epoch': 58.96, 'step': 3685}
{'loss': 0.4674, 'learning_rate': 2.915555555555556e-06, 'epoch': 59.04, 'step': 3690}
{'loss': 0.4462, 'learning_rate': 2.904444444444445e-06, 'epoch': 59.12, 'step': 3695}
{'loss': 0.4689, 'learning_rate': 2.8933333333333337e-06, 'epoch': 59.2, 'step': 3700}
{'loss': 0.4642, 'learning_rate': 2.8822222222222225e-06, 'epoch': 59.28, 'step': 3705}
{'loss': 0.4684, 'learning_rate': 2.8711111111111113e-06, 'epoch': 59.36, 'step': 3710}
{'loss': 0.4513, 'learning_rate': 2.86e-06, 'epoch': 59.44, 'step': 3715}
{'loss': 0.4466, 'learning_rate': 2.8488888888888894e-06, 'epoch': 59.52, 'step': 3720}
{'loss': 0.4541, 'learning_rate': 2.837777777777778e-06, 'epoch': 59.6, 'step': 3725}
{'loss': 0.4718, 'learning_rate': 2.826666666666667e-06, 'epoch': 59.68, 'step': 3730}
{'loss': 0.4626, 'learning_rate': 2.815555555555556e-06, 'epoch': 59.76, 'step': 3735}
{'loss': 0.4493, 'learning_rate': 2.8044444444444446e-06, 'epoch': 59.84, 'step': 3740}
{'loss': 0.4794, 'learning_rate': 2.7933333333333334e-06, 'epoch': 59.92, 'step': 3745}
{'loss': 0.5015, 'learning_rate': 2.7822222222222222e-06, 'epoch': 60.0, 'step': 3750}
{'loss': 0.439, 'learning_rate': 2.771111111111111e-06, 'epoch': 60.08, 'step': 3755}
{'loss': 0.4954, 'learning_rate': 2.7600000000000003e-06, 'epoch': 60.16, 'step': 3760}
{'loss': 0.455, 'learning_rate': 2.748888888888889e-06, 'epoch': 60.24, 'step': 3765}
{'loss': 0.4787, 'learning_rate': 2.7377777777777783e-06, 'epoch': 60.32, 'step': 3770}
{'loss': 0.4619, 'learning_rate': 2.726666666666667e-06, 'epoch': 60.4, 'step': 3775}
{'loss': 0.4821, 'learning_rate': 2.715555555555556e-06, 'epoch': 60.48, 'step': 3780}
{'loss': 0.4825, 'learning_rate': 2.7044444444444447e-06, 'epoch': 60.56, 'step': 3785}
{'loss': 0.4832, 'learning_rate': 2.6933333333333335e-06, 'epoch': 60.64, 'step': 3790}
{'loss': 0.4525, 'learning_rate': 2.6822222222222223e-06, 'epoch': 60.72, 'step': 3795}
{'loss': 0.4664, 'learning_rate': 2.6711111111111116e-06, 'epoch': 60.8, 'step': 3800}
{'loss': 0.4599, 'learning_rate': 2.6600000000000004e-06, 'epoch': 60.88, 'step': 3805}
{'loss': 0.4979, 'learning_rate': 2.648888888888889e-06, 'epoch': 60.96, 'step': 3810}
{'loss': 0.4606, 'learning_rate': 2.637777777777778e-06, 'epoch': 61.04, 'step': 3815}
{'loss': 0.4867, 'learning_rate': 2.6266666666666668e-06, 'epoch': 61.12, 'step': 3820}
{'loss': 0.4922, 'learning_rate': 2.6155555555555556e-06, 'epoch': 61.2, 'step': 3825}
{'loss': 0.4577, 'learning_rate': 2.6044444444444444e-06, 'epoch': 61.28, 'step': 3830}
{'loss': 0.4578, 'learning_rate': 2.5933333333333336e-06, 'epoch': 61.36, 'step': 3835}
{'loss': 0.4716, 'learning_rate': 2.5822222222222224e-06, 'epoch': 61.44, 'step': 3840}
{'loss': 0.4693, 'learning_rate': 2.5711111111111112e-06, 'epoch': 61.52, 'step': 3845}
{'loss': 0.4643, 'learning_rate': 2.56e-06, 'epoch': 61.6, 'step': 3850}
{'loss': 0.4469, 'learning_rate': 2.5488888888888893e-06, 'epoch': 61.68, 'step': 3855}
{'loss': 0.4504, 'learning_rate': 2.537777777777778e-06, 'epoch': 61.76, 'step': 3860}
{'loss': 0.5053, 'learning_rate': 2.526666666666667e-06, 'epoch': 61.84, 'step': 3865}
{'loss': 0.4793, 'learning_rate': 2.5155555555555557e-06, 'epoch': 61.92, 'step': 3870}
{'loss': 0.4622, 'learning_rate': 2.504444444444445e-06, 'epoch': 62.0, 'step': 3875}
{'loss': 0.4724, 'learning_rate': 2.4933333333333333e-06, 'epoch': 62.08, 'step': 3880}
{'loss': 0.4668, 'learning_rate': 2.4822222222222225e-06, 'epoch': 62.16, 'step': 3885}
{'loss': 0.4433, 'learning_rate': 2.4711111111111114e-06, 'epoch': 62.24, 'step': 3890}
{'loss': 0.4491, 'learning_rate': 2.46e-06, 'epoch': 62.32, 'step': 3895}
{'loss': 0.4502, 'learning_rate': 2.448888888888889e-06, 'epoch': 62.4, 'step': 3900}
{'loss': 0.4529, 'learning_rate': 2.437777777777778e-06, 'epoch': 62.48, 'step': 3905}
{'loss': 0.4551, 'learning_rate': 2.426666666666667e-06, 'epoch': 62.56, 'step': 3910}
{'loss': 0.4778, 'learning_rate': 2.415555555555556e-06, 'epoch': 62.64, 'step': 3915}
{'loss': 0.4634, 'learning_rate': 2.4044444444444446e-06, 'epoch': 62.72, 'step': 3920}
{'loss': 0.46, 'learning_rate': 2.3933333333333334e-06, 'epoch': 62.8, 'step': 3925}
{'loss': 0.4742, 'learning_rate': 2.3822222222222222e-06, 'epoch': 62.88, 'step': 3930}
{'loss': 0.4599, 'learning_rate': 2.371111111111111e-06, 'epoch': 62.96, 'step': 3935}
{'loss': 0.4913, 'learning_rate': 2.3600000000000003e-06, 'epoch': 63.04, 'step': 3940}
{'loss': 0.4717, 'learning_rate': 2.348888888888889e-06, 'epoch': 63.12, 'step': 3945}
{'loss': 0.4842, 'learning_rate': 2.337777777777778e-06, 'epoch': 63.2, 'step': 3950}
{'loss': 0.456, 'learning_rate': 2.3266666666666667e-06, 'epoch': 63.28, 'step': 3955}
{'loss': 0.4589, 'learning_rate': 2.3155555555555555e-06, 'epoch': 63.36, 'step': 3960}
{'loss': 0.4596, 'learning_rate': 2.3044444444444447e-06, 'epoch': 63.44, 'step': 3965}
{'loss': 0.4334, 'learning_rate': 2.2955555555555557e-06, 'epoch': 63.52, 'step': 3970}
{'loss': 0.505, 'learning_rate': 2.2844444444444445e-06, 'epoch': 63.6, 'step': 3975}
{'loss': 0.4808, 'learning_rate': 2.2733333333333333e-06, 'epoch': 63.68, 'step': 3980}
{'loss': 0.447, 'learning_rate': 2.262222222222222e-06, 'epoch': 63.76, 'step': 3985}
{'loss': 0.4684, 'learning_rate': 2.2511111111111113e-06, 'epoch': 63.84, 'step': 3990}
{'loss': 0.4552, 'learning_rate': 2.24e-06, 'epoch': 63.92, 'step': 3995}
{'loss': 0.4666, 'learning_rate': 2.228888888888889e-06, 'epoch': 64.0, 'step': 4000}
{'eval_loss': 0.4930969178676605, 'eval_runtime': 0.099, 'eval_samples_per_second': 50.515, 'eval_steps_per_second': 10.103, 'epoch': 64.0, 'step': 4000}
{'loss': 0.4535, 'learning_rate': 2.2177777777777778e-06, 'epoch': 64.08, 'step': 4005}
{'loss': 0.4619, 'learning_rate': 2.206666666666667e-06, 'epoch': 64.16, 'step': 4010}
{'loss': 0.4562, 'learning_rate': 2.195555555555556e-06, 'epoch': 64.24, 'step': 4015}
{'loss': 0.4661, 'learning_rate': 2.1844444444444446e-06, 'epoch': 64.32, 'step': 4020}
{'loss': 0.4788, 'learning_rate': 2.1733333333333334e-06, 'epoch': 64.4, 'step': 4025}
{'loss': 0.4485, 'learning_rate': 2.1622222222222226e-06, 'epoch': 64.48, 'step': 4030}
{'loss': 0.4557, 'learning_rate': 2.1511111111111115e-06, 'epoch': 64.56, 'step': 4035}
{'loss': 0.4755, 'learning_rate': 2.1400000000000003e-06, 'epoch': 64.64, 'step': 4040}
{'loss': 0.4495, 'learning_rate': 2.128888888888889e-06, 'epoch': 64.72, 'step': 4045}
{'loss': 0.4508, 'learning_rate': 2.117777777777778e-06, 'epoch': 64.8, 'step': 4050}
{'loss': 0.4598, 'learning_rate': 2.1066666666666667e-06, 'epoch': 64.88, 'step': 4055}
{'loss': 0.4683, 'learning_rate': 2.0955555555555555e-06, 'epoch': 64.96, 'step': 4060}
{'loss': 0.4543, 'learning_rate': 2.0844444444444443e-06, 'epoch': 65.04, 'step': 4065}
{'loss': 0.4805, 'learning_rate': 2.0733333333333335e-06, 'epoch': 65.12, 'step': 4070}
{'loss': 0.4704, 'learning_rate': 2.0622222222222223e-06, 'epoch': 65.2, 'step': 4075}
{'loss': 0.4548, 'learning_rate': 2.051111111111111e-06, 'epoch': 65.28, 'step': 4080}
{'loss': 0.4548, 'learning_rate': 2.04e-06, 'epoch': 65.36, 'step': 4085}
{'loss': 0.4556, 'learning_rate': 2.028888888888889e-06, 'epoch': 65.44, 'step': 4090}
{'loss': 0.4647, 'learning_rate': 2.017777777777778e-06, 'epoch': 65.52, 'step': 4095}
{'loss': 0.4802, 'learning_rate': 2.006666666666667e-06, 'epoch': 65.6, 'step': 4100}
{'loss': 0.451, 'learning_rate': 1.995555555555556e-06, 'epoch': 65.68, 'step': 4105}
{'loss': 0.4651, 'learning_rate': 1.984444444444445e-06, 'epoch': 65.76, 'step': 4110}
{'loss': 0.4445, 'learning_rate': 1.9733333333333336e-06, 'epoch': 65.84, 'step': 4115}
{'loss': 0.4914, 'learning_rate': 1.9622222222222224e-06, 'epoch': 65.92, 'step': 4120}
{'loss': 0.4708, 'learning_rate': 1.9511111111111113e-06, 'epoch': 66.0, 'step': 4125}
{'loss': 0.5147, 'learning_rate': 1.94e-06, 'epoch': 66.08, 'step': 4130}
{'loss': 0.4539, 'learning_rate': 1.928888888888889e-06, 'epoch': 66.16, 'step': 4135}
{'loss': 0.4753, 'learning_rate': 1.9177777777777777e-06, 'epoch': 66.24, 'step': 4140}
{'loss': 0.4806, 'learning_rate': 1.906666666666667e-06, 'epoch': 66.32, 'step': 4145}
{'loss': 0.4764, 'learning_rate': 1.8955555555555557e-06, 'epoch': 66.4, 'step': 4150}
{'loss': 0.4667, 'learning_rate': 1.8844444444444445e-06, 'epoch': 66.48, 'step': 4155}
{'loss': 0.462, 'learning_rate': 1.8733333333333333e-06, 'epoch': 66.56, 'step': 4160}
{'loss': 0.4518, 'learning_rate': 1.8622222222222226e-06, 'epoch': 66.64, 'step': 4165}
{'loss': 0.4856, 'learning_rate': 1.8511111111111114e-06, 'epoch': 66.72, 'step': 4170}
{'loss': 0.4553, 'learning_rate': 1.8400000000000002e-06, 'epoch': 66.8, 'step': 4175}
{'loss': 0.4441, 'learning_rate': 1.828888888888889e-06, 'epoch': 66.88, 'step': 4180}
{'loss': 0.4695, 'learning_rate': 1.817777777777778e-06, 'epoch': 66.96, 'step': 4185}
{'loss': 0.4748, 'learning_rate': 1.8066666666666668e-06, 'epoch': 67.04, 'step': 4190}
{'loss': 0.4538, 'learning_rate': 1.7955555555555556e-06, 'epoch': 67.12, 'step': 4195}
{'loss': 0.4802, 'learning_rate': 1.7844444444444444e-06, 'epoch': 67.2, 'step': 4200}
{'loss': 0.4772, 'learning_rate': 1.7733333333333336e-06, 'epoch': 67.28, 'step': 4205}
{'loss': 0.4709, 'learning_rate': 1.7622222222222225e-06, 'epoch': 67.36, 'step': 4210}
{'loss': 0.453, 'learning_rate': 1.7511111111111113e-06, 'epoch': 67.44, 'step': 4215}
{'loss': 0.4685, 'learning_rate': 1.74e-06, 'epoch': 67.52, 'step': 4220}
{'loss': 0.4721, 'learning_rate': 1.728888888888889e-06, 'epoch': 67.6, 'step': 4225}
{'loss': 0.466, 'learning_rate': 1.717777777777778e-06, 'epoch': 67.68, 'step': 4230}
{'loss': 0.4585, 'learning_rate': 1.7066666666666667e-06, 'epoch': 67.76, 'step': 4235}
{'loss': 0.4491, 'learning_rate': 1.6955555555555555e-06, 'epoch': 67.84, 'step': 4240}
{'loss': 0.4858, 'learning_rate': 1.6844444444444447e-06, 'epoch': 67.92, 'step': 4245}
{'loss': 0.4691, 'learning_rate': 1.6733333333333335e-06, 'epoch': 68.0, 'step': 4250}
{'loss': 0.4482, 'learning_rate': 1.6622222222222224e-06, 'epoch': 68.08, 'step': 4255}
{'loss': 0.4688, 'learning_rate': 1.6511111111111112e-06, 'epoch': 68.16, 'step': 4260}
{'loss': 0.48, 'learning_rate': 1.6400000000000002e-06, 'epoch': 68.24, 'step': 4265}
{'loss': 0.4964, 'learning_rate': 1.628888888888889e-06, 'epoch': 68.32, 'step': 4270}
{'loss': 0.4716, 'learning_rate': 1.6177777777777778e-06, 'epoch': 68.4, 'step': 4275}
{'loss': 0.4593, 'learning_rate': 1.606666666666667e-06, 'epoch': 68.48, 'step': 4280}
{'loss': 0.4912, 'learning_rate': 1.5955555555555558e-06, 'epoch': 68.56, 'step': 4285}
{'loss': 0.4668, 'learning_rate': 1.5844444444444446e-06, 'epoch': 68.64, 'step': 4290}
{'loss': 0.4542, 'learning_rate': 1.5733333333333334e-06, 'epoch': 68.72, 'step': 4295}
{'loss': 0.4657, 'learning_rate': 1.5622222222222225e-06, 'epoch': 68.8, 'step': 4300}
{'loss': 0.4657, 'learning_rate': 1.5511111111111113e-06, 'epoch': 68.88, 'step': 4305}
{'loss': 0.4601, 'learning_rate': 1.54e-06, 'epoch': 68.96, 'step': 4310}
{'loss': 0.4537, 'learning_rate': 1.5288888888888889e-06, 'epoch': 69.04, 'step': 4315}
{'loss': 0.4419, 'learning_rate': 1.5177777777777781e-06, 'epoch': 69.12, 'step': 4320}
{'loss': 0.4599, 'learning_rate': 1.506666666666667e-06, 'epoch': 69.2, 'step': 4325}
{'loss': 0.4445, 'learning_rate': 1.4955555555555557e-06, 'epoch': 69.28, 'step': 4330}
{'loss': 0.449, 'learning_rate': 1.4844444444444445e-06, 'epoch': 69.36, 'step': 4335}
{'loss': 0.4507, 'learning_rate': 1.4733333333333336e-06, 'epoch': 69.44, 'step': 4340}
{'loss': 0.443, 'learning_rate': 1.4622222222222224e-06, 'epoch': 69.52, 'step': 4345}
{'loss': 0.4711, 'learning_rate': 1.4511111111111112e-06, 'epoch': 69.6, 'step': 4350}
{'loss': 0.5361, 'learning_rate': 1.44e-06, 'epoch': 69.68, 'step': 4355}
{'loss': 0.4479, 'learning_rate': 1.4288888888888892e-06, 'epoch': 69.76, 'step': 4360}
{'loss': 0.4528, 'learning_rate': 1.417777777777778e-06, 'epoch': 69.84, 'step': 4365}
{'loss': 0.4616, 'learning_rate': 1.4066666666666668e-06, 'epoch': 69.92, 'step': 4370}
{'loss': 0.4457, 'learning_rate': 1.3955555555555556e-06, 'epoch': 70.0, 'step': 4375}
{'loss': 0.4548, 'learning_rate': 1.3844444444444446e-06, 'epoch': 70.08, 'step': 4380}
{'loss': 0.4589, 'learning_rate': 1.3733333333333335e-06, 'epoch': 70.16, 'step': 4385}
{'loss': 0.5155, 'learning_rate': 1.3622222222222223e-06, 'epoch': 70.24, 'step': 4390}
{'loss': 0.482, 'learning_rate': 1.351111111111111e-06, 'epoch': 70.32, 'step': 4395}
{'loss': 0.4778, 'learning_rate': 1.34e-06, 'epoch': 70.4, 'step': 4400}
{'loss': 0.4555, 'learning_rate': 1.3288888888888891e-06, 'epoch': 70.48, 'step': 4405}
{'loss': 0.445, 'learning_rate': 1.317777777777778e-06, 'epoch': 70.56, 'step': 4410}
{'loss': 0.4718, 'learning_rate': 1.3066666666666667e-06, 'epoch': 70.64, 'step': 4415}
{'loss': 0.4422, 'learning_rate': 1.2955555555555557e-06, 'epoch': 70.72, 'step': 4420}
{'loss': 0.4579, 'learning_rate': 1.2844444444444445e-06, 'epoch': 70.8, 'step': 4425}
{'loss': 0.4516, 'learning_rate': 1.2733333333333334e-06, 'epoch': 70.88, 'step': 4430}
{'loss': 0.4449, 'learning_rate': 1.2622222222222224e-06, 'epoch': 70.96, 'step': 4435}
{'loss': 0.4808, 'learning_rate': 1.2511111111111112e-06, 'epoch': 71.04, 'step': 4440}
{'loss': 0.4635, 'learning_rate': 1.2400000000000002e-06, 'epoch': 71.12, 'step': 4445}
{'loss': 0.4483, 'learning_rate': 1.228888888888889e-06, 'epoch': 71.2, 'step': 4450}
{'loss': 0.4605, 'learning_rate': 1.2177777777777778e-06, 'epoch': 71.28, 'step': 4455}
{'loss': 0.4582, 'learning_rate': 1.2066666666666668e-06, 'epoch': 71.36, 'step': 4460}
{'loss': 0.4932, 'learning_rate': 1.1955555555555556e-06, 'epoch': 71.44, 'step': 4465}
{'loss': 0.4491, 'learning_rate': 1.1844444444444447e-06, 'epoch': 71.52, 'step': 4470}
{'loss': 0.4615, 'learning_rate': 1.1733333333333335e-06, 'epoch': 71.6, 'step': 4475}
{'loss': 0.4713, 'learning_rate': 1.1622222222222223e-06, 'epoch': 71.68, 'step': 4480}
{'loss': 0.4534, 'learning_rate': 1.151111111111111e-06, 'epoch': 71.76, 'step': 4485}
{'loss': 0.4766, 'learning_rate': 1.14e-06, 'epoch': 71.84, 'step': 4490}
{'loss': 0.437, 'learning_rate': 1.128888888888889e-06, 'epoch': 71.92, 'step': 4495}
{'loss': 0.4557, 'learning_rate': 1.117777777777778e-06, 'epoch': 72.0, 'step': 4500}
{'loss': 0.4491, 'learning_rate': 1.1066666666666667e-06, 'epoch': 72.08, 'step': 4505}
{'loss': 0.4455, 'learning_rate': 1.0955555555555557e-06, 'epoch': 72.16, 'step': 4510}
{'loss': 0.451, 'learning_rate': 1.0844444444444446e-06, 'epoch': 72.24, 'step': 4515}
{'loss': 0.4859, 'learning_rate': 1.0733333333333334e-06, 'epoch': 72.32, 'step': 4520}
{'loss': 0.4534, 'learning_rate': 1.0622222222222222e-06, 'epoch': 72.4, 'step': 4525}
{'loss': 0.4584, 'learning_rate': 1.0511111111111112e-06, 'epoch': 72.48, 'step': 4530}
{'loss': 0.4795, 'learning_rate': 1.04e-06, 'epoch': 72.56, 'step': 4535}
{'loss': 0.4597, 'learning_rate': 1.028888888888889e-06, 'epoch': 72.64, 'step': 4540}
{'loss': 0.4846, 'learning_rate': 1.0177777777777778e-06, 'epoch': 72.72, 'step': 4545}
{'loss': 0.4648, 'learning_rate': 1.0066666666666668e-06, 'epoch': 72.8, 'step': 4550}
{'loss': 0.4459, 'learning_rate': 9.955555555555556e-07, 'epoch': 72.88, 'step': 4555}
{'loss': 0.5241, 'learning_rate': 9.844444444444445e-07, 'epoch': 72.96, 'step': 4560}
{'loss': 0.474, 'learning_rate': 9.733333333333333e-07, 'epoch': 73.04, 'step': 4565}
{'loss': 0.4458, 'learning_rate': 9.622222222222223e-07, 'epoch': 73.12, 'step': 4570}
{'loss': 0.4594, 'learning_rate': 9.511111111111111e-07, 'epoch': 73.2, 'step': 4575}
{'loss': 0.4736, 'learning_rate': 9.400000000000001e-07, 'epoch': 73.28, 'step': 4580}
{'loss': 0.5006, 'learning_rate': 9.288888888888889e-07, 'epoch': 73.36, 'step': 4585}
{'loss': 0.4708, 'learning_rate': 9.177777777777778e-07, 'epoch': 73.44, 'step': 4590}
{'loss': 0.464, 'learning_rate': 9.066666666666668e-07, 'epoch': 73.52, 'step': 4595}
{'loss': 0.4449, 'learning_rate': 8.955555555555557e-07, 'epoch': 73.6, 'step': 4600}
{'loss': 0.4637, 'learning_rate': 8.844444444444446e-07, 'epoch': 73.68, 'step': 4605}
{'loss': 0.4585, 'learning_rate': 8.733333333333334e-07, 'epoch': 73.76, 'step': 4610}
{'loss': 0.4471, 'learning_rate': 8.622222222222224e-07, 'epoch': 73.84, 'step': 4615}
{'loss': 0.4649, 'learning_rate': 8.511111111111112e-07, 'epoch': 73.92, 'step': 4620}
{'loss': 0.4616, 'learning_rate': 8.400000000000001e-07, 'epoch': 74.0, 'step': 4625}
{'loss': 0.4655, 'learning_rate': 8.288888888888889e-07, 'epoch': 74.08, 'step': 4630}
{'loss': 0.4515, 'learning_rate': 8.177777777777779e-07, 'epoch': 74.16, 'step': 4635}
{'loss': 0.464, 'learning_rate': 8.066666666666667e-07, 'epoch': 74.24, 'step': 4640}
{'loss': 0.4985, 'learning_rate': 7.955555555555557e-07, 'epoch': 74.32, 'step': 4645}
{'loss': 0.4687, 'learning_rate': 7.844444444444445e-07, 'epoch': 74.4, 'step': 4650}
{'loss': 0.468, 'learning_rate': 7.733333333333335e-07, 'epoch': 74.48, 'step': 4655}
{'loss': 0.4529, 'learning_rate': 7.622222222222223e-07, 'epoch': 74.56, 'step': 4660}
{'loss': 0.453, 'learning_rate': 7.511111111111112e-07, 'epoch': 74.64, 'step': 4665}
{'loss': 0.4472, 'learning_rate': 7.4e-07, 'epoch': 74.72, 'step': 4670}
{'loss': 0.4774, 'learning_rate': 7.28888888888889e-07, 'epoch': 74.8, 'step': 4675}
{'loss': 0.4504, 'learning_rate': 7.177777777777778e-07, 'epoch': 74.88, 'step': 4680}
{'loss': 0.4582, 'learning_rate': 7.066666666666667e-07, 'epoch': 74.96, 'step': 4685}
{'loss': 0.444, 'learning_rate': 6.955555555555556e-07, 'epoch': 75.04, 'step': 4690}
{'loss': 0.4868, 'learning_rate': 6.844444444444446e-07, 'epoch': 75.12, 'step': 4695}
{'loss': 0.4547, 'learning_rate': 6.733333333333334e-07, 'epoch': 75.2, 'step': 4700}
{'loss': 0.4577, 'learning_rate': 6.622222222222223e-07, 'epoch': 75.28, 'step': 4705}
{'loss': 0.4743, 'learning_rate': 6.511111111111111e-07, 'epoch': 75.36, 'step': 4710}
{'loss': 0.4514, 'learning_rate': 6.4e-07, 'epoch': 75.44, 'step': 4715}
{'loss': 0.4948, 'learning_rate': 6.288888888888889e-07, 'epoch': 75.52, 'step': 4720}
{'loss': 0.4408, 'learning_rate': 6.177777777777778e-07, 'epoch': 75.6, 'step': 4725}
{'loss': 0.4611, 'learning_rate': 6.066666666666668e-07, 'epoch': 75.68, 'step': 4730}
{'loss': 0.4706, 'learning_rate': 5.955555555555556e-07, 'epoch': 75.76, 'step': 4735}
{'loss': 0.4666, 'learning_rate': 5.844444444444445e-07, 'epoch': 75.84, 'step': 4740}
{'loss': 0.4634, 'learning_rate': 5.733333333333334e-07, 'epoch': 75.92, 'step': 4745}
{'loss': 0.4739, 'learning_rate': 5.622222222222223e-07, 'epoch': 76.0, 'step': 4750}
{'loss': 0.4752, 'learning_rate': 5.511111111111111e-07, 'epoch': 76.08, 'step': 4755}
{'loss': 0.4578, 'learning_rate': 5.4e-07, 'epoch': 76.16, 'step': 4760}
{'loss': 0.4848, 'learning_rate': 5.288888888888889e-07, 'epoch': 76.24, 'step': 4765}
{'loss': 0.4679, 'learning_rate': 5.177777777777778e-07, 'epoch': 76.32, 'step': 4770}
{'loss': 0.4449, 'learning_rate': 5.066666666666667e-07, 'epoch': 76.4, 'step': 4775}
{'loss': 0.4407, 'learning_rate': 4.955555555555556e-07, 'epoch': 76.48, 'step': 4780}
{'loss': 0.4784, 'learning_rate': 4.844444444444445e-07, 'epoch': 76.56, 'step': 4785}
{'loss': 0.4708, 'learning_rate': 4.7333333333333334e-07, 'epoch': 76.64, 'step': 4790}
{'loss': 0.4521, 'learning_rate': 4.6222222222222225e-07, 'epoch': 76.72, 'step': 4795}
{'loss': 0.4786, 'learning_rate': 4.511111111111111e-07, 'epoch': 76.8, 'step': 4800}
{'loss': 0.4479, 'learning_rate': 4.4e-07, 'epoch': 76.88, 'step': 4805}
{'loss': 0.4737, 'learning_rate': 4.288888888888889e-07, 'epoch': 76.96, 'step': 4810}
{'loss': 0.4688, 'learning_rate': 4.177777777777778e-07, 'epoch': 77.04, 'step': 4815}
{'loss': 0.4476, 'learning_rate': 4.0666666666666666e-07, 'epoch': 77.12, 'step': 4820}
{'loss': 0.4708, 'learning_rate': 3.9555555555555557e-07, 'epoch': 77.2, 'step': 4825}
{'loss': 0.4677, 'learning_rate': 3.8444444444444453e-07, 'epoch': 77.28, 'step': 4830}
{'loss': 0.4414, 'learning_rate': 3.733333333333334e-07, 'epoch': 77.36, 'step': 4835}
{'loss': 0.4764, 'learning_rate': 3.622222222222223e-07, 'epoch': 77.44, 'step': 4840}
{'loss': 0.4979, 'learning_rate': 3.5111111111111117e-07, 'epoch': 77.52, 'step': 4845}
{'loss': 0.4365, 'learning_rate': 3.4000000000000003e-07, 'epoch': 77.6, 'step': 4850}
{'loss': 0.4447, 'learning_rate': 3.2888888888888894e-07, 'epoch': 77.68, 'step': 4855}
{'loss': 0.444, 'learning_rate': 3.177777777777778e-07, 'epoch': 77.76, 'step': 4860}
{'loss': 0.4661, 'learning_rate': 3.0666666666666666e-07, 'epoch': 77.84, 'step': 4865}
{'loss': 0.4411, 'learning_rate': 2.9555555555555557e-07, 'epoch': 77.92, 'step': 4870}
{'loss': 0.4657, 'learning_rate': 2.844444444444445e-07, 'epoch': 78.0, 'step': 4875}
{'loss': 0.4461, 'learning_rate': 2.7333333333333335e-07, 'epoch': 78.08, 'step': 4880}
{'loss': 0.4576, 'learning_rate': 2.6222222222222226e-07, 'epoch': 78.16, 'step': 4885}
{'loss': 0.4648, 'learning_rate': 2.511111111111111e-07, 'epoch': 78.24, 'step': 4890}
{'loss': 0.4477, 'learning_rate': 2.4000000000000003e-07, 'epoch': 78.32, 'step': 4895}
{'loss': 0.4598, 'learning_rate': 2.2888888888888892e-07, 'epoch': 78.4, 'step': 4900}
{'loss': 0.4407, 'learning_rate': 2.177777777777778e-07, 'epoch': 78.48, 'step': 4905}
{'loss': 0.4606, 'learning_rate': 2.066666666666667e-07, 'epoch': 78.56, 'step': 4910}
{'loss': 0.4731, 'learning_rate': 1.9555555555555558e-07, 'epoch': 78.64, 'step': 4915}
{'loss': 0.4665, 'learning_rate': 1.8444444444444446e-07, 'epoch': 78.72, 'step': 4920}
{'loss': 0.4604, 'learning_rate': 1.7333333333333335e-07, 'epoch': 78.8, 'step': 4925}
{'loss': 0.4759, 'learning_rate': 1.6222222222222224e-07, 'epoch': 78.88, 'step': 4930}
{'loss': 0.4489, 'learning_rate': 1.5111111111111112e-07, 'epoch': 78.96, 'step': 4935}
{'loss': 0.448, 'learning_rate': 1.4e-07, 'epoch': 79.04, 'step': 4940}
{'loss': 0.4947, 'learning_rate': 1.288888888888889e-07, 'epoch': 79.12, 'step': 4945}
{'loss': 0.4564, 'learning_rate': 1.1777777777777778e-07, 'epoch': 79.2, 'step': 4950}
{'loss': 0.461, 'learning_rate': 1.0666666666666667e-07, 'epoch': 79.28, 'step': 4955}
{'loss': 0.4879, 'learning_rate': 9.555555555555556e-08, 'epoch': 79.36, 'step': 4960}
{'loss': 0.4706, 'learning_rate': 8.444444444444444e-08, 'epoch': 79.44, 'step': 4965}
{'loss': 0.4429, 'learning_rate': 7.333333333333334e-08, 'epoch': 79.52, 'step': 4970}
{'loss': 0.4583, 'learning_rate': 6.222222222222223e-08, 'epoch': 79.6, 'step': 4975}
{'loss': 0.5808, 'learning_rate': 5.111111111111112e-08, 'epoch': 79.68, 'step': 4980}
{'loss': 0.4709, 'learning_rate': 4e-08, 'epoch': 79.76, 'step': 4985}
{'loss': 0.4524, 'learning_rate': 2.888888888888889e-08, 'epoch': 79.84, 'step': 4990}
{'loss': 0.4602, 'learning_rate': 1.777777777777778e-08, 'epoch': 79.92, 'step': 4995}
{'loss': 0.4836, 'learning_rate': 6.666666666666667e-09, 'epoch': 80.0, 'step': 5000}
{'eval_loss': 0.49818500876426697, 'eval_runtime': 0.096, 'eval_samples_per_second': 52.11, 'eval_steps_per_second': 10.422, 'epoch': 80.0, 'step': 5000}
{'train_runtime': 3960.3822, 'train_samples_per_second': 40.4, 'train_steps_per_second': 1.263, 'total_flos': 1.2448638938368512e+16, 'train_loss': 0.5119387811660766, 'epoch': 80.0, 'step': 5000}
11:33:32	Start Saving the model...
11:33:45	Start Pushing Fine-tuned model to Huggingface
11:33:47	Input text: Is Indonesia finally set to become an economic superpower?
11:33:47	Generation Speaker Embedding for australia male
11:33:48	Generating output audio file...

Start -- Nov 15 12:26:44 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:26:44	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:26:46	Using Locally Processed Dataset. Skip Processing...
12:26:46	Using model: ./model/trained_yifanhua-threadripper_Nov15_10:27
12:26:48	Input text: Is Indonesia finally set to become an economic superpower?
12:26:48	Generation Speaker Embedding for australia male
12:26:48	Exception: Traceback (most recent call last):
  File "/home/yifanhua/5455-term-project/src/main.py", line 533, in <module>
    spectrogram = pretrained_model.generate_speech(inputs["input_ids"], speaker_embeddings).to(device)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2824, in generate_speech
    return _generate_speech(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 2480, in _generate_speech
    encoder_out = model.speecht5.encoder(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 1448, in forward
    hidden_states = self.prenet(input_values)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py", line 787, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/yifanhua/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)


Start -- Nov 15 12:26:59 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:26:59	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:27:01	Using Locally Processed Dataset. Skip Processing...
12:27:01	Using model: ./model/trained_yifanhua-threadripper_Nov15_10:27
12:27:04	Input text: Is Indonesia finally set to become an economic superpower?
12:27:04	Generation Speaker Embedding for australia male
12:27:05	Generating output audio file...

Start -- Nov 15 12:28:03 -- pytorch=2.1.0+cu121, device=cuda, cpu_count=48
12:28:03	Initializing pretrained model, processor, tokenizer, speaker_model, data_collator, and vocoder
12:28:06	Using Locally Processed Dataset. Skip Processing...
12:28:06	Using model: ./model/trained_yifanhua-threadripper_Nov15_10:27
12:28:08	Input text: Is Indonesia finally set to become an economic superpower?
12:28:08	Generation Speaker Embedding for australia male
12:28:09	Generating output audio file...
